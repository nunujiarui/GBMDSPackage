"doi","Title","Abstract","Keywords","Journal","text_combined"
"""10.4007/annals.2023.198.3.4""","""Characterizing finitely generated fields by a single field axiom""","""We resolve the strong Elementary Equivalence versus Isomorphism Problem for finitely generated fields. That is, we show that for every field in this class, there is a first-order sentence that characterizes this field within the class up to isomorphism. Our solution is conditional on resolution of singularities in characteristic two and unconditional in all other characteristics.""","""elementary equivalence versus isomorphism"", ""finitely generated fields"", ""first-order definability of valuations"", ""higher-dimensional cohomological local-global principles"", ""Pfister forms""","AOM","""Characterizing finitely generated fields by a single field axiom""""We resolve the strong Elementary Equivalence versus Isomorphism Problem for finitely generated fields. That is, we show that for every field in this class, there is a first-order sentence that characterizes this field within the class up to isomorphism. Our solution is conditional on resolution of singularities in characteristic two and unconditional in all other characteristics.""""elementary equivalence versus isomorphism"", ""finitely generated fields"", ""first-order definability of valuations"", ""higher-dimensional cohomological local-global principles"", ""Pfister forms"""
"""10.4007/annals.2023.198.3.6""","""Wreath-like products of groups and their von Neumann algebras I: W^∗-superrigidity""","""We introduce a new class of groups called wreath-like products. These groups are close relatives of the classical wreath products and arise naturally in the context of group theoretic Dehn filling. Unlike ordinary wreath products, many wreath-like products have Kazhdan's property (T). In this paper, we prove that any group $G$ in a natural family of wreath-like products with property (T) is $\\mathrm{W}^\\ast$-superrigid: the group von Neumann algebra $\\mathrm{L}(G)$ remembers the isomorphism class of $G$. This allows us to provide the first examples (in fact, $2^{\\aleph_0}$ pairwise non-isomorphic examples) of $\\mathrm{W}^\\ast$-superrigid groups with property (T).""","""$\\mathrm{II} 1$ factor"", ""$\\mathrm{W}^\\ast$-superrigidity"", ""Connes' rigidity conjecture"", ""deformation/rigidity theory"", ""group theoretic Dehn Filling"", ""group von Neumann algebra"", ""hyperbolic groups"", ""property (T)"", ""wreath-like product groups""","AOM","""Wreath-like products of groups and their von Neumann algebras I: W^∗-superrigidity""""We introduce a new class of groups called wreath-like products. These groups are close relatives of the classical wreath products and arise naturally in the context of group theoretic Dehn filling. Unlike ordinary wreath products, many wreath-like products have Kazhdan's property (T). In this paper, we prove that any group $G$ in a natural family of wreath-like products with property (T) is $\\mathrm{W}^\\ast$-superrigid: the group von Neumann algebra $\\mathrm{L}(G)$ remembers the isomorphism class of $G$. This allows us to provide the first examples (in fact, $2^{\\aleph_0}$ pairwise non-isomorphic examples) of $\\mathrm{W}^\\ast$-superrigid groups with property (T).""""$\\mathrm{II} 1$ factor"", ""$\\mathrm{W}^\\ast$-superrigidity"", ""Connes' rigidity conjecture"", ""deformation/rigidity theory"", ""group theoretic Dehn Filling"", ""group von Neumann algebra"", ""hyperbolic groups"", ""property (T)"", ""wreath-like product groups"""
"""10.4007/annals.2023.198.3.3""","""Statistical dynamics of a hard sphere gas: fluctuating Boltzmann equation and large deviations""","""We present a mathematical theory of dynamical fluctuations for the hard sphere gas in the Boltzmann-Grad limit. We prove that (1) fluctuations of the empirical measure from the solution of the Boltzmann equation, scaled with the square root of the average number of particles, converge to a Gaussian process driven by the fluctuating Boltzmann equation, as predicted by Spohn; (2) large deviations are exponentially small in the average number of particles and are characterized, under regularity assumptions, by a large deviation functional as previously obtained by Rezakhanlou for dynamics with stochastic collisions. The results are valid away from thermal equilibrium, but only for short times. Our strategy is based on uniform a priori bounds on the cumulant generating function, characterizing the fine structure of the small correlations.""","""Boltzmann equation"", ""cluster expansion"", ""Fluctuations"", ""hard sphere gas"", ""large deviations"", ""low density limit""","AOM","""Statistical dynamics of a hard sphere gas: fluctuating Boltzmann equation and large deviations""""We present a mathematical theory of dynamical fluctuations for the hard sphere gas in the Boltzmann-Grad limit. We prove that (1) fluctuations of the empirical measure from the solution of the Boltzmann equation, scaled with the square root of the average number of particles, converge to a Gaussian process driven by the fluctuating Boltzmann equation, as predicted by Spohn; (2) large deviations are exponentially small in the average number of particles and are characterized, under regularity assumptions, by a large deviation functional as previously obtained by Rezakhanlou for dynamics with stochastic collisions. The results are valid away from thermal equilibrium, but only for short times. Our strategy is based on uniform a priori bounds on the cumulant generating function, characterizing the fine structure of the small correlations.""""Boltzmann equation"", ""cluster expansion"", ""Fluctuations"", ""hard sphere gas"", ""large deviations"", ""low density limit"""
"""10.4007/annals.2023.198.3.5""","""Schur multipliers in Schatten-von Neumann classes""","""We establish a rather unexpected and simple criterion for the boundedness of Schur multipliers $S_M$ on Schatten $p$-classes which solves a conjecture proposed by Mikael de la Salle. Given $1\\lt p\\lt \\infty$, a simple form of our main result for $\\mathbf{R}^n \\times \\mathbf{R}^n$ matrices reads as follows: $$\\big\\| S_M \\colon S_p \\to S_p \\big\\|_{\\mathrm{cb}} \\lesssim \\frac{p^2}{p-1} \\sum_{|\\gamma| \\le [\\frac{n}{2}] +1} \\Big\\| |x-y|^{|\\gamma|} \\Big\\{ \\big| \\partial_x^\\gamma M(x,y) \\big| + \\big| \\partial_y^\\gamma M(x,y) \\big| \\Big\\} \\Big\\|_\\infty.$$ In this form, it is a full matrix (nonToeplitz/nontrigonometric) amplification of the Hörmander-Mikhlin multiplier theorem, which admits lower fractional differentiability orders $\\sigma > \\frac{n}{2}$ as well. It trivially includes Arazy's conjecture for $S_p$-multipliers and extends it to $\\alpha$-divided differences. It also leads to new Littlewood-Paley characterizations of $S_p$-norms and strong applications in harmonic analysis for nilpotent and high rank simple Lie group algebras.""","""Hörmander-Mikhlin theorem"", ""noncommutative Calderón-Zygmund theory"", ""Schatten classes"", ""Schur multiplier""","AOM","""Schur multipliers in Schatten-von Neumann classes""""We establish a rather unexpected and simple criterion for the boundedness of Schur multipliers $S_M$ on Schatten $p$-classes which solves a conjecture proposed by Mikael de la Salle. Given $1\\lt p\\lt \\infty$, a simple form of our main result for $\\mathbf{R}^n \\times \\mathbf{R}^n$ matrices reads as follows: $$\\big\\| S_M \\colon S_p \\to S_p \\big\\|_{\\mathrm{cb}} \\lesssim \\frac{p^2}{p-1} \\sum_{|\\gamma| \\le [\\frac{n}{2}] +1} \\Big\\| |x-y|^{|\\gamma|} \\Big\\{ \\big| \\partial_x^\\gamma M(x,y) \\big| + \\big| \\partial_y^\\gamma M(x,y) \\big| \\Big\\} \\Big\\|_\\infty.$$ In this form, it is a full matrix (nonToeplitz/nontrigonometric) amplification of the Hörmander-Mikhlin multiplier theorem, which admits lower fractional differentiability orders $\\sigma > \\frac{n}{2}$ as well. It trivially includes Arazy's conjecture for $S_p$-multipliers and extends it to $\\alpha$-divided differences. It also leads to new Littlewood-Paley characterizations of $S_p$-norms and strong applications in harmonic analysis for nilpotent and high rank simple Lie group algebras.""""Hörmander-Mikhlin theorem"", ""noncommutative Calderón-Zygmund theory"", ""Schatten classes"", ""Schur multiplier"""
"""10.4007/annals.2023.198.3.1""","""Universality for lozenge tiling local statistics""","""In this paper we consider uniformly random lozenge tilings of arbitrary domains approximating (after suitable normalization) a closed, simply-connected subset of $\\mathbb{R}^2$ with piecewise smooth, simple boundary. We show that the local statistics of this model around any point in the liquid region of its limit shape are given by the infinite-volume, translation-invariant, extremal Gibbs measure of the appropriate slope, thereby confirming a prediction of Cohn-Kenyon-Propp from 2001 in the case of lozenge tilings. Our proofs proceed by locally coupling a uniformly random lozenge tiling with a model of Bernoulli random walks conditioned to never intersect, whose convergence of local statistics has been recently understood by the work of Gorin-Petrov. Central to implementing this procedure is to establish a local law for the random tiling, which states that the associated height function is approximately linear on any mesoscopic scale.""","""boundary conditions"", ""Local law"", ""local statistics"", ""lozenge tilings"", ""non-intersecting random walks"", ""Universality""","AOM","""Universality for lozenge tiling local statistics""""In this paper we consider uniformly random lozenge tilings of arbitrary domains approximating (after suitable normalization) a closed, simply-connected subset of $\\mathbb{R}^2$ with piecewise smooth, simple boundary. We show that the local statistics of this model around any point in the liquid region of its limit shape are given by the infinite-volume, translation-invariant, extremal Gibbs measure of the appropriate slope, thereby confirming a prediction of Cohn-Kenyon-Propp from 2001 in the case of lozenge tilings. Our proofs proceed by locally coupling a uniformly random lozenge tiling with a model of Bernoulli random walks conditioned to never intersect, whose convergence of local statistics has been recently understood by the work of Gorin-Petrov. Central to implementing this procedure is to establish a local law for the random tiling, which states that the associated height function is approximately linear on any mesoscopic scale.""""boundary conditions"", ""Local law"", ""local statistics"", ""lozenge tilings"", ""non-intersecting random walks"", ""Universality"""
"""10.4007/annals.2023.198.3.7""","""Retraction: \""Quasi-projectivity of moduli spaces of polarized varieties\""""","""In his article [1] János Kollár has published a counterexample to the main result Theorem~1 of the paper [2]. The critical point is that in general the positive curvature current of the singular hermitian metric on the constructed holomorphic line bundle does not have vanishing Lelong numbers (cf. Theorem 4 and Theorem 5) as claimed. Consequently the quasi-projectivity criterion of the paper cannot be applied.""","","AOM","""Retraction: \""Quasi-projectivity of moduli spaces of polarized varieties\""""""In his article [1] János Kollár has published a counterexample to the main result Theorem~1 of the paper [2]. The critical point is that in general the positive curvature current of the singular hermitian metric on the constructed holomorphic line bundle does not have vanishing Lelong numbers (cf. Theorem 4 and Theorem 5) as claimed. Consequently the quasi-projectivity criterion of the paper cannot be applied."""
"""10.4007/annals.2023.198.3.2""","""Regularity of minimal surfaces near quadratic cones""","""Hardt-Simon proved that every area-minimizing hypercone $\\mathbf{C}$ having only an isolated singularity fits into a foliation of $\\mathbb{R}^{n+1}$ by smooth, area-minimizing hypersurfaces asymptotic to $\\mathbf{C}$. In this paper we prove that if a stationary integral $n$-varifold $M$ in the unit ball $B_1 \\subset \\mathbb{R}^{n+1}$ lies sufficiently close to a minimizing quadratic cone (for example, the Simons' cone $\\mathbf{C}^{3,3}$), then $\\mathrm{spt}\\, M \\cap B_{1/2}$ is a $C^{1,\\alpha}$ perturbation of either the cone itself, or some leaf of its associated foliation. In particular, we show that singularities modeled on these cones determine the local structure not only of $M$, but of any nearby minimal surface. Our result also implies the Bernstein-type result of Simon-Solomon, which characterizes area-minimizing hypersurfaces asymptotic to a quadratic cone as either the cone itself, or some leaf of the foliation.""","""Foliation"", ""minimal surface"", ""quadratic cone"", ""regularity""","AOM","""Regularity of minimal surfaces near quadratic cones""""Hardt-Simon proved that every area-minimizing hypercone $\\mathbf{C}$ having only an isolated singularity fits into a foliation of $\\mathbb{R}^{n+1}$ by smooth, area-minimizing hypersurfaces asymptotic to $\\mathbf{C}$. In this paper we prove that if a stationary integral $n$-varifold $M$ in the unit ball $B_1 \\subset \\mathbb{R}^{n+1}$ lies sufficiently close to a minimizing quadratic cone (for example, the Simons' cone $\\mathbf{C}^{3,3}$), then $\\mathrm{spt}\\, M \\cap B_{1/2}$ is a $C^{1,\\alpha}$ perturbation of either the cone itself, or some leaf of its associated foliation. In particular, we show that singularities modeled on these cones determine the local structure not only of $M$, but of any nearby minimal surface. Our result also implies the Bernstein-type result of Simon-Solomon, which characterizes area-minimizing hypersurfaces asymptotic to a quadratic cone as either the cone itself, or some leaf of the foliation.""""Foliation"", ""minimal surface"", ""quadratic cone"", ""regularity"""
"""10.4007/annals.2023.198.2.3""","""Parabolicity conjecture of $F$-isocrystals""","""In this article we prove Crew's parabolicity conjecture of $F$-isocrystals. For this purpose, we introduce and study the notion of $\\dagger$-hull of a sub-$F$-isocrystal. On the way, we prove a new Lefschetz theorem for overconvergent $F$-isocrystals.""","""$F$-isocrystal"", ""monodromy group"", ""slope""","AOM","""Parabolicity conjecture of $F$-isocrystals""""In this article we prove Crew's parabolicity conjecture of $F$-isocrystals. For this purpose, we introduce and study the notion of $\\dagger$-hull of a sub-$F$-isocrystal. On the way, we prove a new Lefschetz theorem for overconvergent $F$-isocrystals.""""$F$-isocrystal"", ""monodromy group"", ""slope"""
"""10.4007/annals.2023.198.2.8""","""Erratum to \""On the averaged Colmez conjecture\""""","""We will fix an error in our paper on the average Colmez conjecture by proving a slightly weaker statement than Theorem 2.7, which is sufficient for application to the main results.""","""Colmez conjecture"", ""Complex Multiplication"", ""Faltings height""","AOM","""Erratum to \""On the averaged Colmez conjecture\""""""We will fix an error in our paper on the average Colmez conjecture by proving a slightly weaker statement than Theorem 2.7, which is sufficient for application to the main results.""""Colmez conjecture"", ""Complex Multiplication"", ""Faltings height"""
"""10.4007/annals.2023.198.2.4""","""High rank invariant subvarieties""","""We classify $\\mathrm{GL}(2,\\mathbb{R})$ orbit closures of translation surfaces of rank at least half the genus plus $1$.""","""Abelian differential"", ""invariant subvariety"", ""moduli space"", ""orbit closure"", ""‎rank‎"", ""translation surface""","AOM","""High rank invariant subvarieties""""We classify $\\mathrm{GL}(2,\\mathbb{R})$ orbit closures of translation surfaces of rank at least half the genus plus $1$.""""Abelian differential"", ""invariant subvariety"", ""moduli space"", ""orbit closure"", ""‎rank‎"", ""translation surface"""
"""10.4007/annals.2023.198.2.7""","""Wall crossing for moduli of stable log pairs""","""We prove, under suitable conditions, that there exist wall-crossing and reduction morphisms for moduli spaces of stable log pairs in all dimensions as one varies the coefficients of the divisor.""","""minimal model program"", ""moduli spaces"", ""varieties of log general type""","AOM","""Wall crossing for moduli of stable log pairs""""We prove, under suitable conditions, that there exist wall-crossing and reduction morphisms for moduli spaces of stable log pairs in all dimensions as one varies the coefficients of the divisor.""""minimal model program"", ""moduli spaces"", ""varieties of log general type"""
"""10.4007/annals.2023.198.2.6""","""Near optimal spectral gaps for hyperbolic surfaces""","""We prove that if $X$ is a finite area non-compact hyperbolic surface, then for any $\\epsilon >0$, with probability tending to one as $n\\to \\infty$, a uniformly random degree $n$ Riemannian cover of $X$ has no eigenvalues of the Laplacian in $[0,\\frac{1}{4}-\\epsilon)$ other than those of $X$, and with the same multiplicities. As a result, using a compactification procedure due to Buser, Burger, and Dodziuk, we settle in the affirmative the question of whether there exists a sequence of closed hyperbolic surfaces with genera tending to infinity and first non-zero eigenvalue of the Laplacian tending to $\\frac{1}{4}$.""","""hyperbolic surface"", ""random covering space"", ""small eigenvalues"", ""spectral gap""","AOM","""Near optimal spectral gaps for hyperbolic surfaces""""We prove that if $X$ is a finite area non-compact hyperbolic surface, then for any $\\epsilon >0$, with probability tending to one as $n\\to \\infty$, a uniformly random degree $n$ Riemannian cover of $X$ has no eigenvalues of the Laplacian in $[0,\\frac{1}{4}-\\epsilon)$ other than those of $X$, and with the same multiplicities. As a result, using a compactification procedure due to Buser, Burger, and Dodziuk, we settle in the affirmative the question of whether there exists a sequence of closed hyperbolic surfaces with genera tending to infinity and first non-zero eigenvalue of the Laplacian tending to $\\frac{1}{4}$.""""hyperbolic surface"", ""random covering space"", ""small eigenvalues"", ""spectral gap"""
"""10.4007/annals.2023.198.2.1""","""The Frobenius structure theorem for affine log Calabi-Yau varieties containing a torus""","""Let $U$ be an affine log Calabi-Yau variety containing an open algebraic torus. We show that the naive counts of rational curves in $U$ uniquely determine a commutative associative algebra equipped with a compatible multilinear form. This proves a variant of the Frobenius structure conjecture by Gross-Hacking-Keel in mirror symmetry, and the spectrum of this algebra is supposed to give the hypothetical mirror family. Although the statement of our theorem involves only elementary algebraic geometry, our proof employs Berkovich non-archimedean analytic methods. We construct the structure constants of the algebra via counting non-archimedean analytic disks in the analytification of $U$. We establish various properties of the counting, notably deformation invariance, symmetry, gluing formula and convexity. In the special case when $U$ is a Fock-Goncharov skew-symmetric X-cluster variety, we prove that our algebra generalizes, and gives a direct geometric construction of, the mirror algebra of Gross-Hacking-Keel-Kontsevich. The comparison is proved via a canonical scattering diagram constructed from counts of infinitesimal non-archimedean analytic cylinders, without using the Kontsevich-Soibelman algorithm. Several combinatorial conjectures of GHKK, as well as the positivity in the Laurent phenomenon, follow readily from the geometric description.""","""broken lines"", ""cluster algebra"", ""Frobenius structure"", ""log Calabi-Yau"", ""mirror symmetry"", ""non-Archimedean geometry"", ""rigid analytic geometry"", ""scattering diagram"", ""skeletal curve"", ""wall-crossing""","AOM","""The Frobenius structure theorem for affine log Calabi-Yau varieties containing a torus""""Let $U$ be an affine log Calabi-Yau variety containing an open algebraic torus. We show that the naive counts of rational curves in $U$ uniquely determine a commutative associative algebra equipped with a compatible multilinear form. This proves a variant of the Frobenius structure conjecture by Gross-Hacking-Keel in mirror symmetry, and the spectrum of this algebra is supposed to give the hypothetical mirror family. Although the statement of our theorem involves only elementary algebraic geometry, our proof employs Berkovich non-archimedean analytic methods. We construct the structure constants of the algebra via counting non-archimedean analytic disks in the analytification of $U$. We establish various properties of the counting, notably deformation invariance, symmetry, gluing formula and convexity. In the special case when $U$ is a Fock-Goncharov skew-symmetric X-cluster variety, we prove that our algebra generalizes, and gives a direct geometric construction of, the mirror algebra of Gross-Hacking-Keel-Kontsevich. The comparison is proved via a canonical scattering diagram constructed from counts of infinitesimal non-archimedean analytic cylinders, without using the Kontsevich-Soibelman algorithm. Several combinatorial conjectures of GHKK, as well as the positivity in the Laurent phenomenon, follow readily from the geometric description.""""broken lines"", ""cluster algebra"", ""Frobenius structure"", ""log Calabi-Yau"", ""mirror symmetry"", ""non-Archimedean geometry"", ""rigid analytic geometry"", ""scattering diagram"", ""skeletal curve"", ""wall-crossing"""
"""10.4007/annals.2023.198.2.2""","""A proof of the Erdős--Faber--Lovász conjecture""","""The Erdős--Faber--Lovász conjecture (posed in 1972) states that the chromatic index of any linear hypergraph on $n$ vertices is at most $n$. In this paper, we prove this conjecture for every large $n$. We also provide stability versions of this result, which confirm a prediction of Kahn.""","""absorption"", ""chromatic index"", ""Graph coloring"", ""hypergraph edge coloring"", ""nibble""","AOM","""A proof of the Erdős--Faber--Lovász conjecture""""The Erdős--Faber--Lovász conjecture (posed in 1972) states that the chromatic index of any linear hypergraph on $n$ vertices is at most $n$. In this paper, we prove this conjecture for every large $n$. We also provide stability versions of this result, which confirm a prediction of Kahn.""""absorption"", ""chromatic index"", ""Graph coloring"", ""hypergraph edge coloring"", ""nibble"""
"""10.4007/annals.2023.198.2.5""","""Compact moduli of K3 surfaces""","""We construct geometric compactifications of the moduli space $F_{2d}$ of polarized K3 surfaces in any degree $2d$. Our construction is via KSBA theory, by considering canonical choices of divisor $R\\in |nL|$ on each polarized K3 surface $(X,L)\\in F_{2d}$. The main new notion is that of a recognizable divisor $R$, a choice which can be consistently extended to all central fibers of Kulikov models. We prove that any choice of recognizable divisor leads to a semitoroidal compactification of the period space, at least up to normalization. Finally, we prove that the rational curve divisor is recognizable for all degrees.""","""K3 surfaces"", ""KSBA compactification"", ""moduli spaces""","AOM","""Compact moduli of K3 surfaces""""We construct geometric compactifications of the moduli space $F_{2d}$ of polarized K3 surfaces in any degree $2d$. Our construction is via KSBA theory, by considering canonical choices of divisor $R\\in |nL|$ on each polarized K3 surface $(X,L)\\in F_{2d}$. The main new notion is that of a recognizable divisor $R$, a choice which can be consistently extended to all central fibers of Kulikov models. We prove that any choice of recognizable divisor leads to a semitoroidal compactification of the period space, at least up to normalization. Finally, we prove that the rational curve divisor is recognizable for all degrees.""""K3 surfaces"", ""KSBA compactification"", ""moduli spaces"""
"""10.4007/annals.2023.198.1.2""","""Transversality and super-rigidity for multiply covered holomorphic curves""","""We develop new techniques to study regularity questions for moduli spaces of pseudoholomorphic curves that are multiply covered. Among the main results, we show that unbranched multiple covers of closed holomorphic curves are generically regular, and simple index $0$ curves in dimensions greater than four are generically super-rigid, implying, e.g., that the Gromov-Witten invariants of Calabi-Yau $3$-folds reduce to sums of local invariants for finite sets of embedded curves. We also establish partial results on super-rigidity in dimension four and regularity of branched covers, and briefly discuss the outlook for bifurcation analysis. The proofs are based on a general stratification result for moduli spaces of multiple covers, framed in terms of a representation-theoretic splitting of Cauchy-Riemann operators with symmetries.""","""Calabi-Yau manifolds"", ""Gromov-Witten invariants"", ""holomorphic curves"", ""super-rigidity"", ""transversality""","AOM","""Transversality and super-rigidity for multiply covered holomorphic curves""""We develop new techniques to study regularity questions for moduli spaces of pseudoholomorphic curves that are multiply covered. Among the main results, we show that unbranched multiple covers of closed holomorphic curves are generically regular, and simple index $0$ curves in dimensions greater than four are generically super-rigid, implying, e.g., that the Gromov-Witten invariants of Calabi-Yau $3$-folds reduce to sums of local invariants for finite sets of embedded curves. We also establish partial results on super-rigidity in dimension four and regularity of branched covers, and briefly discuss the outlook for bifurcation analysis. The proofs are based on a general stratification result for moduli spaces of multiple covers, framed in terms of a representation-theoretic splitting of Cauchy-Riemann operators with symmetries.""""Calabi-Yau manifolds"", ""Gromov-Witten invariants"", ""holomorphic curves"", ""super-rigidity"", ""transversality"""
"""10.4007/annals.2023.198.1.4""","""On $L^\\infty$ estimates for complex Monge-Ampère equations""","""A PDE proof is provided for the sharp $L^\\infty$ estimates for the complex Monge-Ampère equation that had required pluripotential theory before. The proof covers both cases of fixed background as well as degenerating background metrics. It extends to more general fully non-linear equations satisfying a structural condition, and it also gives estimates of Trudinger type.""","""auxiliary equations"", ""energy and entropy estimates"", ""fully non-linear equations"", ""test function for comparison""","AOM","""On $L^\\infty$ estimates for complex Monge-Ampère equations""""A PDE proof is provided for the sharp $L^\\infty$ estimates for the complex Monge-Ampère equation that had required pluripotential theory before. The proof covers both cases of fixed background as well as degenerating background metrics. It extends to more general fully non-linear equations satisfying a structural condition, and it also gives estimates of Trudinger type.""""auxiliary equations"", ""energy and entropy estimates"", ""fully non-linear equations"", ""test function for comparison"""
"""10.4007/annals.2023.198.1.1""","""Pseudorandom sets in Grassmann graph have near-perfect expansion""","""We prove that pseudorandom sets in Grassmann graph have near-perfect expansion. This completes the proof of the $2$-to-$2$ Games Conjecture (albeit with imperfect completeness). Some implications of this new result are improved hardness results for Minimum Vertex Cover, improving on the work of Dinur and Safra [Ann. of Math. ${\\bf 162}$ (2005), 439--485], and new hardness gaps for Unique-Games. The Grassmann graph ${\\sf Gr}_{\\sf{global}}$ contains induced subgraphs ${\\sf Gr}_{\\sf{local}}$ that are themselves isomorphic to Grassmann graphs of lower orders. A set is called pseudorandom if its density is $o(1)$ inside all subgraphs ${\\sf Gr}_{\\sf{local}}$ whose order is $O(1)$ lower than that of ${\\sf Gr}_{\\sf{global}}$. We prove that pseudorandom sets have expansion $1-o(1)$, greatly extending the results and techniques of a previous work of the authors with Dinur and Kindler.""","""hypercontractivity"", ""probabilistically checkable proofs"", ""small-set expansion"", ""unique-games conjecture""","AOM","""Pseudorandom sets in Grassmann graph have near-perfect expansion""""We prove that pseudorandom sets in Grassmann graph have near-perfect expansion. This completes the proof of the $2$-to-$2$ Games Conjecture (albeit with imperfect completeness). Some implications of this new result are improved hardness results for Minimum Vertex Cover, improving on the work of Dinur and Safra [Ann. of Math. ${\\bf 162}$ (2005), 439--485], and new hardness gaps for Unique-Games. The Grassmann graph ${\\sf Gr}_{\\sf{global}}$ contains induced subgraphs ${\\sf Gr}_{\\sf{local}}$ that are themselves isomorphic to Grassmann graphs of lower orders. A set is called pseudorandom if its density is $o(1)$ inside all subgraphs ${\\sf Gr}_{\\sf{local}}$ whose order is $O(1)$ lower than that of ${\\sf Gr}_{\\sf{global}}$. We prove that pseudorandom sets have expansion $1-o(1)$, greatly extending the results and techniques of a previous work of the authors with Dinur and Kindler.""""hypercontractivity"", ""probabilistically checkable proofs"", ""small-set expansion"", ""unique-games conjecture"""
"""10.4007/annals.2023.198.1.3""","""Naked singularities for the Einstein vacuum equations: The exterior solution""","""In this work we initiate the mathematical study of naked singularities for the Einstein vacuum equations in $3+1$ dimensions by constructing solutions which correspond to the exterior region of a naked singularity. A key element is our introduction of a new type of self-similarity for the Einstein vacuum equations. Connected to this is a new geometric twisting phenomenon which plays the leading role in singularity formation. Prior to this work, the only known examples of naked singularities were the solutions constructed by Christodoulou for the spherically symmetric Einstein-scalar-field system, as well as other solutions explored numerically for either the spherically symmetric Einstein equations coupled to suitable matter models or for the Einstein equations in higher dimensions.""","""naked singularity"", ""singularity"", ""weak cosmic censorship""","AOM","""Naked singularities for the Einstein vacuum equations: The exterior solution""""In this work we initiate the mathematical study of naked singularities for the Einstein vacuum equations in $3+1$ dimensions by constructing solutions which correspond to the exterior region of a naked singularity. A key element is our introduction of a new type of self-similarity for the Einstein vacuum equations. Connected to this is a new geometric twisting phenomenon which plays the leading role in singularity formation. Prior to this work, the only known examples of naked singularities were the solutions constructed by Christodoulou for the spherically symmetric Einstein-scalar-field system, as well as other solutions explored numerically for either the spherically symmetric Einstein equations coupled to suitable matter models or for the Einstein equations in higher dimensions.""""naked singularity"", ""singularity"", ""weak cosmic censorship"""
"""10.4007/annals.2023.197.3.1""","""Existence of infinitely many minimal hypersurfaces in closed manifolds""","""Using min-max theory, we show that in any closed Riemannian manifold of dimension at least $3$ and at most $7$, there exist infinitely many smoothly embedded closed minimal hypersurfaces. It proves a conjecture of S.-T. Yau. This paper builds on the methods developed by F. C. Marques and A. Neves.""","""minimal surface"", ""min-max theory"", ""widths""","AOM","""Existence of infinitely many minimal hypersurfaces in closed manifolds""""Using min-max theory, we show that in any closed Riemannian manifold of dimension at least $3$ and at most $7$, there exist infinitely many smoothly embedded closed minimal hypersurfaces. It proves a conjecture of S.-T. Yau. This paper builds on the methods developed by F. C. Marques and A. Neves.""""minimal surface"", ""min-max theory"", ""widths"""
"""10.4007/annals.2023.197.3.2""","""Potential automorphy over CM fields""","""Let $F$ be a CM number field. We prove modularity lifting theorems for regular $n$-dimensional Galois representations over $F$ without any self-duality condition. We deduce that all elliptic curves $E$ over $F$ are potentially modular, and furthermore satisfy the Sato--Tate conjecture. As an application of a different sort, we also prove the Ramanujan Conjecture for weight zero cuspidal automorphic representations for $\\mathrm{GL}_2(\\mathbb{A}_F)$.""","""automorphic forms"", ""Galois representations""","AOM","""Potential automorphy over CM fields""""Let $F$ be a CM number field. We prove modularity lifting theorems for regular $n$-dimensional Galois representations over $F$ without any self-duality condition. We deduce that all elliptic curves $E$ over $F$ are potentially modular, and furthermore satisfy the Sato--Tate conjecture. As an application of a different sort, we also prove the Ramanujan Conjecture for weight zero cuspidal automorphic representations for $\\mathrm{GL}_2(\\mathbb{A}_F)$.""""automorphic forms"", ""Galois representations"""
"""10.4007/annals.2023.197.3.4""","""Stable minimal hypersurfaces in ℝ^N+1+ℓ with singular set an arbitrary closed K⊂{0}×ℝ^ℓ""","""With respect to a $C^\\infty$ metric which is close to the standard Euclidean metric on $\\mathbb{R}^{N+1+\\ell}$, where $N\\ge 7$ and $\\ell\\ge 1$ are given, we construct a class of embedded $(N+\\ell)$-dimensional hypersurfaces (without boundary) which are minimal and strictly stable, and which have singular set equal to an arbitrary preassigned closed subset $K\\subset \\{0\\}\\times \\mathbb{R}^{\\ell}$. Thus the question is settled, with a strong affirmative, as to whether there can be \"" gaps\"" or even fractional dimensional parts in the singular set. Such questions, for both stable and unstable minimal submanifolds, remain open in all dimensions in the case of real analytic metrics and, in particular, for the standard Euclidean metric. The construction used here involves the analysis of solutions $u$ of the symmetric minimal surface equation on domains $\\Omega\\subset\\mathbb{R}^{n}$ whose symmetric graphs (i.e., $\\{(x,\\xi)\\in\\Omega\\times \\mathbb{R}^{m}: |\\xi| = u(x)\\}$) lie on one side of a cylindrical minimal cone including, in particular, a Liouville type theorem for complete solutions (i.e., the case $\\Omega=\\mathbb{R}^{n}$).""","""minimal submanifold"", ""singular set"", ""strict stability""","AOM","""Stable minimal hypersurfaces in ℝ^N+1+ℓ with singular set an arbitrary closed K⊂{0}×ℝ^ℓ""""With respect to a $C^\\infty$ metric which is close to the standard Euclidean metric on $\\mathbb{R}^{N+1+\\ell}$, where $N\\ge 7$ and $\\ell\\ge 1$ are given, we construct a class of embedded $(N+\\ell)$-dimensional hypersurfaces (without boundary) which are minimal and strictly stable, and which have singular set equal to an arbitrary preassigned closed subset $K\\subset \\{0\\}\\times \\mathbb{R}^{\\ell}$. Thus the question is settled, with a strong affirmative, as to whether there can be \"" gaps\"" or even fractional dimensional parts in the singular set. Such questions, for both stable and unstable minimal submanifolds, remain open in all dimensions in the case of real analytic metrics and, in particular, for the standard Euclidean metric. The construction used here involves the analysis of solutions $u$ of the symmetric minimal surface equation on domains $\\Omega\\subset\\mathbb{R}^{n}$ whose symmetric graphs (i.e., $\\{(x,\\xi)\\in\\Omega\\times \\mathbb{R}^{m}: |\\xi| = u(x)\\}$) lie on one side of a cylindrical minimal cone including, in particular, a Liouville type theorem for complete solutions (i.e., the case $\\Omega=\\mathbb{R}^{n}$).""""minimal submanifold"", ""singular set"", ""strict stability"""
"""10.4007/annals.2023.197.3.5""","""On Frobenius exact symmetric tensor categories""","""A fundamental theorem of P. Deligne (2002) states that a pre-Tannakian category over an algebraically closed field of characteristic zero admits a fiber functor to the category of supervector spaces (i.e., is the representation category of an affine proalgebraic supergroup) if and only if it has moderate growth (i.e., the lengths of tensor powers of an object grow at most exponentially). In this paper we prove a characteristic $p$ version of this theorem. Namely, we show that a pre-Tannakian category over an algebraically closed field of characteristic $p>0$ admits a fiber functor into the Verlinde category $\\mathrm{Ver}_p$ (i.e., is the representation category of an affine group scheme in $\\mathrm{Ver}_p$) if and only if it has moderate growth and is Frobenius exact. This implies that Frobenius exact pre-Tannakian categories of moderate growth admit a well behaved notion of Frobenius-Perron dimension. It follows that any semisimple pre-Tannakian category of moderategrowth has a fiber functor to $\\mathrm{Ver}_p$ (so in particular Deligne's theorem holds on the nose for semisimple pre-Tannakian categories in characteristics $2$,$3$). This settles a conjecture of the third author from 2015. In particular, this result applies to semisimplifications of categories of modular representations of finite groups (or, more generally, affine group schemes), which gives new applications to classical modular representation theory. For example, it allows us to characterize, for a modular representation $V$, the possible growth rates of the number of indecomposable summands in $V^{\\otimes n}$of dimension prime to $p$.""","""Frobenius functor"", ""modular representations"", ""tensor categories"", ""Verlinde category""","AOM","""On Frobenius exact symmetric tensor categories""""A fundamental theorem of P. Deligne (2002) states that a pre-Tannakian category over an algebraically closed field of characteristic zero admits a fiber functor to the category of supervector spaces (i.e., is the representation category of an affine proalgebraic supergroup) if and only if it has moderate growth (i.e., the lengths of tensor powers of an object grow at most exponentially). In this paper we prove a characteristic $p$ version of this theorem. Namely, we show that a pre-Tannakian category over an algebraically closed field of characteristic $p>0$ admits a fiber functor into the Verlinde category $\\mathrm{Ver}_p$ (i.e., is the representation category of an affine group scheme in $\\mathrm{Ver}_p$) if and only if it has moderate growth and is Frobenius exact. This implies that Frobenius exact pre-Tannakian categories of moderate growth admit a well behaved notion of Frobenius-Perron dimension. It follows that any semisimple pre-Tannakian category of moderategrowth has a fiber functor to $\\mathrm{Ver}_p$ (so in particular Deligne's theorem holds on the nose for semisimple pre-Tannakian categories in characteristics $2$,$3$). This settles a conjecture of the third author from 2015. In particular, this result applies to semisimplifications of categories of modular representations of finite groups (or, more generally, affine group schemes), which gives new applications to classical modular representation theory. For example, it allows us to characterize, for a modular representation $V$, the possible growth rates of the number of indecomposable summands in $V^{\\otimes n}$of dimension prime to $p$.""""Frobenius functor"", ""modular representations"", ""tensor categories"", ""Verlinde category"""
"""10.4007/annals.2023.197.3.3""","""The Hasse principle for random Fano hypersurfaces""","""It is known that the Brauer--Manin obstruction to the Hasse principle is vacuous for smooth Fano hypersurfaces of dimension at least $3$ over any number field. Moreover, for such varieties it follows from a general conjecture of Colliot-Thélène that the Brauer--Manin obstruction to the Hasse principle should be the only one, so that the Hasse principle is expected to hold. Working over the field of rational numbers and ordering Fano hypersurfaces of fixed degree and dimension by height, we prove that almost every such hypersurface satisfies the Hasse principle provided that the dimension is at least $3$. This proves a conjecture of Poonen and Voloch in every case except for cubic surfaces.""","""Fano hypersurfaces"", ""Hasse principle"", ""heights"", ""rational points""","AOM","""The Hasse principle for random Fano hypersurfaces""""It is known that the Brauer--Manin obstruction to the Hasse principle is vacuous for smooth Fano hypersurfaces of dimension at least $3$ over any number field. Moreover, for such varieties it follows from a general conjecture of Colliot-Thélène that the Brauer--Manin obstruction to the Hasse principle should be the only one, so that the Hasse principle is expected to hold. Working over the field of rational numbers and ordering Fano hypersurfaces of fixed degree and dimension by height, we prove that almost every such hypersurface satisfies the Hasse principle provided that the dimension is at least $3$. This proves a conjecture of Poonen and Voloch in every case except for cubic surfaces.""""Fano hypersurfaces"", ""Hasse principle"", ""heights"", ""rational points"""
"""10.4007/annals.2022.196.3.1""","""Zimmer's conjecture: Subexponential growth","""We prove several cases of Zimmer's conjecture for actions of higher-rank, cocompact lattices on low-dimensional manifolds. For example, if $\\Gamma$ is a cocompact lattice in $\\mathrm{SL}(n, \\mathbb{R})$, $M$ is a compact manifold, and $\\omega$ a volume form on $M$, we show that any homomorphism $\\alpha\\colon \\Gamma \\rightarrow \\mathrm{Diff}(M)$ has finite image if the dimension of $M$ is less than $n-1$ and that any homomorphism $\\alpha\\colon \\Gamma \\rightarrow\\mathrm{Diff}(M,\\omega)$ has finite image if the dimension of $M$ is less than $n$. The key step in the proof is to show that any such action has uniform subexponential growth of derivatives. This is established using ideas from the smooth ergodic theory of higher-rank abelian groups, structure theory of semisimple groups, and results from homogeneous dynamics. Having established uniform subexponential growth of derivatives, we apply Lafforgue's strong property (T) to establish the existence of an invariant Riemannian metric.""","""actions of abelian groups"", ""actions of lattices"", ""lattices in semisimple Lie groups"", ""Lyapunov exponents"", ""measure rigidity"", ""property(T)"", ""Ratner theory"", ""Zimmer program""","AOM","""Zimmer's conjecture: Subexponential growth""We prove several cases of Zimmer's conjecture for actions of higher-rank, cocompact lattices on low-dimensional manifolds. For example, if $\\Gamma$ is a cocompact lattice in $\\mathrm{SL}(n, \\mathbb{R})$, $M$ is a compact manifold, and $\\omega$ a volume form on $M$, we show that any homomorphism $\\alpha\\colon \\Gamma \\rightarrow \\mathrm{Diff}(M)$ has finite image if the dimension of $M$ is less than $n-1$ and that any homomorphism $\\alpha\\colon \\Gamma \\rightarrow\\mathrm{Diff}(M,\\omega)$ has finite image if the dimension of $M$ is less than $n$. The key step in the proof is to show that any such action has uniform subexponential growth of derivatives. This is established using ideas from the smooth ergodic theory of higher-rank abelian groups, structure theory of semisimple groups, and results from homogeneous dynamics. Having established uniform subexponential growth of derivatives, we apply Lafforgue's strong property (T) to establish the existence of an invariant Riemannian metric.""""actions of abelian groups"", ""actions of lattices"", ""lattices in semisimple Lie groups"", ""Lyapunov exponents"", ""measure rigidity"", ""property(T)"", ""Ratner theory"", ""Zimmer program"""
"""10.4007/annals.2022.196.3.3""","""Universal optimality of the $E_8$ and Leech lattices and interpolation formulas""","""We prove that the $E_8$ root lattice and the Leech lattice are universally optimal among point configurations in Euclidean spaces of dimensions eight and twenty-four, respectively. In other words, they minimize energy for every potential function that is a completely monotonic function of squared distance (for example, inverse power laws or Gaussians), which is a strong form of robustness not previously known for any configuration in more than one dimension. This theorem implies their recently shown optimality as sphere packings, and broadly generalizes it to allow for long-range interactions. The proof uses sharp linear programming bounds for energy. To construct the optimal auxiliary functions used to attain these bounds, we prove a new interpolation theorem, which is of independent interest. It reconstructs a radial Schwartz function $f$ from the values and radial derivatives of $f$ and its Fourier transform $\\hat{f}$ at the radii $\\sqrt{2n}$ for integers $n\\ge 1$ in $\\mathbb{R}^8$ and $n\\ge 2$ in $\\mathbb{R}^{24}$. To prove this theorem, we construct an interpolation basis using integral transforms of quasimodular forms, generalizing Viazovska's work on sphere packing and placing it in the context of a more conceptual theory.""","""energy minimization"", ""Fourier interpolation"", ""modular forms"", ""universal optimality""","AOM","""Universal optimality of the $E_8$ and Leech lattices and interpolation formulas""""We prove that the $E_8$ root lattice and the Leech lattice are universally optimal among point configurations in Euclidean spaces of dimensions eight and twenty-four, respectively. In other words, they minimize energy for every potential function that is a completely monotonic function of squared distance (for example, inverse power laws or Gaussians), which is a strong form of robustness not previously known for any configuration in more than one dimension. This theorem implies their recently shown optimality as sphere packings, and broadly generalizes it to allow for long-range interactions. The proof uses sharp linear programming bounds for energy. To construct the optimal auxiliary functions used to attain these bounds, we prove a new interpolation theorem, which is of independent interest. It reconstructs a radial Schwartz function $f$ from the values and radial derivatives of $f$ and its Fourier transform $\\hat{f}$ at the radii $\\sqrt{2n}$ for integers $n\\ge 1$ in $\\mathbb{R}^8$ and $n\\ge 2$ in $\\mathbb{R}^{24}$. To prove this theorem, we construct an interpolation basis using integral transforms of quasimodular forms, generalizing Viazovska's work on sphere packing and placing it in the context of a more conceptual theory.""""energy minimization"", ""Fourier interpolation"", ""modular forms"", ""universal optimality"""
"""10.4007/annals.2022.196.3.5""","""Prisms and prismatic cohomology""","""We introduce the notion of a prism, which may be regarded as a \""deperfection\"" of the notion of a perfectoid ring. Using prisms, we attach a ringed site --- the prismatic site --- to a $p$-adic formal scheme. The resulting cohomology theory specializes to (and often refines) most known integral $p$-adic cohomology theories. As applications, we prove an improved version of the almost purity theorem allowing ramification along arbitrary closed subsets (without using adic spaces), give a co-ordinate free description of $q$-de Rham cohomology as conjectured by the second author, and settle a vanishing conjecture for the $p$-adic Tate twists $\\mathbf{Z}_p(n)$ introduced in our previous joint work with Morrow.""","""$p$-adic cohomology"", ""$p$-adic Hodge theory"", ""Crystalline cohomology"", ""de Rham cohomology"", ""étale cohomology""","AOM","""Prisms and prismatic cohomology""""We introduce the notion of a prism, which may be regarded as a \""deperfection\"" of the notion of a perfectoid ring. Using prisms, we attach a ringed site --- the prismatic site --- to a $p$-adic formal scheme. The resulting cohomology theory specializes to (and often refines) most known integral $p$-adic cohomology theories. As applications, we prove an improved version of the almost purity theorem allowing ramification along arbitrary closed subsets (without using adic spaces), give a co-ordinate free description of $q$-de Rham cohomology as conjectured by the second author, and settle a vanishing conjecture for the $p$-adic Tate twists $\\mathbf{Z}_p(n)$ introduced in our previous joint work with Morrow.""""$p$-adic cohomology"", ""$p$-adic Hodge theory"", ""Crystalline cohomology"", ""de Rham cohomology"", ""étale cohomology"""
"""10.4007/annals.2022.196.3.2""","""Invariant measures and measurable projective factors for actions of higher-rank lattices on manifolds""","""We consider smooth actions of lattices in higher-rank semisimple Lie groups on manifolds. We define two numbers $r(G)$ and $m(G)$ associated with the roots system of the Lie algebra of a Lie group $G$. If the dimension of the manifold is smaller than $r(G)$, then we show the action preserves a Borel probability measure. If the dimension of the manifold is at most $m(G)$, we show there is a quasi-invariant measure on the manifold such that the action is measurably isomorphic to a relatively measure-preserving action over a standard boundary action.""","""Invariant measures"", ""lattice actions"", ""Lyapunov exponents"", ""Zimmer program""","AOM","""Invariant measures and measurable projective factors for actions of higher-rank lattices on manifolds""""We consider smooth actions of lattices in higher-rank semisimple Lie groups on manifolds. We define two numbers $r(G)$ and $m(G)$ associated with the roots system of the Lie algebra of a Lie group $G$. If the dimension of the manifold is smaller than $r(G)$, then we show the action preserves a Borel probability measure. If the dimension of the manifold is at most $m(G)$, we show there is a quasi-invariant measure on the manifold such that the action is measurably isomorphic to a relatively measure-preserving action over a standard boundary action.""""Invariant measures"", ""lattice actions"", ""Lyapunov exponents"", ""Zimmer program"""
"""10.4007/annals.2022.196.3.4""","""One can hear the shape of ellipses of small eccentricity""","""We show that if the eccentricity of an ellipse is sufficiently small, then up to isometries it is spectrally unique among all smooth domains. We do not assume any symmetry, convexity, or closeness to the ellipse, on the class of domains. In the course of the proof we also show that for nearly circular domains, the lengths of periodic orbits that are shorter than the perimeter of the domain must belong to the singular support of the wave trace. As a result we also obtain a Laplace spectral rigidity result for the class of axially symmetric nearly circular domains using a similar result of De Simoi, Kaloshin, and Wei concerning the length spectrum of such domains.""","""ellipses"", ""inverse spectral problem"", ""isospectral"", ""periodic orbits"", ""wave trace""","AOM","""One can hear the shape of ellipses of small eccentricity""""We show that if the eccentricity of an ellipse is sufficiently small, then up to isometries it is spectrally unique among all smooth domains. We do not assume any symmetry, convexity, or closeness to the ellipse, on the class of domains. In the course of the proof we also show that for nearly circular domains, the lengths of periodic orbits that are shorter than the perimeter of the domain must belong to the singular support of the wave trace. As a result we also obtain a Laplace spectral rigidity result for the class of axially symmetric nearly circular domains using a similar result of De Simoi, Kaloshin, and Wei concerning the length spectrum of such domains.""""ellipses"", ""inverse spectral problem"", ""isospectral"", ""periodic orbits"", ""wave trace"""
"""10.4007/annals.2022.196.2.2""","""Finite generation for valuations computing stability thresholds and applications to K-stability""","""We prove that on any log Fano pair of dimension $n$ whose stability threshold is less than $\\frac{n+1}{n}$, any valuation computing the stability threshold has a finitely generated associated graded ring. Together with earlier works, this implies that (a) a log Fano pair is uniformly K-stable (resp. reduced uniformly K-stable) if and only if it is K-stable (resp. K-polystable); (b) the K-moduli spaces are proper and projective; and combining with the previously known equivalence between the existence of Kähler-Einstein metric and reduced uniform K-stability proved by the variational approach, (c) the Yau-Tian-Donaldson conjecture holds for general (possibly singular) log Fano pairs.""","""{K}-moduli"", ""{K}-stability"", ""Fano variety"", ""Higher Rank Finite Generation"", ""K   hler--Einstein metric""","AOM","""Finite generation for valuations computing stability thresholds and applications to K-stability""""We prove that on any log Fano pair of dimension $n$ whose stability threshold is less than $\\frac{n+1}{n}$, any valuation computing the stability threshold has a finitely generated associated graded ring. Together with earlier works, this implies that (a) a log Fano pair is uniformly K-stable (resp. reduced uniformly K-stable) if and only if it is K-stable (resp. K-polystable); (b) the K-moduli spaces are proper and projective; and combining with the previously known equivalence between the existence of Kähler-Einstein metric and reduced uniform K-stability proved by the variational approach, (c) the Yau-Tian-Donaldson conjecture holds for general (possibly singular) log Fano pairs.""""{K}-moduli"", ""{K}-stability"", ""Fano variety"", ""Higher Rank Finite Generation"", ""K   hler--Einstein metric"""
"""10.4007/annals.2022.196.2.3""","""On the implosion of a compressible fluid I: Smooth self-similar inviscid profiles""","""In this paper and its sequel, we construct a set of finite energy smooth initial data for which the corresponding solutions to the compressible three-dimensional Navier-Stokes and Euler equations implode (with infinite density) at a later time at a point, and we completely describe the associated formation of singularity. This paper is concerned with existence of smooth self-similar profiles for the barotropic Euler equations in dimension $d\\ge 2$ with decaying density at spatial infinity. The phase portrait of the nonlinear ODE governing the equation for spherically symmetric self-similar solutions has been introduced in the pioneering work of Guderley. It allows us to construct global profiles of the self-similar problem, which however turn out to be generically non-smooth across the associated acoustic cone. In a suitable range of barotropic laws and for a sequence of quantized speeds accumulating to a critical value, we prove the existence of non-generic $\\mathcal{C}^\\infty$ self-similar solutions with suitable decay at infinity. The $\\mathcal{C}^\\infty$ regularity is used in a fundamental way in our companion paper (part II) in the analysis of the associated linearized operator and leads, in turn, to the construction of finite energy blow up solutions of the compressible Euler and Navier-Stokes equations in dimensions $d=2,3$.""","""Euler equations"", ""self-similar profile""","AOM","""On the implosion of a compressible fluid I: Smooth self-similar inviscid profiles""""In this paper and its sequel, we construct a set of finite energy smooth initial data for which the corresponding solutions to the compressible three-dimensional Navier-Stokes and Euler equations implode (with infinite density) at a later time at a point, and we completely describe the associated formation of singularity. This paper is concerned with existence of smooth self-similar profiles for the barotropic Euler equations in dimension $d\\ge 2$ with decaying density at spatial infinity. The phase portrait of the nonlinear ODE governing the equation for spherically symmetric self-similar solutions has been introduced in the pioneering work of Guderley. It allows us to construct global profiles of the self-similar problem, which however turn out to be generically non-smooth across the associated acoustic cone. In a suitable range of barotropic laws and for a sequence of quantized speeds accumulating to a critical value, we prove the existence of non-generic $\\mathcal{C}^\\infty$ self-similar solutions with suitable decay at infinity. The $\\mathcal{C}^\\infty$ regularity is used in a fundamental way in our companion paper (part II) in the analysis of the associated linearized operator and leads, in turn, to the construction of finite energy blow up solutions of the compressible Euler and Navier-Stokes equations in dimensions $d=2,3$.""""Euler equations"", ""self-similar profile"""
"""10.4007/annals.2022.196.2.4""","""On the implosion of a compressible fluid II: Singularity formation""","""In this paper, which continues our investigation of strong singularity formation in compressible fluids, we consider the compressible three-dimensional Navier-Stokes and Euler equations. In a suitable regime of barotropic laws, we construct a set of finite energy smooth initial data for which the corresponding solutions to both equations implode (with infinite density) at a later time at a point, and completely describe the associated formation of singularity. An essential step in the proof is the existence of $\\mathcal{C}^\\infty$ smooth self-similar solutions to the compressible Euler equations for quantized values of the speed constructed in our companion paper (part I). All blow up dynamics obtained for the Navier-Stokes problem are of type II (non self-similar).""","""compressible fluids"", ""singularity formation""","AOM","""On the implosion of a compressible fluid II: Singularity formation""""In this paper, which continues our investigation of strong singularity formation in compressible fluids, we consider the compressible three-dimensional Navier-Stokes and Euler equations. In a suitable regime of barotropic laws, we construct a set of finite energy smooth initial data for which the corresponding solutions to both equations implode (with infinite density) at a later time at a point, and completely describe the associated formation of singularity. An essential step in the proof is the existence of $\\mathcal{C}^\\infty$ smooth self-similar solutions to the compressible Euler equations for quantized values of the speed constructed in our companion paper (part I). All blow up dynamics obtained for the Navier-Stokes problem are of type II (non self-similar).""""compressible fluids"", ""singularity formation"""
"""10.4007/annals.2022.196.2.1""","""On the Chowla and twin primes conjectures over $\\mathbb{F}_q[T]$""","""Using geometric methods, we improve on the function field version of the Burgess bound and show that, when restricted to certain special subspaces, the Möbius function over $\\mathbb{F}_q[T]$ can be mimicked by Dirichlet characters. Combining these, we obtain a level of distribution close to $1$ for the Möbius function in arithmetic progressions and resolve Chowla's $k$-point correlation conjecture with large uniformity in the shifts. Using a function field variant of a result by Fouvry-Michel on exponential sums involving the Möbius function, we obtain a level of distribution beyond $1/2$ for irreducible polynomials, and establish the twin prime conjecture in a quantitative form. All these results hold for finite fields satisfying a simple condition.""","""level of distribution for irreducible polynomials"", ""parity barrier over function fields"", ""short character sums"", ""twin irreducible polynomials""","AOM","""On the Chowla and twin primes conjectures over $\\mathbb{F}_q[T]$""""Using geometric methods, we improve on the function field version of the Burgess bound and show that, when restricted to certain special subspaces, the Möbius function over $\\mathbb{F}_q[T]$ can be mimicked by Dirichlet characters. Combining these, we obtain a level of distribution close to $1$ for the Möbius function in arithmetic progressions and resolve Chowla's $k$-point correlation conjecture with large uniformity in the shifts. Using a function field variant of a result by Fouvry-Michel on exponential sums involving the Möbius function, we obtain a level of distribution beyond $1/2$ for irreducible polynomials, and establish the twin prime conjecture in a quantitative form. All these results hold for finite fields satisfying a simple condition.""""level of distribution for irreducible polynomials"", ""parity barrier over function fields"", ""short character sums"", ""twin irreducible polynomials"""
"""10.4007/annals.2022.196.1.1""","""Proof of the satisfiability conjecture for large $k$""","""We establish the satisfiability threshold for random $k$-SAT for all $k\\ge k_0$, with $k_0$ an absolute constant. That is, there exists a limiting density $\\alpha_{\\rm sat}(k)$ such that a random $k$-SAT formula of clause density $\\alpha$ is with high probability satisfiable for $\\alpha\\lt \\alpha_{\\rm sat}$, and unsatisfiable for $\\alpha>\\alpha_{\\rm sat}$. We show that the threshold $\\alpha_{\\rm sat}(k)$ is given explicitly by the one-step replica symmetry breaking prediction from statistical physics. The proof develops a new analytic method for moment calculations on random graphs, mapping a high-dimensional optimization problem to a more tractable problem of analyzing tree recursions. We believe that our method may apply to a range of random CSPs in the $1$-RSB universality class.""","""boolean satisfiability"", ""Random constraint satisfaction problem"", ""replica symmetry breaking"", ""Spin glasses"", ""warning propagation""","AOM","""Proof of the satisfiability conjecture for large $k$""""We establish the satisfiability threshold for random $k$-SAT for all $k\\ge k_0$, with $k_0$ an absolute constant. That is, there exists a limiting density $\\alpha_{\\rm sat}(k)$ such that a random $k$-SAT formula of clause density $\\alpha$ is with high probability satisfiable for $\\alpha\\lt \\alpha_{\\rm sat}$, and unsatisfiable for $\\alpha>\\alpha_{\\rm sat}$. We show that the threshold $\\alpha_{\\rm sat}(k)$ is given explicitly by the one-step replica symmetry breaking prediction from statistical physics. The proof develops a new analytic method for moment calculations on random graphs, mapping a high-dimensional optimization problem to a more tractable problem of analyzing tree recursions. We believe that our method may apply to a range of random CSPs in the $1$-RSB universality class.""""boolean satisfiability"", ""Random constraint satisfaction problem"", ""replica symmetry breaking"", ""Spin glasses"", ""warning propagation"""
"""10.4007/annals.2022.196.1.2""","""The Birkhoff-Poritsky conjecture for centrally-symmetric billiard tables""","""In this paper we prove the Birkhoff-Poritsky conjecture for centrally-symmetric $C^2$-smooth convex planar billiards. We assume that the domain $\\mathcal{A}$ between the invariant curve of $4$-periodic orbits and the boundary of the phase cylinder is foliated by $C^0$-invariant curves. Under this assumption we prove that the billiard curve is an ellipse. For the original Birkhoff-Poritsky formulation we show that if a neighborhood of the boundary of billiard domain has a $C^1$-smooth foliation by convex caustics of rotation numbers in the interval $(0; 1/4]$, then the boundary curve is an ellipse. In the language of first integrals one can assert that if the billiard inside a centrally-symmetric $C^2$-smooth convex curve $\\gamma$ admits a $C^1$-smooth first integral with non-vanishing gradient on $\\mathcal{A}$, then the curve $\\gamma$ is an ellipse. The main ingredients of the proof are (1) the non-standard generating function for convex billiards; (2) the remarkable structure of the invariant curve consisting of $4$-periodic orbits; and (3) the integral-geometry approach for rigidity results that was invented by the first named author for circular billiards. Surprisingly, we establish a Hopf-type rigidity for billiard in ellipse.""","""Birkhoff billiard"", ""Birkhoff-Poritsky conjecture"", ""integrable billiard""","AOM","""The Birkhoff-Poritsky conjecture for centrally-symmetric billiard tables""""In this paper we prove the Birkhoff-Poritsky conjecture for centrally-symmetric $C^2$-smooth convex planar billiards. We assume that the domain $\\mathcal{A}$ between the invariant curve of $4$-periodic orbits and the boundary of the phase cylinder is foliated by $C^0$-invariant curves. Under this assumption we prove that the billiard curve is an ellipse. For the original Birkhoff-Poritsky formulation we show that if a neighborhood of the boundary of billiard domain has a $C^1$-smooth foliation by convex caustics of rotation numbers in the interval $(0; 1/4]$, then the boundary curve is an ellipse. In the language of first integrals one can assert that if the billiard inside a centrally-symmetric $C^2$-smooth convex curve $\\gamma$ admits a $C^1$-smooth first integral with non-vanishing gradient on $\\mathcal{A}$, then the curve $\\gamma$ is an ellipse. The main ingredients of the proof are (1) the non-standard generating function for convex billiards; (2) the remarkable structure of the invariant curve consisting of $4$-periodic orbits; and (3) the integral-geometry approach for rigidity results that was invented by the first named author for circular billiards. Surprisingly, we establish a Hopf-type rigidity for billiard in ellipse.""""Birkhoff billiard"", ""Birkhoff-Poritsky conjecture"", ""integrable billiard"""
"""10.4007/annals.2022.196.1.3""","""Non-uniqueness of Leray solutions of the forced Navier-Stokes equations""","""In a seminal work, Leray (1934) demonstrated the existence of global weak solutions to the Navier-Stokes equations in three dimensions. We exhibit two distinct Leray solutions with zero initial velocity and identical body force. Our approach is to construct a ``background\"" solution which is unstable for the Navier-Stokes dynamics in similarity variables; its similarity profile is a smooth, compactly supported vortex ring whose cross-section is a modification of the unstable two-dimensional vortex constructed by Vishik (2018). The second solution is a trajectory on the unstable manifold associated to the background solution, in accordance with the predictions of Jia and Šverák (2015). Our solutions live precisely on the borderline of the known well-posedness theory.""","""Leray-Hopf solutions"", ""Navier-Stokes equations"", ""non-uniqueness""","AOM","""Non-uniqueness of Leray solutions of the forced Navier-Stokes equations""""In a seminal work, Leray (1934) demonstrated the existence of global weak solutions to the Navier-Stokes equations in three dimensions. We exhibit two distinct Leray solutions with zero initial velocity and identical body force. Our approach is to construct a ``background\"" solution which is unstable for the Navier-Stokes dynamics in similarity variables; its similarity profile is a smooth, compactly supported vortex ring whose cross-section is a modification of the unstable two-dimensional vortex constructed by Vishik (2018). The second solution is a trajectory on the unstable manifold associated to the background solution, in accordance with the predictions of Jia and Šverák (2015). Our solutions live precisely on the borderline of the known well-posedness theory.""""Leray-Hopf solutions"", ""Navier-Stokes equations"", ""non-uniqueness"""
"""10.4007/annals.2022.195.3.1""","""On the Hofer-Zehnder conjecture""","""We prove that if a Hamiltonian diffeomorphism of a closed monotone symplectic manifold with semisimple quantum homology has more contractible fixed points, counted homologically, than the total dimension of the homology of the manifold, then it must have an infinite number of contractible periodic points. This constitutes a higher-dimensional homological generalization of a celebrated result of Franks from 1992, as conjectured by Hofer and Zehnder in 1994.""","""barcodes"", ""equivariant cohomology"", ""Floer homology"", ""hamiltonian diffeomorphisms"", ""Periodic points"", ""persistence modules"", ""Smith theory""","AOM","""On the Hofer-Zehnder conjecture""""We prove that if a Hamiltonian diffeomorphism of a closed monotone symplectic manifold with semisimple quantum homology has more contractible fixed points, counted homologically, than the total dimension of the homology of the manifold, then it must have an infinite number of contractible periodic points. This constitutes a higher-dimensional homological generalization of a celebrated result of Franks from 1992, as conjectured by Hofer and Zehnder in 1994.""""barcodes"", ""equivariant cohomology"", ""Floer homology"", ""hamiltonian diffeomorphisms"", ""Periodic points"", ""persistence modules"", ""Smith theory"""
"""10.4007/annals.2022.195.3.3""","""Rigid local systems and the multiplicative eigenvalue problem""","""We give a construction that produces irreducible complex rigid local systems on $\\mathbb{P}_C^1-\\{p_1,\\ldots ,p_s\\}$ via quantum Schubert calculus and strange duality. These local systems are unitary and arise from a study of vertices in the polytopes controlling the multiplicative eigenvalue problem for the special unitary groups $\\mathrm{SU}(n)$ (i.e., determination of the possible eigenvalues of a product of unitary matrices given the eigenvalues of the matrices). Roughly speaking, we show that the strange duals of the simplest vertices of these polytopes give all possible unitary irreducible rigid local systems. As a consequence we obtain that the ranks of unitary irreducible rigid local systems, including those with finite global monodromy, on $\\mathbb{P}^1-S$ are bounded above if we fix the cardinality of the set $S=\\{p_1,\\ldots ,p_s\\}$ and require that the local monodromies have orders that divide $n$ for a fixed $n$. Answering a question of N.~Katz, we show that there are no irreducible rigid local systems of rank greater than one, with finite global monodromy, all of whose local monodromies have orders dividing $n$, when $n$ is a prime number. We also show that all unitary irreducible rigid local systems on $\\mathbb{P}_C^1-S$ with finite local monodromies arise as solutions to the Knizhnik-Zamalodchikov equations on conformal blocks for the special linear group. Along the way, generalizing previous works of the author and J.~Kiers, we give an inductive mechanism for determining all vertices in the multiplicative eigenvalue problem for $\\mathrm{SU}(n)$.""","""KZ equations"", ""multiplicative eigenvalue problem"", ""quantum Schubert calculus"", ""rigid local systems"", ""strange duality""","AOM","""Rigid local systems and the multiplicative eigenvalue problem""""We give a construction that produces irreducible complex rigid local systems on $\\mathbb{P}_C^1-\\{p_1,\\ldots ,p_s\\}$ via quantum Schubert calculus and strange duality. These local systems are unitary and arise from a study of vertices in the polytopes controlling the multiplicative eigenvalue problem for the special unitary groups $\\mathrm{SU}(n)$ (i.e., determination of the possible eigenvalues of a product of unitary matrices given the eigenvalues of the matrices). Roughly speaking, we show that the strange duals of the simplest vertices of these polytopes give all possible unitary irreducible rigid local systems. As a consequence we obtain that the ranks of unitary irreducible rigid local systems, including those with finite global monodromy, on $\\mathbb{P}^1-S$ are bounded above if we fix the cardinality of the set $S=\\{p_1,\\ldots ,p_s\\}$ and require that the local monodromies have orders that divide $n$ for a fixed $n$. Answering a question of N.~Katz, we show that there are no irreducible rigid local systems of rank greater than one, with finite global monodromy, all of whose local monodromies have orders dividing $n$, when $n$ is a prime number. We also show that all unitary irreducible rigid local systems on $\\mathbb{P}_C^1-S$ with finite local monodromies arise as solutions to the Knizhnik-Zamalodchikov equations on conformal blocks for the special linear group. Along the way, generalizing previous works of the author and J.~Kiers, we give an inductive mechanism for determining all vertices in the multiplicative eigenvalue problem for $\\mathrm{SU}(n)$.""""KZ equations"", ""multiplicative eigenvalue problem"", ""quantum Schubert calculus"", ""rigid local systems"", ""strange duality"""
"""10.4007/annals.2022.195.3.4""","""Pointwise ergodic theorems for non-conventional bilinear polynomial averages""","""We establish convergence in norm and pointwise almost everywhere for the non-conventional (in the sense of Furstenberg) bilinear polynomial ergodic averages $$ A_N(f,g)(x) := \\frac{1}{N} \\sum_{n=1}^{N} f(T^nx) g(T^{P(n)}x) $$ as $N \\to \\infty$, where $T\\colon X\\to X$ is a measure-preserving transformation of a $\\sigma$-finite measure space $(X,\\mu), P(\\mathrm{n})\\in \\mathbb{Z}[\\mathrm{n}]$ is a polynomial of degree $d \\geq 2$, and $f \\in L^{p_1}(X)$, $g \\in L^{p_2}(X)$ for some $p_1, p_2 > 1$ with $\\frac{1}{p_1} + \\frac{1}{p_2} \\leq 1$. We also establish an $r$-variational inequality for these averages (at lacunary scales) in the optimal range $r > 2$. We are also able to ``break duality\"" by handling some ranges of exponents $p_1$, $p_2$ with $\\frac{1}{p_1} + \\frac{1}{p_2} > 1$, at the cost of increasing $r$ slightly. This gives an affirmative answer to Problem 11 from Frantzikinakis' open problems survey for the Furstenberg--Weiss averages (with $P(\\mathrm{n})=\\mathrm{n}^2)$, which is a bilinear variant of Question 9 considered by Bergelson in his survey on Ergodic Ramsey Theory from 1996. This also gives a contribution to the Furstenberg--Bergelson--Leibman conjecture. Our methods combine techniques from harmonic analysis with the recent inverse theorems of Peluse and Prendiville in additive combinatorics. At large scales, the harmonic analysis of the adelic integers $\\mathbb{A}_{\\mathbb{Z}}$ also plays a role.""","""adelic integres"", ""circle method"", ""Fourier methods"", ""inverse theorems"", ""non-conventional ergodic averages"", ""pointwise convergence"", ""variational estimates""","AOM","""Pointwise ergodic theorems for non-conventional bilinear polynomial averages""""We establish convergence in norm and pointwise almost everywhere for the non-conventional (in the sense of Furstenberg) bilinear polynomial ergodic averages $$ A_N(f,g)(x) := \\frac{1}{N} \\sum_{n=1}^{N} f(T^nx) g(T^{P(n)}x) $$ as $N \\to \\infty$, where $T\\colon X\\to X$ is a measure-preserving transformation of a $\\sigma$-finite measure space $(X,\\mu), P(\\mathrm{n})\\in \\mathbb{Z}[\\mathrm{n}]$ is a polynomial of degree $d \\geq 2$, and $f \\in L^{p_1}(X)$, $g \\in L^{p_2}(X)$ for some $p_1, p_2 > 1$ with $\\frac{1}{p_1} + \\frac{1}{p_2} \\leq 1$. We also establish an $r$-variational inequality for these averages (at lacunary scales) in the optimal range $r > 2$. We are also able to ``break duality\"" by handling some ranges of exponents $p_1$, $p_2$ with $\\frac{1}{p_1} + \\frac{1}{p_2} > 1$, at the cost of increasing $r$ slightly. This gives an affirmative answer to Problem 11 from Frantzikinakis' open problems survey for the Furstenberg--Weiss averages (with $P(\\mathrm{n})=\\mathrm{n}^2)$, which is a bilinear variant of Question 9 considered by Bergelson in his survey on Ergodic Ramsey Theory from 1996. This also gives a contribution to the Furstenberg--Bergelson--Leibman conjecture. Our methods combine techniques from harmonic analysis with the recent inverse theorems of Peluse and Prendiville in additive combinatorics. At large scales, the harmonic analysis of the adelic integers $\\mathbb{A}_{\\mathbb{Z}}$ also plays a role.""""adelic integres"", ""circle method"", ""Fourier methods"", ""inverse theorems"", ""non-conventional ergodic averages"", ""pointwise convergence"", ""variational estimates"""
"""10.4007/annals.2022.195.3.2""","""Global group laws and equivariant bordism rings""","""For every abelian compact Lie group $A$, we prove that the homotopical $A$-equivariant complex bordism ring, introduced by tom Dieck (1970), is isomorphic to the $A$-equivariant Lazard ring, introduced by Cole--Greenlees--Kriz (2000). This settles a conjecture of Greenlees. We also show an analog for homotopical real bordism rings over elementary abelian $2$-groups. Our results generalize classical theorems of Quillen (1969) on the connection between non-equivariant bordism rings and formal group laws, and extend the case $A=C_2$ due to Hanke--Wiemeler (2018). We work in the framework of global homotopy theory, which is essential for our proof. In addition to the statements for a fixed group $A$, we also prove a global algebraic universal property that characterizes the collection of all equivariant complex bordism rings simultaneously. We show that they form the universal contravariant functor from abelian compact Lie groups to commutative rings that is equipped with a coordinate; the coordinate is given by the universal Euler class at the circle group. More generally, the ring of $n$-fold cooperations of equivariant complex bordism is shown to be universal among functors equipped with a strict $n$-tuple of coordinates.""","""bordism"", ""formal groups"", ""global equivariant homotopy theory""","AOM","""Global group laws and equivariant bordism rings""""For every abelian compact Lie group $A$, we prove that the homotopical $A$-equivariant complex bordism ring, introduced by tom Dieck (1970), is isomorphic to the $A$-equivariant Lazard ring, introduced by Cole--Greenlees--Kriz (2000). This settles a conjecture of Greenlees. We also show an analog for homotopical real bordism rings over elementary abelian $2$-groups. Our results generalize classical theorems of Quillen (1969) on the connection between non-equivariant bordism rings and formal group laws, and extend the case $A=C_2$ due to Hanke--Wiemeler (2018). We work in the framework of global homotopy theory, which is essential for our proof. In addition to the statements for a fixed group $A$, we also prove a global algebraic universal property that characterizes the collection of all equivariant complex bordism rings simultaneously. We show that they form the universal contravariant functor from abelian compact Lie groups to commutative rings that is equipped with a coordinate; the coordinate is given by the universal Euler class at the circle group. More generally, the ring of $n$-fold cooperations of equivariant complex bordism is shown to be universal among functors equipped with a strict $n$-tuple of coordinates.""""bordism"", ""formal groups"", ""global equivariant homotopy theory"""
"""10.4007/annals.2022.195.3.5""","""A negative answer to Ulam's Problem 19 from the Scottish Book""","""We give a negative answer to Ulam's Problem 19 from the Scottish Book asking is a solid of uniform density which will float in water in every position a sphere ? Assuming that the density of water is $1$, we show that there exists a strictly convex body of revolution $K\\subset \\mathbb{R}^3$ of uniform density $\\frac{1}{2}$, which is not a Euclidean ball, yet floats in equilibrium in every orientation. We prove an analogous result in all dimensions $d\\ge 3$.""","""convex body"", ""floating body"", ""Ulam's problem""","AOM","""A negative answer to Ulam's Problem 19 from the Scottish Book""""We give a negative answer to Ulam's Problem 19 from the Scottish Book asking is a solid of uniform density which will float in water in every position a sphere ? Assuming that the density of water is $1$, we show that there exists a strictly convex body of revolution $K\\subset \\mathbb{R}^3$ of uniform density $\\frac{1}{2}$, which is not a Euclidean ball, yet floats in equilibrium in every orientation. We prove an analogous result in all dimensions $d\\ge 3$.""""convex body"", ""floating body"", ""Ulam's problem"""
"""10.4007/annals.2022.195.2.4""","""Keel's base point free theorem and quotients in mixed characteristic""","""We develop techniques of mimicking the Frobenius action in the study of universal homeomorphisms in mixed characteristic. As a consequence, we show a mixed characteristic Keel's base point free theorem obtaining applications towards the mixed characteristic Minimal Model Program, we generalise Kollár's theorem on the existence of quotients by finite equivalence relations to mixed characteristic, and we provide a new proof of the existence of quotients by affine group schemes.""","""mixed characteristic"", ""pushouts"", ""quotients"", ""semi-ample"", ""universal homeomorphisms""","AOM","""Keel's base point free theorem and quotients in mixed characteristic""""We develop techniques of mimicking the Frobenius action in the study of universal homeomorphisms in mixed characteristic. As a consequence, we show a mixed characteristic Keel's base point free theorem obtaining applications towards the mixed characteristic Minimal Model Program, we generalise Kollár's theorem on the existence of quotients by finite equivalence relations to mixed characteristic, and we provide a new proof of the existence of quotients by affine group schemes.""""mixed characteristic"", ""pushouts"", ""quotients"", ""semi-ample"", ""universal homeomorphisms"""
"""10.4007/annals.2022.195.2.1""","""The stable Adams conjecture and higher associative structures on Moore spectra""","""In this paper, we provide a new proof of the stable Adams conjecture. Our proof constructs a canonical null-homotopy of the stable J-homomorphism composed with a virtual Adams operation, by applying the K-theory functor to a multinatural transformation. We also point out that the original proof of the stable Adams conjecture is incorrect and present a correction. This correction is crucial to our main application. We settle the question on the height of higher associative structures on the mod $p^k$ Moore spectrum $\\mathrm{M}_p(k)$ at odd primes. More precisely, for any odd prime $p$, we show that $\\mathrm{M}_p(k)$ admits a Thomified $\\mathbb{A}_n$-structure if and only if $n \\lt p^k$. We also prove a weaker result for $p=2$.""","""étale homotopy theory"", ""higher Associative structures"", ""Moore spectra"", ""stable $J$-homomorphism""","AOM","""The stable Adams conjecture and higher associative structures on Moore spectra""""In this paper, we provide a new proof of the stable Adams conjecture. Our proof constructs a canonical null-homotopy of the stable J-homomorphism composed with a virtual Adams operation, by applying the K-theory functor to a multinatural transformation. We also point out that the original proof of the stable Adams conjecture is incorrect and present a correction. This correction is crucial to our main application. We settle the question on the height of higher associative structures on the mod $p^k$ Moore spectrum $\\mathrm{M}_p(k)$ at odd primes. More precisely, for any odd prime $p$, we show that $\\mathrm{M}_p(k)$ admits a Thomified $\\mathbb{A}_n$-structure if and only if $n \\lt p^k$. We also prove a weaker result for $p=2$.""""étale homotopy theory"", ""higher Associative structures"", ""Moore spectra"", ""stable $J$-homomorphism"""
"""10.4007/annals.2022.195.2.3""","""Rough solutions of the 3-D compressible Euler equations""","""We prove the local-in-time well-posedness for the solution of the compressible Euler equations in 3-D for the Cauchy data of the velocity, density and vorticity $(v,\\varrho\\mathfrak{w} \\in H^s \\times H^s \\times H^{s'}$, $2\\lt s'\\lt s$. The result extends the sharp results of Smith-Tataru and of Wang, established in the irrotational case, i.e., $\\mathfrak{w}=0$, which is known to be optimal for $s>2$. At the opposite extreme, in the incompressible case, i.e., with a constant density, the result is known to hold for $\\mathfrak{w}\\in H^s$, $s>3/2$ and fails for $s\\le 3/2$. We therefore, conjecture that the optimal result should be $(v,\\varrho,\\mathfrak{w} \\in H^s\\times H^s\\times H^{s'}$, $s>2$, $s' > \\frac{3}{2}$. We view our work here as an important step in proving the conjecture. The main difficulty in establishing sharp well-posedness results for general compressible Euler flow is due to the highly nontrivial interaction between the sound waves, governed by quasilinear wave equations, and vorticity which is transported by the flow. To overcome this difficulty, we separate the dispersive part of sound wave from the transported part, and gain regularity significantly by exploiting the nonlinear structure of the system and the geometric structures of the acoustical spacetime.""","""compressible Euler equations"", ""local well-posedness"", ""Quasilinear wave equations"", ""Strichartz estimates""","AOM","""Rough solutions of the 3-D compressible Euler equations""""We prove the local-in-time well-posedness for the solution of the compressible Euler equations in 3-D for the Cauchy data of the velocity, density and vorticity $(v,\\varrho\\mathfrak{w} \\in H^s \\times H^s \\times H^{s'}$, $2\\lt s'\\lt s$. The result extends the sharp results of Smith-Tataru and of Wang, established in the irrotational case, i.e., $\\mathfrak{w}=0$, which is known to be optimal for $s>2$. At the opposite extreme, in the incompressible case, i.e., with a constant density, the result is known to hold for $\\mathfrak{w}\\in H^s$, $s>3/2$ and fails for $s\\le 3/2$. We therefore, conjecture that the optimal result should be $(v,\\varrho,\\mathfrak{w} \\in H^s\\times H^s\\times H^{s'}$, $s>2$, $s' > \\frac{3}{2}$. We view our work here as an important step in proving the conjecture. The main difficulty in establishing sharp well-posedness results for general compressible Euler flow is due to the highly nontrivial interaction between the sound waves, governed by quasilinear wave equations, and vorticity which is transported by the flow. To overcome this difficulty, we separate the dispersive part of sound wave from the transported part, and gain regularity significantly by exploiting the nonlinear structure of the system and the geometric structures of the acoustical spacetime.""""compressible Euler equations"", ""local well-posedness"", ""Quasilinear wave equations"", ""Strichartz estimates"""
"""10.1214/23-BA1405""","""Nearest-Neighbor Mixture Models for Non-Gaussian Spatial Processes""","""We develop a class of nearest-neighbor mixture models that provide direct, computationally efficient, probabilistic modeling for non-Gaussian geospatial data. The class is defined over a directed acyclic graph, which implies conditional independence in representing a multivariate distribution through factorization into a product of univariate conditionals, and is extended to a full spatial process. We model each conditional as a mixture of spatially varying transition kernels, with locally adaptive weights, for each one of a given number of nearest neighbors. The modeling framework emphasizes direct spatial modeling of non-Gaussian data, in contrast with approaches that introduce a spatial process for transformed data, or for functionals of the data probability distribution. We study model construction and properties analytically through specification of bivariate distributions that define the local transition kernels. This provides a general strategy for modeling different types of non-Gaussian data based on the bivariate distribution family, and offers avenues to incorporate spatial association via different dependence measures. Regarding computation, direct spatial modeling facilitates efficient, full simulation-based inference; moreover, the framework leverages its mixture model structure to avoid computational issues that arise from large matrix operations, and thus has the potential to achieve scalability. We illustrate the methodology using synthetic data examples and an analysis of Mediterranean Sea surface temperature observations.""","""Bayesian hierarchical models"", ""copulas"", ""Markov chain Monte Carlo"", ""spatial statistics"", ""tail dependence""","BA","""Nearest-Neighbor Mixture Models for Non-Gaussian Spatial Processes""""We develop a class of nearest-neighbor mixture models that provide direct, computationally efficient, probabilistic modeling for non-Gaussian geospatial data. The class is defined over a directed acyclic graph, which implies conditional independence in representing a multivariate distribution through factorization into a product of univariate conditionals, and is extended to a full spatial process. We model each conditional as a mixture of spatially varying transition kernels, with locally adaptive weights, for each one of a given number of nearest neighbors. The modeling framework emphasizes direct spatial modeling of non-Gaussian data, in contrast with approaches that introduce a spatial process for transformed data, or for functionals of the data probability distribution. We study model construction and properties analytically through specification of bivariate distributions that define the local transition kernels. This provides a general strategy for modeling different types of non-Gaussian data based on the bivariate distribution family, and offers avenues to incorporate spatial association via different dependence measures. Regarding computation, direct spatial modeling facilitates efficient, full simulation-based inference; moreover, the framework leverages its mixture model structure to avoid computational issues that arise from large matrix operations, and thus has the potential to achieve scalability. We illustrate the methodology using synthetic data examples and an analysis of Mediterranean Sea surface temperature observations.""""Bayesian hierarchical models"", ""copulas"", ""Markov chain Monte Carlo"", ""spatial statistics"", ""tail dependence"""
"""10.1214/23-BA1410""","""Semiparametric Functional Factor Models with Bayesian Rank Selection""","""Functional data are frequently accompanied by a parametric template that describes the typical shapes of the functions. However, these parametric templates can incur significant bias, which undermines both utility and interpretability. To correct for model misspecification, we augment the parametric template with an infinite-dimensional nonparametric functional basis. The nonparametric basis functions are learned from the data and constrained to be orthogonal to the parametric template, which preserves distinctness between the parametric and nonparametric terms. This distinctness is essential to prevent functional confounding, which otherwise induces severe bias for the parametric terms. The nonparametric factors are regularized with an ordered spike-and-slab prior that provides consistent rank selection and satisfies several appealing theoretical properties. The versatility of the proposed approach is illustrated through applications to synthetic data, human motor control data, and dynamic yield curve data. Relative to parametric and semiparametric alternatives, the proposed semiparametric functional factor model eliminates bias, reduces excessive posterior and predictive uncertainty, and provides reliable inference on the effective number of nonparametric terms—all with minimal additional computational costs.""","""factor analysis"", ""Nonparametric regression"", ""shrinkage prior"", ""spike-and-slab prior"", ""yield curve""","BA","""Semiparametric Functional Factor Models with Bayesian Rank Selection""""Functional data are frequently accompanied by a parametric template that describes the typical shapes of the functions. However, these parametric templates can incur significant bias, which undermines both utility and interpretability. To correct for model misspecification, we augment the parametric template with an infinite-dimensional nonparametric functional basis. The nonparametric basis functions are learned from the data and constrained to be orthogonal to the parametric template, which preserves distinctness between the parametric and nonparametric terms. This distinctness is essential to prevent functional confounding, which otherwise induces severe bias for the parametric terms. The nonparametric factors are regularized with an ordered spike-and-slab prior that provides consistent rank selection and satisfies several appealing theoretical properties. The versatility of the proposed approach is illustrated through applications to synthetic data, human motor control data, and dynamic yield curve data. Relative to parametric and semiparametric alternatives, the proposed semiparametric functional factor model eliminates bias, reduces excessive posterior and predictive uncertainty, and provides reliable inference on the effective number of nonparametric terms—all with minimal additional computational costs.""""factor analysis"", ""Nonparametric regression"", ""shrinkage prior"", ""spike-and-slab prior"", ""yield curve"""
"""10.1214/23-BA1378""","""High-Dimensional Bayesian Network Classification with Network Global-Local Shrinkage Priors""","""This article proposes a novel Bayesian binary classification framework for networks with labeled nodes. Our approach is motivated by applications in brain connectome studies, where the overarching goal is to identify both regions of interest (ROIs) in the brain and connections between ROIs that influence how study subjects are classified. We propose a novel binary logistic regression framework with the network as the predictor, and model the associated network coefficient using a novel class of global-local network shrinkage priors. We perform a theoretical analysis of a member of this class of priors (which we call the Network Lasso Prior) and show asymptotically correct classification of networks even when the number of network edges grows faster than the sample size. Two representative members from this class of priors, the Network Lasso prior and the Network Horseshoe prior, are implemented using an efficient Markov Chain Monte Carlo algorithm, and empirically evaluated through simulation studies and the analysis of a real brain connectome dataset.""","""brain connectome"", ""global-local shrinkage prior"", ""high-dimensional binary regression"", ""network predictor"", ""node selection"", ""posterior consistency""","BA","""High-Dimensional Bayesian Network Classification with Network Global-Local Shrinkage Priors""""This article proposes a novel Bayesian binary classification framework for networks with labeled nodes. Our approach is motivated by applications in brain connectome studies, where the overarching goal is to identify both regions of interest (ROIs) in the brain and connections between ROIs that influence how study subjects are classified. We propose a novel binary logistic regression framework with the network as the predictor, and model the associated network coefficient using a novel class of global-local network shrinkage priors. We perform a theoretical analysis of a member of this class of priors (which we call the Network Lasso Prior) and show asymptotically correct classification of networks even when the number of network edges grows faster than the sample size. Two representative members from this class of priors, the Network Lasso prior and the Network Horseshoe prior, are implemented using an efficient Markov Chain Monte Carlo algorithm, and empirically evaluated through simulation studies and the analysis of a real brain connectome dataset.""""brain connectome"", ""global-local shrinkage prior"", ""high-dimensional binary regression"", ""network predictor"", ""node selection"", ""posterior consistency"""
"""10.1214/22-BA1338""","""Bayesian Learning of Graph Substructures""","""Graphical models provide a powerful methodology for learning the conditional independence structure in multivariate data. Inference is often focused on estimating individual edges in the latent graph. Nonetheless, there is increasing interest in inferring more complex structures, such as communities, for multiple reasons, including more effective information retrieval and better interpretability. Stochastic blockmodels offer a powerful tool to detect such structure in a network. We thus propose to exploit advances in random graph theory and embed them within the graphical models framework. A consequence of this approach is the propagation of the uncertainty in graph estimation to large-scale structure learning. We consider Bayesian nonparametric stochastic blockmodels as priors on the graph. We extend such models to consider clique-based blocks and to multiple graph settings introducing a novel prior process based on a Dependent Dirichlet process. Moreover, we devise a tailored computation strategy of Bayes factors for block structure based on the Savage-Dickey ratio to test for presence of larger structure in a graph. We demonstrate our approach in simulations as well as on real data applications in finance and transcriptomics.""","""Bayesian nonparametrics"", ""degree-corrected stochastic blockmodels"", ""dependent Dirichlet process"", ""Gaussian graphical models"", ""multiple graphical models"", ""multivariate data analysis""","BA","""Bayesian Learning of Graph Substructures""""Graphical models provide a powerful methodology for learning the conditional independence structure in multivariate data. Inference is often focused on estimating individual edges in the latent graph. Nonetheless, there is increasing interest in inferring more complex structures, such as communities, for multiple reasons, including more effective information retrieval and better interpretability. Stochastic blockmodels offer a powerful tool to detect such structure in a network. We thus propose to exploit advances in random graph theory and embed them within the graphical models framework. A consequence of this approach is the propagation of the uncertainty in graph estimation to large-scale structure learning. We consider Bayesian nonparametric stochastic blockmodels as priors on the graph. We extend such models to consider clique-based blocks and to multiple graph settings introducing a novel prior process based on a Dependent Dirichlet process. Moreover, we devise a tailored computation strategy of Bayes factors for block structure based on the Savage-Dickey ratio to test for presence of larger structure in a graph. We demonstrate our approach in simulations as well as on real data applications in finance and transcriptomics.""""Bayesian nonparametrics"", ""degree-corrected stochastic blockmodels"", ""dependent Dirichlet process"", ""Gaussian graphical models"", ""multiple graphical models"", ""multivariate data analysis"""
"""10.1214/22-BA1340""","""A Bayesian Computer Model Analysis of Robust Bayesian Analyses""","""We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.""","""emulation"", ""Gaussian process"", ""sensitivity analysis""","BA","""A Bayesian Computer Model Analysis of Robust Bayesian Analyses""""We harness the power of Bayesian emulation techniques, designed to aid the analysis of complex computer models, to examine the structure of complex Bayesian analyses themselves. These techniques facilitate robust Bayesian analyses and/or sensitivity analyses of complex problems, and hence allow global exploration of the impacts of choices made in both the likelihood and prior specification. We show how previously intractable problems in robustness studies can be overcome using emulation techniques, and how these methods allow other scientists to quickly extract approximations to posterior results corresponding to their own particular subjective specification. The utility and flexibility of our method is demonstrated on a reanalysis of a real application where Bayesian methods were employed to capture beliefs about river flow. We discuss the obvious extensions and directions of future research that such an approach opens up.""""emulation"", ""Gaussian process"", ""sensitivity analysis"""
"""10.1214/22-BA1305""","""Sequentially Guided MCMC Proposals for Synthetic Likelihoods and Correlated Synthetic Likelihoods""","""Synthetic likelihood (SL) is a strategy for parameter inference when the likelihood function is analytically or computationally intractable. In SL, the likelihood function of the data is replaced by a multivariate Gaussian density over summary statistics of the data. SL requires simulation of many replicate datasets at every parameter value considered by a sampling algorithm, such as Markov chain Monte Carlo (MCMC), making the method computationally-intensive. We propose two strategies to alleviate the computational burden. First, we introduce an algorithm producing a proposal distribution that is sequentially tuned and made conditional to data, thus it rapidly guides the proposed parameters towards high posterior density regions. In our experiments, a small number of iterations of our algorithm is enough to rapidly locate high density regions, which we use to initialize one or several chains that make use of off-the-shelf adaptive MCMC methods. Our “guided” approach can also be potentially used with MCMC samplers for approximate Bayesian computation (ABC). Second, we exploit strategies borrowed from the correlated pseudo-marginal MCMC literature, to improve the chains mixing in a SL framework. Moreover, our methods enable inference for challenging case studies, when the posterior is multimodal and when the chain is initialised in low posterior probability regions of the parameter space, where standard samplers failed. To illustrate the advantages stemming from our framework we consider five benchmark examples, including estimation of parameters for a cosmological model and a stochastic model with highly non-Gaussian summary statistics.""","""Bayesian inference"", ""cosmological parameters"", ""intractable likelihoods"", ""likelihood-free""","BA","""Sequentially Guided MCMC Proposals for Synthetic Likelihoods and Correlated Synthetic Likelihoods""""Synthetic likelihood (SL) is a strategy for parameter inference when the likelihood function is analytically or computationally intractable. In SL, the likelihood function of the data is replaced by a multivariate Gaussian density over summary statistics of the data. SL requires simulation of many replicate datasets at every parameter value considered by a sampling algorithm, such as Markov chain Monte Carlo (MCMC), making the method computationally-intensive. We propose two strategies to alleviate the computational burden. First, we introduce an algorithm producing a proposal distribution that is sequentially tuned and made conditional to data, thus it rapidly guides the proposed parameters towards high posterior density regions. In our experiments, a small number of iterations of our algorithm is enough to rapidly locate high density regions, which we use to initialize one or several chains that make use of off-the-shelf adaptive MCMC methods. Our “guided” approach can also be potentially used with MCMC samplers for approximate Bayesian computation (ABC). Second, we exploit strategies borrowed from the correlated pseudo-marginal MCMC literature, to improve the chains mixing in a SL framework. Moreover, our methods enable inference for challenging case studies, when the posterior is multimodal and when the chain is initialised in low posterior probability regions of the parameter space, where standard samplers failed. To illustrate the advantages stemming from our framework we consider five benchmark examples, including estimation of parameters for a cosmological model and a stochastic model with highly non-Gaussian summary statistics.""""Bayesian inference"", ""cosmological parameters"", ""intractable likelihoods"", ""likelihood-free"""
"""10.1214/22-BA1313""","""The Posterior Predictive Null""","""Bayesian model criticism is an important part of the practice of Bayesian statistics. Traditionally, model criticism methods have been based on the predictive check, an adaptation of goodness-of-fit testing to Bayesian modeling and an effective method to understand how well a model captures the distribution of the data. In modern practice, however, researchers iteratively build and develop many models, exploring a space of models to help solve the problem at hand. While classical predictive checks can help assess each one, they cannot help the researcher understand how the models relate to each other. This paper introduces the posterior predictive null check (PPN), a method for Bayesian model criticism that helps characterize the relationships between models. The idea behind the PPN is to check whether data from one model’s predictive distribution can pass a predictive check designed for another model. This form of criticism complements the classical predictive check by providing a comparative tool. A collection of PPNs, which we call a PPN study, can help us understand which models are equivalent and which models provide different perspectives on the data. With mixture models, we demonstrate how a PPN study, along with traditional predictive checks, can help select the number of components by the principle of parsimony. With probabilistic factor models, we demonstrate how a PPN study can help understand relationships between different classes of models, such as linear models and models based on neural networks. Finally, we analyze data from the literature on predictive checks to show how a PPN study can improve the practice of Bayesian model criticism.""","""Bayesian workflow"", ""model criticism"", ""predictive checks""","BA","""The Posterior Predictive Null""""Bayesian model criticism is an important part of the practice of Bayesian statistics. Traditionally, model criticism methods have been based on the predictive check, an adaptation of goodness-of-fit testing to Bayesian modeling and an effective method to understand how well a model captures the distribution of the data. In modern practice, however, researchers iteratively build and develop many models, exploring a space of models to help solve the problem at hand. While classical predictive checks can help assess each one, they cannot help the researcher understand how the models relate to each other. This paper introduces the posterior predictive null check (PPN), a method for Bayesian model criticism that helps characterize the relationships between models. The idea behind the PPN is to check whether data from one model’s predictive distribution can pass a predictive check designed for another model. This form of criticism complements the classical predictive check by providing a comparative tool. A collection of PPNs, which we call a PPN study, can help us understand which models are equivalent and which models provide different perspectives on the data. With mixture models, we demonstrate how a PPN study, along with traditional predictive checks, can help select the number of components by the principle of parsimony. With probabilistic factor models, we demonstrate how a PPN study can help understand relationships between different classes of models, such as linear models and models based on neural networks. Finally, we analyze data from the literature on predictive checks to show how a PPN study can improve the practice of Bayesian model criticism.""""Bayesian workflow"", ""model criticism"", ""predictive checks"""
"""10.1214/22-BA1336""","""Posterior Predictive Checking for Partially Observed Stochastic Epidemic Models""","""We address the problem of assessing the fit of stochastic epidemic models to data. Two novel model assessment methods are developed, based on disease progression curves, namely the distance method and the position-time method. The methods are illustrated using SIR (susceptible-infective-removed) models. We assume a typical data observation setting in which case-detection times are observed while infection times are not. Both methods involve Bayesian posterior predictive checking, in which the observed data are compared to data generated from the posterior predictive distribution. The distance method does this by calculating distances between disease progression curves, while the position-time method does this pointwise at suitably selected time points. Both methods provide visual and quantitative outputs with meaningful interpretations. The performance of the methods benefits from the development and application of a time-shifting method that accounts for the random time delay until an epidemic takes off. Extensive simulation studies show that both methods can successfully be used to assess the choice of infectious period distribution and the choice of infection rate function.""","""Epidemic model"", ""infectious disease data"", ""posterior predictive p-value""","BA","""Posterior Predictive Checking for Partially Observed Stochastic Epidemic Models""""We address the problem of assessing the fit of stochastic epidemic models to data. Two novel model assessment methods are developed, based on disease progression curves, namely the distance method and the position-time method. The methods are illustrated using SIR (susceptible-infective-removed) models. We assume a typical data observation setting in which case-detection times are observed while infection times are not. Both methods involve Bayesian posterior predictive checking, in which the observed data are compared to data generated from the posterior predictive distribution. The distance method does this by calculating distances between disease progression curves, while the position-time method does this pointwise at suitably selected time points. Both methods provide visual and quantitative outputs with meaningful interpretations. The performance of the methods benefits from the development and application of a time-shifting method that accounts for the random time delay until an epidemic takes off. Extensive simulation studies show that both methods can successfully be used to assess the choice of infectious period distribution and the choice of infection rate function.""""Epidemic model"", ""infectious disease data"", ""posterior predictive p-value"""
"""10.1214/23-BA1390""","""Inducing High Spatial Correlation with Randomly Edge-Weighted Neighborhood Graphs""","""Traditional models for areal data assume a hierarchical structure where one of the components is the random effects that spatially correlate the areas. The conditional autoregressive (CAR) model is the most popular distribution to jointly model the prior uncertainty about these spatial random effects. A limitation of the CAR distribution is its inability to accommodate high correlations between neighboring areas. We propose a new model for areal data that alleviates this problem. We represent the map by an undirected graph where the nodes are the areas, and randomly-weighted edges connect nodes that are neighbors. The model is based on a spatially-structured, multivariate Normal/Independent(NI) distribution, in which the precision matrix is indirectly built assuming a multivariate distribution for the random edge effects. The joint distribution for the edge effects is a spatial multivariate NI distribution that induces another NI distribution for the areas’ spatial effects, which inherit its capacity to accommodate outliers and heavy-tailed behavior. Most important, it can produce a higher marginal correlation between the spatial effects than the CAR model overcoming one of the main limitations of this model. We fit the proposed model to analyze real cancer maps and compared its performance with several state-of-art competitors. Our proposed model provides better fitting in almost all cases.""","""normal/independent distribution family"", ""random edge effects"", ""spatial autoregression"", ""spatial Bayesian inference""","BA","""Inducing High Spatial Correlation with Randomly Edge-Weighted Neighborhood Graphs""""Traditional models for areal data assume a hierarchical structure where one of the components is the random effects that spatially correlate the areas. The conditional autoregressive (CAR) model is the most popular distribution to jointly model the prior uncertainty about these spatial random effects. A limitation of the CAR distribution is its inability to accommodate high correlations between neighboring areas. We propose a new model for areal data that alleviates this problem. We represent the map by an undirected graph where the nodes are the areas, and randomly-weighted edges connect nodes that are neighbors. The model is based on a spatially-structured, multivariate Normal/Independent(NI) distribution, in which the precision matrix is indirectly built assuming a multivariate distribution for the random edge effects. The joint distribution for the edge effects is a spatial multivariate NI distribution that induces another NI distribution for the areas’ spatial effects, which inherit its capacity to accommodate outliers and heavy-tailed behavior. Most important, it can produce a higher marginal correlation between the spatial effects than the CAR model overcoming one of the main limitations of this model. We fit the proposed model to analyze real cancer maps and compared its performance with several state-of-art competitors. Our proposed model provides better fitting in almost all cases.""""normal/independent distribution family"", ""random edge effects"", ""spatial autoregression"", ""spatial Bayesian inference"""
"""10.1214/22-BA1331""","""Data Augmentation for Bayesian Deep Learning""","""Deep Learning (DL) methods have emerged as one of the most powerful tools for functional approximation and prediction. While the representation properties of DL have been well studied, uncertainty quantification remains challenging and largely unexplored. Data augmentation techniques are a natural approach to provide uncertainty quantification and to incorporate stochastic Monte Carlo search into stochastic gradient descent (SGD) methods. The purpose of our paper is to show that training DL architectures with data augmentation leads to efficiency gains. We use the theory of scale mixtures of normals to derive data augmentation strategies for deep learning. This allows variants of the expectation-maximization and MCMC algorithms to be brought to bear on these high dimensional nonlinear deep learning models. To demonstrate our methodology, we develop data augmentation algorithms for a variety of commonly used activation functions: logit, ReLU, leaky ReLU and SVM. Our methodology is compared to traditional stochastic gradient descent with back-propagation. Our optimization procedure leads to a version of iteratively re-weighted least squares and can be implemented at scale with accelerated linear algebra methods providing substantial improvement in speed. We illustrate our methodology on a number of standard datasets. Finally, we conclude with directions for future research.""","""back-propagation"", ""Data augmentation"", ""deep learning"", ""MCMC"", ""SGD""","BA","""Data Augmentation for Bayesian Deep Learning""""Deep Learning (DL) methods have emerged as one of the most powerful tools for functional approximation and prediction. While the representation properties of DL have been well studied, uncertainty quantification remains challenging and largely unexplored. Data augmentation techniques are a natural approach to provide uncertainty quantification and to incorporate stochastic Monte Carlo search into stochastic gradient descent (SGD) methods. The purpose of our paper is to show that training DL architectures with data augmentation leads to efficiency gains. We use the theory of scale mixtures of normals to derive data augmentation strategies for deep learning. This allows variants of the expectation-maximization and MCMC algorithms to be brought to bear on these high dimensional nonlinear deep learning models. To demonstrate our methodology, we develop data augmentation algorithms for a variety of commonly used activation functions: logit, ReLU, leaky ReLU and SVM. Our methodology is compared to traditional stochastic gradient descent with back-propagation. Our optimization procedure leads to a version of iteratively re-weighted least squares and can be implemented at scale with accelerated linear algebra methods providing substantial improvement in speed. We illustrate our methodology on a number of standard datasets. Finally, we conclude with directions for future research.""""back-propagation"", ""Data augmentation"", ""deep learning"", ""MCMC"", ""SGD"""
"""10.1214/22-BA1337""","""Comparing Dependent Undirected Gaussian Networks""","""A Bayesian approach is proposed, which unifies network constructions and comparisons between two longitudinal undirected Gaussian networks on their differentiation status (identical or differential) for data collected at two time points. Utilizing the concept of modeling repeated measures, we construct a joint likelihood of networks. The conditional posterior probability mass function for network differentiation is derived and its asymptotic proposition is theoretically assessed. An alternative approach built upon latent rather than manifest data is proposed to significantly reduce computing burden. Simulations are used to demonstrate and compare the two methods and compare them with existing approaches. Based on epigenetic data collected at different ages, the proposed methods are demonstrated on their ability to detect dependent network differentiations. Our theoretical assessment, simulations, and real data applications support the effectiveness of the proposed methods, although the approach relying on latent data is less efficient.""","""Bayesian methods"", ""efficient Bayesian sampling"", ""Gaussian network testing"", ""latent data likelihood"", ""penalized conditional posterior probability""","BA","""Comparing Dependent Undirected Gaussian Networks""""A Bayesian approach is proposed, which unifies network constructions and comparisons between two longitudinal undirected Gaussian networks on their differentiation status (identical or differential) for data collected at two time points. Utilizing the concept of modeling repeated measures, we construct a joint likelihood of networks. The conditional posterior probability mass function for network differentiation is derived and its asymptotic proposition is theoretically assessed. An alternative approach built upon latent rather than manifest data is proposed to significantly reduce computing burden. Simulations are used to demonstrate and compare the two methods and compare them with existing approaches. Based on epigenetic data collected at different ages, the proposed methods are demonstrated on their ability to detect dependent network differentiations. Our theoretical assessment, simulations, and real data applications support the effectiveness of the proposed methods, although the approach relying on latent data is less efficient.""""Bayesian methods"", ""efficient Bayesian sampling"", ""Gaussian network testing"", ""latent data likelihood"", ""penalized conditional posterior probability"""
"""10.1214/22-BA1342""","""Controlling the Flexibility of Non-Gaussian Processes Through Shrinkage Priors""","""The normal inverse Gaussian (NIG) and generalized asymmetric Laplace (GAL) distributions can be seen as skewed and semi-heavy-tailed extensions of the Gaussian distribution. Models driven by these more flexible noise distributions are then regarded as flexible extensions of simpler Gaussian models. Inferential procedures tend to overestimate the degree of non-Gaussianity in the data and therefore we propose controlling the flexibility of these non-Gaussian models by adding sensible priors in the inferential framework that contract the model towards Gaussianity. In our venture to derive sensible priors, we also propose a new intuitive parameterization of the non-Gaussian models and discuss how to implement them efficiently in Stan. The methods are derived for a generic class of non-Gaussian models that include spatial Matérn fields, autoregressive models for time series, and simultaneous autoregressive models for aerial data. The results are illustrated with a simulation study and geostatistics application, where priors that penalize model complexity were shown to lead to more robust estimation and give preference to the Gaussian model, while at the same time allowing for non-Gaussianity if there is sufficient evidence in the data.""","""Bayesian"", ""generalized asymmetric Laplace"", ""Matérn covariance"", ""non-Gaussian"", ""normal inverse Gaussian"", ""penalized complexity"", ""priors"", ""SPDE""","BA","""Controlling the Flexibility of Non-Gaussian Processes Through Shrinkage Priors""""The normal inverse Gaussian (NIG) and generalized asymmetric Laplace (GAL) distributions can be seen as skewed and semi-heavy-tailed extensions of the Gaussian distribution. Models driven by these more flexible noise distributions are then regarded as flexible extensions of simpler Gaussian models. Inferential procedures tend to overestimate the degree of non-Gaussianity in the data and therefore we propose controlling the flexibility of these non-Gaussian models by adding sensible priors in the inferential framework that contract the model towards Gaussianity. In our venture to derive sensible priors, we also propose a new intuitive parameterization of the non-Gaussian models and discuss how to implement them efficiently in Stan. The methods are derived for a generic class of non-Gaussian models that include spatial Matérn fields, autoregressive models for time series, and simultaneous autoregressive models for aerial data. The results are illustrated with a simulation study and geostatistics application, where priors that penalize model complexity were shown to lead to more robust estimation and give preference to the Gaussian model, while at the same time allowing for non-Gaussianity if there is sufficient evidence in the data.""""Bayesian"", ""generalized asymmetric Laplace"", ""Matérn covariance"", ""non-Gaussian"", ""normal inverse Gaussian"", ""penalized complexity"", ""priors"", ""SPDE"""
"""10.1214/22-BA1324""","""Bayesian Inferences on Uncertain Ranks and Orderings: Application to Ranking Players and Lineups""","""It is common to be interested in rankings or order relationships among entities. In complex settings where one does not directly measure a univariate statistic upon which to base ranks, such inferences typically rely on statistical models having entity-specific parameters. These can be treated as random effects in hierarchical models characterizing variation among the entities. In this paper, we are particularly interested in the problem of ranking basketball players in terms of their contribution to team performance. Using data from the National Basketball Association (NBA) in the United States, we find that many players have similar latent ability levels, making any single estimated ranking highly misleading. The current literature fails to provide summaries of order relationships that adequately account for uncertainty. Motivated by this, we propose a Bayesian strategy for characterizing uncertainty in inferences on order relationships among players and lineups. Our approach adapts to scenarios in which uncertainty in ordering is high by producing more conservative results that improve interpretability. This is achieved through a reward function within a decision theoretic framework. We apply our approach to data from the 2009–2010 NBA season.""","""Bayesian"", ""decision theory"", ""ordering statements"", ""ranking"", ""sports statistics""","BA","""Bayesian Inferences on Uncertain Ranks and Orderings: Application to Ranking Players and Lineups""""It is common to be interested in rankings or order relationships among entities. In complex settings where one does not directly measure a univariate statistic upon which to base ranks, such inferences typically rely on statistical models having entity-specific parameters. These can be treated as random effects in hierarchical models characterizing variation among the entities. In this paper, we are particularly interested in the problem of ranking basketball players in terms of their contribution to team performance. Using data from the National Basketball Association (NBA) in the United States, we find that many players have similar latent ability levels, making any single estimated ranking highly misleading. The current literature fails to provide summaries of order relationships that adequately account for uncertainty. Motivated by this, we propose a Bayesian strategy for characterizing uncertainty in inferences on order relationships among players and lineups. Our approach adapts to scenarios in which uncertainty in ordering is high by producing more conservative results that improve interpretability. This is achieved through a reward function within a decision theoretic framework. We apply our approach to data from the 2009–2010 NBA season.""""Bayesian"", ""decision theory"", ""ordering statements"", ""ranking"", ""sports statistics"""
"""10.1214/22-BA1328""","""Regularized Zero-Variance Control Variates""","""Zero-variance control variates (ZV-CV) are a post-processing method to reduce the variance of Monte Carlo estimators of expectations using the derivatives of the log target. Once the derivatives are available, the only additional computational effort lies in solving a linear regression problem. Significant variance reductions have been achieved with this method in low dimensional examples, but the number of covariates in the regression rapidly increases with the dimension of the target. In this paper, we present compelling empirical evidence that the use of penalized regression techniques in the selection of high-dimensional control variates provides performance gains over the classical least squares method. Another type of regularization based on using subsets of derivatives, or a priori regularization as we refer to it in this paper, is also proposed to reduce computational and storage requirements. Several examples showing the utility and limitations of regularized ZV-CV for Bayesian inference are given. The methods proposed in this paper are accessible through the R package ZVCV.""","""Bayesian inference"", ""controlled thermodynamic integration – CTI"", ""curse of dimensionality"", ""Markov chain Monte Carlo simulation"", ""MCMC"", ""Monte Carlo simulations"", ""penalized regression"", ""sequential Monte Carlo – SMC"", ""Stein operator"", ""variance reduction""","BA","""Regularized Zero-Variance Control Variates""""Zero-variance control variates (ZV-CV) are a post-processing method to reduce the variance of Monte Carlo estimators of expectations using the derivatives of the log target. Once the derivatives are available, the only additional computational effort lies in solving a linear regression problem. Significant variance reductions have been achieved with this method in low dimensional examples, but the number of covariates in the regression rapidly increases with the dimension of the target. In this paper, we present compelling empirical evidence that the use of penalized regression techniques in the selection of high-dimensional control variates provides performance gains over the classical least squares method. Another type of regularization based on using subsets of derivatives, or a priori regularization as we refer to it in this paper, is also proposed to reduce computational and storage requirements. Several examples showing the utility and limitations of regularized ZV-CV for Bayesian inference are given. The methods proposed in this paper are accessible through the R package ZVCV.""""Bayesian inference"", ""controlled thermodynamic integration – CTI"", ""curse of dimensionality"", ""Markov chain Monte Carlo simulation"", ""MCMC"", ""Monte Carlo simulations"", ""penalized regression"", ""sequential Monte Carlo – SMC"", ""Stein operator"", ""variance reduction"""
"""10.1214/22-BA1330""","""Robustness Against Conflicting Prior Information in Regression""","""Including prior information about model parameters is a fundamental step of any Bayesian statistical analysis. It is viewed positively by some as it allows, among others, to quantitatively incorporate expert opinion about model parameters. It is viewed negatively by others because it sets the stage for subjectivity in statistical analysis. Certainly, it creates problems when the inference is skewed due to a conflict with the data collected. According to the theory of conflict resolution (O’Hagan and Pericchi, 2012), a solution to such problems is to diminish the impact of conflicting prior information, yielding inference consistent with the data. This is typically achieved by using heavy-tailed priors. We study both theoretically and numerically the efficacy of such a solution in a regression framework where the prior information about the coefficients takes the form of a product of density functions with known location and scale parameters. We study functions with regularly-varying tails (Student distributions), log-regularly-varying tails (as introduced in Desgagné (2015)), and propose functions with slower tail decays that allow to resolve any conflict that can happen under that regression framework, contrarily to the two previous types of functions. The code to reproduce all numerical experiments is available online.""","""Bayesian statistics"", ""Built-in robustness"", ""constant-tailed priors"", ""heavy-tailed distributions"", ""weak convergence"", ""whole robustness""","BA","""Robustness Against Conflicting Prior Information in Regression""""Including prior information about model parameters is a fundamental step of any Bayesian statistical analysis. It is viewed positively by some as it allows, among others, to quantitatively incorporate expert opinion about model parameters. It is viewed negatively by others because it sets the stage for subjectivity in statistical analysis. Certainly, it creates problems when the inference is skewed due to a conflict with the data collected. According to the theory of conflict resolution (O’Hagan and Pericchi, 2012), a solution to such problems is to diminish the impact of conflicting prior information, yielding inference consistent with the data. This is typically achieved by using heavy-tailed priors. We study both theoretically and numerically the efficacy of such a solution in a regression framework where the prior information about the coefficients takes the form of a product of density functions with known location and scale parameters. We study functions with regularly-varying tails (Student distributions), log-regularly-varying tails (as introduced in Desgagné (2015)), and propose functions with slower tail decays that allow to resolve any conflict that can happen under that regression framework, contrarily to the two previous types of functions. The code to reproduce all numerical experiments is available online.""""Bayesian statistics"", ""Built-in robustness"", ""constant-tailed priors"", ""heavy-tailed distributions"", ""weak convergence"", ""whole robustness"""
"""10.1214/22-BA1335""","""Bayesian Optimal Experimental Design for Inferring Causal Structure""","""Inferring the causal structure of a system typically requires interventional data, rather than just observational data. Since interventional experiments can be costly, it is preferable to select interventions that yield the maximum amount of information about a system. We propose a novel Bayesian method for optimal experimental design by sequentially selecting interventions that minimize the expected posterior entropy as rapidly as possible. A key feature is that the method can be implemented by computing simple summaries of the current posterior, avoiding the computationally burdensome task of repeatedly performing posterior inference on hypothetical future datasets drawn from the posterior predictive. After deriving the method in a general setting, we apply it to the problem of inferring causal networks. We present a series of simulation studies, in which we find that the proposed method performs favorably compared to existing alternative methods. Finally, we apply the method to real data from two gene regulatory networks.""","""Active learning"", ""graphical models"", ""optimal experimental design""","BA","""Bayesian Optimal Experimental Design for Inferring Causal Structure""""Inferring the causal structure of a system typically requires interventional data, rather than just observational data. Since interventional experiments can be costly, it is preferable to select interventions that yield the maximum amount of information about a system. We propose a novel Bayesian method for optimal experimental design by sequentially selecting interventions that minimize the expected posterior entropy as rapidly as possible. A key feature is that the method can be implemented by computing simple summaries of the current posterior, avoiding the computationally burdensome task of repeatedly performing posterior inference on hypothetical future datasets drawn from the posterior predictive. After deriving the method in a general setting, we apply it to the problem of inferring causal networks. We present a series of simulation studies, in which we find that the proposed method performs favorably compared to existing alternative methods. Finally, we apply the method to real data from two gene regulatory networks.""""Active learning"", ""graphical models"", ""optimal experimental design"""
"""10.1214/22-BA1319""","""Posterior Computation with the Gibbs Zig-Zag Sampler""","""An intriguing new class of piecewise deterministic Markov processes (PDMPs) has recently been proposed as an alternative to Markov chain Monte Carlo (MCMC). We propose a new class of PDMPs termed Gibbs zig-zag samplers, which allow parameters to be updated in blocks with a zig-zag sampler applied to certain parameters and traditional MCMC-style updates to others. We demonstrate the flexibility of this framework on posterior sampling for logistic models with shrinkage priors for high-dimensional regression and random effects, and provide conditions for geometric ergodicity and the validity of a central limit theorem.""","""Gibbs sampler"", ""Markov chain Monte Carlo"", ""non-reversible"", ""Piecewise deterministic Markov process"", ""sub-sampling""","BA","""Posterior Computation with the Gibbs Zig-Zag Sampler""""An intriguing new class of piecewise deterministic Markov processes (PDMPs) has recently been proposed as an alternative to Markov chain Monte Carlo (MCMC). We propose a new class of PDMPs termed Gibbs zig-zag samplers, which allow parameters to be updated in blocks with a zig-zag sampler applied to certain parameters and traditional MCMC-style updates to others. We demonstrate the flexibility of this framework on posterior sampling for logistic models with shrinkage priors for high-dimensional regression and random effects, and provide conditions for geometric ergodicity and the validity of a central limit theorem.""""Gibbs sampler"", ""Markov chain Monte Carlo"", ""non-reversible"", ""Piecewise deterministic Markov process"", ""sub-sampling"""
"""10.1214/22-BA1325""","""Quantification of Empirical Determinacy: The Impact of Likelihood Weighting on Posterior Location and Spread in Bayesian Meta-Analysis Estimated with JAGS and INLA""","""The popular Bayesian meta-analysis expressed by the normal-normal hierarchical model synthesizes knowledge from several studies and is highly relevant in practice. The normal-normal hierarchical model is the simplest Bayesian hierarchical model, but illustrates problems typical in more complex Bayesian hierarchical models. Until now, it has been unclear to what extent the data determines the marginal posterior distributions of the parameters in the normal-normal hierarchical model. To address this issue we computed the second derivative of the Bhattacharyya coefficient with respect to the weighted likelihood. This quantity, which we define as the total empirical determinacy (TED), can be written as the sum of two terms: the empirical determinacy of location (EDL), and the empirical determinacy of spread (EDS). We implemented this method in the R package ed4bhm and considered two case studies and one simulation study. We quantified TED, EDL and EDS under different modeling conditions such as model parametrization, the primary outcome, and the prior. This clarifies to what extent the location and spread of the marginal posterior distributions of the parameters are determined by the data. Although these investigations focused on Bayesian normal-normal hierarchical model, the method proposed is applicable more generally to complex Bayesian hierarchical models.""","""Bayesian hierarchical models"", ""Bayesian meta-analysis"", ""empirical determinacy"", ""Identification"", ""likelihood weighting""","BA","""Quantification of Empirical Determinacy: The Impact of Likelihood Weighting on Posterior Location and Spread in Bayesian Meta-Analysis Estimated with JAGS and INLA""""The popular Bayesian meta-analysis expressed by the normal-normal hierarchical model synthesizes knowledge from several studies and is highly relevant in practice. The normal-normal hierarchical model is the simplest Bayesian hierarchical model, but illustrates problems typical in more complex Bayesian hierarchical models. Until now, it has been unclear to what extent the data determines the marginal posterior distributions of the parameters in the normal-normal hierarchical model. To address this issue we computed the second derivative of the Bhattacharyya coefficient with respect to the weighted likelihood. This quantity, which we define as the total empirical determinacy (TED), can be written as the sum of two terms: the empirical determinacy of location (EDL), and the empirical determinacy of spread (EDS). We implemented this method in the R package ed4bhm and considered two case studies and one simulation study. We quantified TED, EDL and EDS under different modeling conditions such as model parametrization, the primary outcome, and the prior. This clarifies to what extent the location and spread of the marginal posterior distributions of the parameters are determined by the data. Although these investigations focused on Bayesian normal-normal hierarchical model, the method proposed is applicable more generally to complex Bayesian hierarchical models.""""Bayesian hierarchical models"", ""Bayesian meta-analysis"", ""empirical determinacy"", ""Identification"", ""likelihood weighting"""
"""10.1214/22-BA1327""","""Combining Chains of Bayesian Models with Markov Melding""","""A challenge for practitioners of Bayesian inference is specifying a model that incorporates multiple relevant, heterogeneous data sets. It may be easier to instead specify distinct submodels for each source of data, then join the submodels together. We consider chains of submodels, where submodels directly relate to their neighbours via common quantities which may be parameters or deterministic functions thereof. We propose chained Markov melding, an extension of Markov melding, a generic method to combine chains of submodels into a joint model. One challenge we address is appropriately capturing the prior dependence between common quantities within a submodel, whilst also reconciling differences in priors for the same common quantity between two adjacent submodels. Estimating the posterior of the resulting overall joint model is also challenging, so we describe a sampler that uses the chain structure to incorporate information contained in the submodels in multiple stages, possibly in parallel. We demonstrate our methodology using two examples. The first example considers an ecological integrated population model, where multiple data sets are required to accurately estimate population immigration and reproduction rates. We also consider a joint longitudinal and time-to-event model with uncertain, submodel-derived event times. Chained Markov melding is a conceptually appealing approach to integrating submodels in these settings.""","""Bayesian graphical models"", ""combining models"", ""integrated population model"", ""Markov melding"", ""model/data integration"", ""multi-stage estimation""","BA","""Combining Chains of Bayesian Models with Markov Melding""""A challenge for practitioners of Bayesian inference is specifying a model that incorporates multiple relevant, heterogeneous data sets. It may be easier to instead specify distinct submodels for each source of data, then join the submodels together. We consider chains of submodels, where submodels directly relate to their neighbours via common quantities which may be parameters or deterministic functions thereof. We propose chained Markov melding, an extension of Markov melding, a generic method to combine chains of submodels into a joint model. One challenge we address is appropriately capturing the prior dependence between common quantities within a submodel, whilst also reconciling differences in priors for the same common quantity between two adjacent submodels. Estimating the posterior of the resulting overall joint model is also challenging, so we describe a sampler that uses the chain structure to incorporate information contained in the submodels in multiple stages, possibly in parallel. We demonstrate our methodology using two examples. The first example considers an ecological integrated population model, where multiple data sets are required to accurately estimate population immigration and reproduction rates. We also consider a joint longitudinal and time-to-event model with uncertain, submodel-derived event times. Chained Markov melding is a conceptually appealing approach to integrating submodels in these settings.""""Bayesian graphical models"", ""combining models"", ""integrated population model"", ""Markov melding"", ""model/data integration"", ""multi-stage estimation"""
"""10.1214/22-BA1323""","""Bayesian Posterior Repartitioning for Nested Sampling""","""Priors in Bayesian analyses often encode informative domain knowledge that can be useful in making the inference process more efficient. Occasionally, however, priors may be unrepresentative of the parameter values for a given dataset, which can result in inefficient parameter space exploration, or even incorrect inferences, particularly for nested sampling (NS) algorithms. Simply broadening the prior in such cases may be inappropriate or impossible in some applications. Hence our previous solution to this problem, known as posterior repartitioning (PR), redefines the prior and likelihood while keeping their product fixed, so that the posterior inferences and evidence estimates remain unchanged, but the efficiency of the NS process is significantly increased. In its most practical form, PR raises the prior to some power β, which is introduced as an auxiliary variable that must be determined on a case-by-case basis, usually by lowering β from unity according to some pre-defined ‘annealing schedule’ until the resulting inferences converge to a consistent solution. Here we present a very simple yet powerful alternative Bayesian approach, in which β is instead treated as a hyperparameter that is inferred from the data alongside the original parameters of the problem, and then marginalised over to obtain the final inference. We show through numerical examples that this Bayesian PR (BPR) method provides a very robust, self-adapting and computationally efficient ‘hands-off’ solution to the problem of unrepresentative priors in Bayesian inference using NS. Moreover, unlike the original PR method, we show that even for representative priors BPR has a negligible computational overhead relative to standard nesting sampling, which suggests that it should be used as the default in all NS analyses.""","""automatic posterior repartitioning"", ""Bayesian inference"", ""nested sampling"", ""unrepresentative prior""","BA","""Bayesian Posterior Repartitioning for Nested Sampling""""Priors in Bayesian analyses often encode informative domain knowledge that can be useful in making the inference process more efficient. Occasionally, however, priors may be unrepresentative of the parameter values for a given dataset, which can result in inefficient parameter space exploration, or even incorrect inferences, particularly for nested sampling (NS) algorithms. Simply broadening the prior in such cases may be inappropriate or impossible in some applications. Hence our previous solution to this problem, known as posterior repartitioning (PR), redefines the prior and likelihood while keeping their product fixed, so that the posterior inferences and evidence estimates remain unchanged, but the efficiency of the NS process is significantly increased. In its most practical form, PR raises the prior to some power β, which is introduced as an auxiliary variable that must be determined on a case-by-case basis, usually by lowering β from unity according to some pre-defined ‘annealing schedule’ until the resulting inferences converge to a consistent solution. Here we present a very simple yet powerful alternative Bayesian approach, in which β is instead treated as a hyperparameter that is inferred from the data alongside the original parameters of the problem, and then marginalised over to obtain the final inference. We show through numerical examples that this Bayesian PR (BPR) method provides a very robust, self-adapting and computationally efficient ‘hands-off’ solution to the problem of unrepresentative priors in Bayesian inference using NS. Moreover, unlike the original PR method, we show that even for representative priors BPR has a negligible computational overhead relative to standard nesting sampling, which suggests that it should be used as the default in all NS analyses.""""automatic posterior repartitioning"", ""Bayesian inference"", ""nested sampling"", ""unrepresentative prior"""
"""10.1214/22-BA1326""","""Seemingly Unrelated Multi-State Processes: A Bayesian Semiparametric Approach""","""Many applications in medical statistics and other fields can be described by transitions between multiple states (e.g. from health to disease) experienced by individuals over time. In this context, multi-state models are a popular statistical technique, in particular when the exact transition times are not observed. The key quantities of interest are the transition rates, capturing the instantaneous risk of moving from one state to another. The main contribution of this work is to propose a joint semiparametric model for several possibly related multi-state processes (Seemingly Unrelated Multi-State, SUMS, processes), assuming a Markov structure for the transitions over time. The dependence between different processes is captured by specifying a joint prior distribution on the transition rates of each process. In this case, we assume a flexible distribution, which allows for clustering of the individuals, overdispersion and outliers. Moreover, we employ a graph structure to describe the dependence among processes, exploiting tools from the Gaussian Graphical model literature. It is also possible to include covariate effects. We use our approach to model disease progression in mental health. Posterior inference is performed through a specially devised MCMC algorithm.""","""graphical models"", ""Markov chain Monte Carlo"", ""Mixture models"", ""multi-state models"", ""Normalised Point Processes""","BA","""Seemingly Unrelated Multi-State Processes: A Bayesian Semiparametric Approach""""Many applications in medical statistics and other fields can be described by transitions between multiple states (e.g. from health to disease) experienced by individuals over time. In this context, multi-state models are a popular statistical technique, in particular when the exact transition times are not observed. The key quantities of interest are the transition rates, capturing the instantaneous risk of moving from one state to another. The main contribution of this work is to propose a joint semiparametric model for several possibly related multi-state processes (Seemingly Unrelated Multi-State, SUMS, processes), assuming a Markov structure for the transitions over time. The dependence between different processes is captured by specifying a joint prior distribution on the transition rates of each process. In this case, we assume a flexible distribution, which allows for clustering of the individuals, overdispersion and outliers. Moreover, we employ a graph structure to describe the dependence among processes, exploiting tools from the Gaussian Graphical model literature. It is also possible to include covariate effects. We use our approach to model disease progression in mental health. Posterior inference is performed through a specially devised MCMC algorithm.""""graphical models"", ""Markov chain Monte Carlo"", ""Mixture models"", ""multi-state models"", ""Normalised Point Processes"""
"""10.1214/22-BA1332""","""Gaussian Variational Approximations for High-dimensional State Space Models""","""We consider a Gaussian variational approximation of the posterior density in high-dimensional state space models. The number of parameters in the covariance matrix of the variational approximation grows as the square of the number of model parameters, so it is necessary to find simple yet effective parametrisations of the covariance structure when the number of model parameters is large. We approximate the joint posterior density of the state vectors by a dynamic factor model, having Markovian time dependence and a factor covariance structure for the states. This gives a reduced description of the dependence structure for the states, as well as a temporal conditional independence structure similar to that in the true posterior. We illustrate the methodology on two examples. The first is a spatio-temporal model for the spread of the Eurasian collared-dove across North America. Our approach compares favorably to a recently proposed ensemble Kalman filter method for approximate inference in high-dimensional hierarchical spatio-temporal models. Our second example is a Wishart-based multivariate stochastic volatility model for financial returns, which is outside the class of models the ensemble Kalman filter method can handle.""","""dynamic factor"", ""spatio-temporal modelling"", ""stochastic gradient""","BA","""Gaussian Variational Approximations for High-dimensional State Space Models""""We consider a Gaussian variational approximation of the posterior density in high-dimensional state space models. The number of parameters in the covariance matrix of the variational approximation grows as the square of the number of model parameters, so it is necessary to find simple yet effective parametrisations of the covariance structure when the number of model parameters is large. We approximate the joint posterior density of the state vectors by a dynamic factor model, having Markovian time dependence and a factor covariance structure for the states. This gives a reduced description of the dependence structure for the states, as well as a temporal conditional independence structure similar to that in the true posterior. We illustrate the methodology on two examples. The first is a spatio-temporal model for the spread of the Eurasian collared-dove across North America. Our approach compares favorably to a recently proposed ensemble Kalman filter method for approximate inference in high-dimensional hierarchical spatio-temporal models. Our second example is a Wishart-based multivariate stochastic volatility model for financial returns, which is outside the class of models the ensemble Kalman filter method can handle.""""dynamic factor"", ""spatio-temporal modelling"", ""stochastic gradient"""
"""10.1214/22-BA1333""","""Post-Processed Posteriors for Banded Covariances""","""We consider Bayesian inference of banded covariance matrices and propose a post-processed posterior. The post-processing of the posterior consists of two steps. In the first step, posterior samples are obtained from the conjugate inverse-Wishart posterior, which does not satisfy any structural restrictions. In the second step, the posterior samples are transformed to satisfy the structural restriction through a post-processing function. The conceptually straightforward procedure of the post-processed posterior makes its computation efficient and can render interval estimators of functionals of covariance matrices. We show that it has nearly optimal minimax rates for banded covariances among all possible pairs of priors and post-processing functions. Additionally, we provide a theorem on the credible set of the post-processed posterior under the finite dimension assumption. We prove that the expected coverage probability of the 100(1−α)% highest posterior density region of the post-processed posterior is asymptotically 1−α with respect to any conventional posterior distribution. It implies that the highest posterior density region of the post-processed posterior is, on average, a credible set of conventional posterior. The advantages of the post-processed posterior are demonstrated by a simulation study and a real data analysis.""","""Bayesian"", ""high-dimensional analysis"", ""inverse-Wishart"", ""Minimax rate""","BA","""Post-Processed Posteriors for Banded Covariances""""We consider Bayesian inference of banded covariance matrices and propose a post-processed posterior. The post-processing of the posterior consists of two steps. In the first step, posterior samples are obtained from the conjugate inverse-Wishart posterior, which does not satisfy any structural restrictions. In the second step, the posterior samples are transformed to satisfy the structural restriction through a post-processing function. The conceptually straightforward procedure of the post-processed posterior makes its computation efficient and can render interval estimators of functionals of covariance matrices. We show that it has nearly optimal minimax rates for banded covariances among all possible pairs of priors and post-processing functions. Additionally, we provide a theorem on the credible set of the post-processed posterior under the finite dimension assumption. We prove that the expected coverage probability of the 100(1−α)% highest posterior density region of the post-processed posterior is asymptotically 1−α with respect to any conventional posterior distribution. It implies that the highest posterior density region of the post-processed posterior is, on average, a credible set of conventional posterior. The advantages of the post-processed posterior are demonstrated by a simulation study and a real data analysis.""""Bayesian"", ""high-dimensional analysis"", ""inverse-Wishart"", ""Minimax rate"""
"""10.1214/22-BA1334""","""Regularised B-splines Projected Gaussian Process Priors to Estimate Time-trends in Age-specific COVID-19 Deaths""","""The COVID-19 pandemic has caused severe public health consequences in the United States. In this study, we use a hierarchical Bayesian model to estimate the age-specific COVID-19 attributable deaths over time in the United States. The model is specified by a novel non-parametric spatial approach over time and age, a low-rank Gaussian Process (GP) projected by regularised B-splines. We show that this projection defines a GP with attractive smoothness and computational efficiency properties, derive its kernel function, and discuss the penalty terms induced by the projected GP. Simulation analyses and benchmark results show that the B-splines projected GP may perform better than standard B-splines and Bayesian P-splines, and equivalently well as a standard GP at considerably lower runtimes. We apply the model to weekly, age-stratified COVID-19 attributable deaths reported by the US Centers for Disease Control, which are subject to censoring and reporting biases. Using the B-splines projected GP, we can estimate longitudinal trends in COVID-19 associated deaths across the US by 1-year age bands. These estimates are instrumental to calculate age-specific mortality rates, describe variation in age-specific deaths across the US, and for fitting epidemic models. Here, we couple the model with age-specific vaccination rates to show that vaccination rates were significantly associated with the magnitude of resurgences in COVID-19 deaths during the summer 2021. With counterfactual analyses, we quantify the avoided COVID-19 deaths under lower vaccination rates and avoidable COVID-19 deaths under higher vaccination rates. The B-splines projected GP priors that we develop are likely an appealing addition to the arsenal of Bayesian regularising priors.""","""Bayesian statistics"", ""Covid-19"", ""Gaussian process approximations"", ""mortality"", ""Non-parametric statistics"", ""Stan probabilistic computing language""","BA","""Regularised B-splines Projected Gaussian Process Priors to Estimate Time-trends in Age-specific COVID-19 Deaths""""The COVID-19 pandemic has caused severe public health consequences in the United States. In this study, we use a hierarchical Bayesian model to estimate the age-specific COVID-19 attributable deaths over time in the United States. The model is specified by a novel non-parametric spatial approach over time and age, a low-rank Gaussian Process (GP) projected by regularised B-splines. We show that this projection defines a GP with attractive smoothness and computational efficiency properties, derive its kernel function, and discuss the penalty terms induced by the projected GP. Simulation analyses and benchmark results show that the B-splines projected GP may perform better than standard B-splines and Bayesian P-splines, and equivalently well as a standard GP at considerably lower runtimes. We apply the model to weekly, age-stratified COVID-19 attributable deaths reported by the US Centers for Disease Control, which are subject to censoring and reporting biases. Using the B-splines projected GP, we can estimate longitudinal trends in COVID-19 associated deaths across the US by 1-year age bands. These estimates are instrumental to calculate age-specific mortality rates, describe variation in age-specific deaths across the US, and for fitting epidemic models. Here, we couple the model with age-specific vaccination rates to show that vaccination rates were significantly associated with the magnitude of resurgences in COVID-19 deaths during the summer 2021. With counterfactual analyses, we quantify the avoided COVID-19 deaths under lower vaccination rates and avoidable COVID-19 deaths under higher vaccination rates. The B-splines projected GP priors that we develop are likely an appealing addition to the arsenal of Bayesian regularising priors.""""Bayesian statistics"", ""Covid-19"", ""Gaussian process approximations"", ""mortality"", ""Non-parametric statistics"", ""Stan probabilistic computing language"""
"""10.1214/22-BA1329""","""Fast Methods for Posterior Inference of Two-Group Normal-Normal Models""","""We describe a class of algorithms for evaluating posterior moments of certain Bayesian linear regression models with a normal likelihood and a normal prior on the regression coefficients. The proposed methods can be used for hierarchical mixed effects models with partial pooling over one group of predictors, as well as random effects models with partial pooling over two groups of predictors. We demonstrate the performance of the methods on two applications, one involving U.S. opinion polls and one involving the modeling of COVID-19 outbreaks in Israel using survey data. The algorithms involve analytical marginalization of regression coefficients followed by numerical integration of the remaining low-dimensional density. The dominant cost of the algorithms is an eigendecomposition computed once for each value of the outside parameter of integration. Our approach drastically reduces run times compared to state-of-the-art Markov chain Monte Carlo (MCMC) algorithms. The latter, in addition to being computationally expensive, can also be difficult to tune when applied to hierarchical models.""","""fast algorithms"", ""hierarchical modeling"", ""Linear regression"", ""mixed effects models""","BA","""Fast Methods for Posterior Inference of Two-Group Normal-Normal Models""""We describe a class of algorithms for evaluating posterior moments of certain Bayesian linear regression models with a normal likelihood and a normal prior on the regression coefficients. The proposed methods can be used for hierarchical mixed effects models with partial pooling over one group of predictors, as well as random effects models with partial pooling over two groups of predictors. We demonstrate the performance of the methods on two applications, one involving U.S. opinion polls and one involving the modeling of COVID-19 outbreaks in Israel using survey data. The algorithms involve analytical marginalization of regression coefficients followed by numerical integration of the remaining low-dimensional density. The dominant cost of the algorithms is an eigendecomposition computed once for each value of the outside parameter of integration. Our approach drastically reduces run times compared to state-of-the-art Markov chain Monte Carlo (MCMC) algorithms. The latter, in addition to being computationally expensive, can also be difficult to tune when applied to hierarchical models.""""fast algorithms"", ""hierarchical modeling"", ""Linear regression"", ""mixed effects models"""
"""10.1214/22-BA1314""","""Bayes Linear Bayes Networks with an Application to Prognostic Indices""","""Bayes linear kinematics and Bayes linear Bayes graphical models provide an extension of Bayes linear methods so that full conditional updates may be combined with Bayes linear belief adjustment. The use of Bayes linear kinematics eliminates the problem of non-commutativity which was observed in earlier work involving moment-based belief updates. In this paper we describe this approach and investigate its application to the rapid computation of prognostic index values in survival when a patient’s values may only be available for a subset of covariates. We consider the use of covariates of various kinds and introduce the use of non-conjugate marginal updates. We apply the technique to an example concerning patients with non-Hodgkin’s lymphoma, in which we treat the linear predictor of the lifetime distribution as a latent variable and use its expectation, given whatever covariates are available, as a prognostic index.""","""Bayes linear Bayes graphical model"", ""Bayes linear kinematics"", ""missing covariates"", ""prognostic index"", ""Survival analysis""","BA","""Bayes Linear Bayes Networks with an Application to Prognostic Indices""""Bayes linear kinematics and Bayes linear Bayes graphical models provide an extension of Bayes linear methods so that full conditional updates may be combined with Bayes linear belief adjustment. The use of Bayes linear kinematics eliminates the problem of non-commutativity which was observed in earlier work involving moment-based belief updates. In this paper we describe this approach and investigate its application to the rapid computation of prognostic index values in survival when a patient’s values may only be available for a subset of covariates. We consider the use of covariates of various kinds and introduce the use of non-conjugate marginal updates. We apply the technique to an example concerning patients with non-Hodgkin’s lymphoma, in which we treat the linear predictor of the lifetime distribution as a latent variable and use its expectation, given whatever covariates are available, as a prognostic index.""""Bayes linear Bayes graphical model"", ""Bayes linear kinematics"", ""missing covariates"", ""prognostic index"", ""Survival analysis"""
"""10.1214/22-BA1316""","""A Multi-Armed Bayesian Ordinal Outcome Utility-Based Sequential Trial with a Pairwise Null Clustering Prior""","""A multi-armed trial based on ordinal outcomes is proposed that leverages a flexible non-proportional odds cumulative logit model and numerical utility scores for each outcome to determine treatment optimality. This trial design uses a Bayesian clustering prior on the treatment effects that encourages the pairwise null hypothesis of no differences between treatments. A group sequential design is proposed to determine which treatments are clinically different with an adaptive decision boundary that becomes more aggressive as the sample size or clinical significance grows, or the number of active treatments decreases. A simulation study is conducted for 3 and 5 treatment arms, which shows that the design has superior operating characteristics (family wise error rate, generalized power, average sample size) compared to utility designs that do not allow clustering, a frequentist proportional odds model, or a permutation test based on empirical mean utilities.""","""Bayesian clustering"", ""group sequential trials"", ""multi-armed trials""","BA","""A Multi-Armed Bayesian Ordinal Outcome Utility-Based Sequential Trial with a Pairwise Null Clustering Prior""""A multi-armed trial based on ordinal outcomes is proposed that leverages a flexible non-proportional odds cumulative logit model and numerical utility scores for each outcome to determine treatment optimality. This trial design uses a Bayesian clustering prior on the treatment effects that encourages the pairwise null hypothesis of no differences between treatments. A group sequential design is proposed to determine which treatments are clinically different with an adaptive decision boundary that becomes more aggressive as the sample size or clinical significance grows, or the number of active treatments decreases. A simulation study is conducted for 3 and 5 treatment arms, which shows that the design has superior operating characteristics (family wise error rate, generalized power, average sample size) compared to utility designs that do not allow clustering, a frequentist proportional odds model, or a permutation test based on empirical mean utilities.""""Bayesian clustering"", ""group sequential trials"", ""multi-armed trials"""
"""10.1214/22-BA1317""","""Bayesian Uncertainty Quantification for Low-Rank Matrix Completion""","""We consider the problem of uncertainty quantification for an unknown low-rank matrix X, given a partial and noisy observation of its entries. This quantification of uncertainty is essential for many real-world problems, including image processing, satellite imaging, and seismology, providing a principled framework for validating scientific conclusions and guiding decision-making. However, existing literature has mainly focused on the completion (i.e., point estimation) of the matrix X, with little work on investigating its uncertainty. To this end, we propose in this work a new Bayesian modeling framework, called BayeSMG, which parametrizes the unknown X via its underlying row and column subspaces. This Bayesian subspace parametrization enables efficient posterior inference on matrix subspaces, which represents interpretable phenomena in many applications. This can then be leveraged for improved matrix recovery. We demonstrate the effectiveness of BayeSMG over existing Bayesian matrix recovery methods in numerical experiments, image inpainting, and a seismic sensor network application.""","""hierarchical modeling"", ""Manifold sampling"", ""Matrix completion"", ""matrix factorization"", ""Seismic imaging"", ""uncertainty quantification""","BA","""Bayesian Uncertainty Quantification for Low-Rank Matrix Completion""""We consider the problem of uncertainty quantification for an unknown low-rank matrix X, given a partial and noisy observation of its entries. This quantification of uncertainty is essential for many real-world problems, including image processing, satellite imaging, and seismology, providing a principled framework for validating scientific conclusions and guiding decision-making. However, existing literature has mainly focused on the completion (i.e., point estimation) of the matrix X, with little work on investigating its uncertainty. To this end, we propose in this work a new Bayesian modeling framework, called BayeSMG, which parametrizes the unknown X via its underlying row and column subspaces. This Bayesian subspace parametrization enables efficient posterior inference on matrix subspaces, which represents interpretable phenomena in many applications. This can then be leveraged for improved matrix recovery. We demonstrate the effectiveness of BayeSMG over existing Bayesian matrix recovery methods in numerical experiments, image inpainting, and a seismic sensor network application.""""hierarchical modeling"", ""Manifold sampling"", ""Matrix completion"", ""matrix factorization"", ""Seismic imaging"", ""uncertainty quantification"""
"""10.1214/22-BA1312""","""Normal Approximation for Bayesian Mixed Effects Binomial Regression Models""","""Bayesian inference for generalized linear mixed models implemented with Markov chain Monte Carlo (MCMC) sampling methods have been widely used. In this paper, we propose to substitute a large sample normal approximation for the intractable full conditional distribution of the latent effects (of size k) in order to simplify the computation. In addition, we develop a second approximation involving what we term a sufficient reduction (SR). We show that the full conditional distributions for the model parameters only depend on a small, say r≪k, dimensional function of the latent effects, and also that this reduction is asymptotically normal under mild conditions. Thus we substitute the sampling of an r dimensional multivariate normal for sampling the k dimensional full conditional for the latent effects. Applications to oncology physician data, to cow abortion data and simulation studies confirm the reasonable performance of the proposed approximation method in terms of estimation accuracy and computational speed.""","""Asymptotic approximation"", ""Binomial regression"", ""generalized linear mixed models"", ""Markov chain Monte Carlo"", ""sufficient reduction""","BA","""Normal Approximation for Bayesian Mixed Effects Binomial Regression Models""""Bayesian inference for generalized linear mixed models implemented with Markov chain Monte Carlo (MCMC) sampling methods have been widely used. In this paper, we propose to substitute a large sample normal approximation for the intractable full conditional distribution of the latent effects (of size k) in order to simplify the computation. In addition, we develop a second approximation involving what we term a sufficient reduction (SR). We show that the full conditional distributions for the model parameters only depend on a small, say r≪k, dimensional function of the latent effects, and also that this reduction is asymptotically normal under mild conditions. Thus we substitute the sampling of an r dimensional multivariate normal for sampling the k dimensional full conditional for the latent effects. Applications to oncology physician data, to cow abortion data and simulation studies confirm the reasonable performance of the proposed approximation method in terms of estimation accuracy and computational speed.""""Asymptotic approximation"", ""Binomial regression"", ""generalized linear mixed models"", ""Markov chain Monte Carlo"", ""sufficient reduction"""
"""10.1214/22-BA1318""","""Bayesian Approximations to Hidden Semi-Markov Models for Telemetric Monitoring of Physical Activity""","""We propose a Bayesian hidden Markov model for analyzing time series and sequential data where a special structure of the transition probability matrix is embedded to model explicit-duration semi-Markovian dynamics. Our formulation allows for the development of highly flexible and interpretable models that can integrate available prior information on state durations while keeping a moderate computational cost to perform efficient posterior inference. We show the benefits of choosing a Bayesian approach for HSMM estimation over its frequentist counterpart, in terms of model selection and out-of-sample forecasting, also highlighting the computational feasibility of our inference procedure whilst incurring negligible statistical error. The use of our methodology is illustrated in an application relevant to e-Health, where we investigate rest-activity rhythms using telemetric activity data collected via a wearable sensing device. This analysis considers for the first time Bayesian model selection for the form of the explicit state dwell distribution. We further investigate the inclusion of a circadian covariate into the emission density and estimate this in a data-driven manner.""","""Bayes factor"", ""circadian rhythm"", ""Hamiltonian Monte Carlo"", ""Markov switching process"", ""telemetric activity data""","BA","""Bayesian Approximations to Hidden Semi-Markov Models for Telemetric Monitoring of Physical Activity""""We propose a Bayesian hidden Markov model for analyzing time series and sequential data where a special structure of the transition probability matrix is embedded to model explicit-duration semi-Markovian dynamics. Our formulation allows for the development of highly flexible and interpretable models that can integrate available prior information on state durations while keeping a moderate computational cost to perform efficient posterior inference. We show the benefits of choosing a Bayesian approach for HSMM estimation over its frequentist counterpart, in terms of model selection and out-of-sample forecasting, also highlighting the computational feasibility of our inference procedure whilst incurring negligible statistical error. The use of our methodology is illustrated in an application relevant to e-Health, where we investigate rest-activity rhythms using telemetric activity data collected via a wearable sensing device. This analysis considers for the first time Bayesian model selection for the form of the explicit state dwell distribution. We further investigate the inclusion of a circadian covariate into the emission density and estimate this in a data-driven manner.""""Bayes factor"", ""circadian rhythm"", ""Hamiltonian Monte Carlo"", ""Markov switching process"", ""telemetric activity data"""
"""10.1214/22-BA1321""","""Distributed Computation for Marginal Likelihood based Model Choice""","""We propose a general method for distributed Bayesian model choice, using the marginal likelihood, where a data set is split in non-overlapping subsets. These subsets are only accessed locally by individual workers and no data is shared between the workers. We approximate the model evidence for the full data set through Monte Carlo sampling from the posterior on every subset generating a model evidence per subset. The results are combined using a novel approach which corrects for the splitting using summary statistics of the generated samples. Our divide-and-conquer approach enables Bayesian model choice in the large data setting, exploiting all available information but limiting communication between workers. We derive theoretical error bounds that quantify the resulting trade-off between computational gain and loss in precision. The embarrassingly parallel nature yields important speed-ups when used on massive data sets as illustrated by our real world experiments. In addition, we show how the suggested approach can be extended to model choice within a reversible jump setting that explores multiple feature combinations within one run.""","""Distributed computation"", ""marginal likelihood""","BA","""Distributed Computation for Marginal Likelihood based Model Choice""""We propose a general method for distributed Bayesian model choice, using the marginal likelihood, where a data set is split in non-overlapping subsets. These subsets are only accessed locally by individual workers and no data is shared between the workers. We approximate the model evidence for the full data set through Monte Carlo sampling from the posterior on every subset generating a model evidence per subset. The results are combined using a novel approach which corrects for the splitting using summary statistics of the generated samples. Our divide-and-conquer approach enables Bayesian model choice in the large data setting, exploiting all available information but limiting communication between workers. We derive theoretical error bounds that quantify the resulting trade-off between computational gain and loss in precision. The embarrassingly parallel nature yields important speed-ups when used on massive data sets as illustrated by our real world experiments. In addition, we show how the suggested approach can be extended to model choice within a reversible jump setting that explores multiple feature combinations within one run.""""Distributed computation"", ""marginal likelihood"""
"""10.1214/22-BA1308""","""Shrinkage with Shrunken Shoulders: Gibbs Sampling Shrinkage Model Posteriors with Guaranteed Convergence Rates""","""Use of continuous shrinkage priors — with a “spike” near zero and heavy-tails towards infinity — is an increasingly popular approach to induce sparsity in parameter estimates. When the parameters are only weakly identified by the likelihood, however, the posterior may end up with tails as heavy as the prior, jeopardizing robustness of inference. A natural solution is to “shrink the shoulders” of a shrinkage prior by lightening up its tails beyond a reasonable parameter range, yielding a regularized version of the prior. We develop a regularization approach which, unlike previous proposals, preserves computationally attractive structures of original shrinkage priors. We study theoretical properties of the Gibbs sampler on resulting posterior distributions, with emphasis on convergence rates of the Pólya-Gamma Gibbs sampler for sparse logistic regression. Our analysis shows that the proposed regularization leads to geometric ergodicity under a broad range of global-local shrinkage priors. Essentially, the only requirement is for the prior πlocal(⋅) on the local scale λ to satisfy πlocal(0) 0, as in the case of Bayesian bridge priors, we show the sampler to be uniformly ergodic.""","""Bayesian inference"", ""ergodicity"", ""generalized linear model"", ""Markov chain Monte Carlo"", ""Sparsity""","BA","""Shrinkage with Shrunken Shoulders: Gibbs Sampling Shrinkage Model Posteriors with Guaranteed Convergence Rates""""Use of continuous shrinkage priors — with a “spike” near zero and heavy-tails towards infinity — is an increasingly popular approach to induce sparsity in parameter estimates. When the parameters are only weakly identified by the likelihood, however, the posterior may end up with tails as heavy as the prior, jeopardizing robustness of inference. A natural solution is to “shrink the shoulders” of a shrinkage prior by lightening up its tails beyond a reasonable parameter range, yielding a regularized version of the prior. We develop a regularization approach which, unlike previous proposals, preserves computationally attractive structures of original shrinkage priors. We study theoretical properties of the Gibbs sampler on resulting posterior distributions, with emphasis on convergence rates of the Pólya-Gamma Gibbs sampler for sparse logistic regression. Our analysis shows that the proposed regularization leads to geometric ergodicity under a broad range of global-local shrinkage priors. Essentially, the only requirement is for the prior πlocal(⋅) on the local scale λ to satisfy πlocal(0) 0, as in the case of Bayesian bridge priors, we show the sampler to be uniformly ergodic.""""Bayesian inference"", ""ergodicity"", ""generalized linear model"", ""Markov chain Monte Carlo"", ""Sparsity"""
"""10.1214/22-BA1315""","""A Bayesian Approach for Partial Gaussian Graphical Models With Sparsity""","""We explore various Bayesian approaches to estimate partial Gaussian graphical models. Our hierarchical structures enable to deal with single-output as well as multiple-output linear regressions, in small or high dimension, enforcing either no sparsity, sparsity, group sparsity or even sparse-group sparsity for a bi-level selection through partial correlations (direct links) between predictors and responses, thanks to spike-and-slab priors corresponding to each setting. Adaptative and global shrinkages are also incorporated in the Bayesian modeling of the direct links. An existing result for model selection consistency is reformulated to stick to our sparse and group-sparse settings, providing a theoretical guarantee under some technical assumptions. Gibbs samplers are developed and a simulation study shows the efficiency of our models which give very competitive results, especially in terms of support recovery. To conclude, a real dataset is investigated.""","""Bayesian approach"", ""Gibbs sampler"", ""high-dimensional linear regression"", ""partial correlation"", ""partial graphical model"", ""Sparsity"", ""spike-and-slab""","BA","""A Bayesian Approach for Partial Gaussian Graphical Models With Sparsity""""We explore various Bayesian approaches to estimate partial Gaussian graphical models. Our hierarchical structures enable to deal with single-output as well as multiple-output linear regressions, in small or high dimension, enforcing either no sparsity, sparsity, group sparsity or even sparse-group sparsity for a bi-level selection through partial correlations (direct links) between predictors and responses, thanks to spike-and-slab priors corresponding to each setting. Adaptative and global shrinkages are also incorporated in the Bayesian modeling of the direct links. An existing result for model selection consistency is reformulated to stick to our sparse and group-sparse settings, providing a theoretical guarantee under some technical assumptions. Gibbs samplers are developed and a simulation study shows the efficiency of our models which give very competitive results, especially in terms of support recovery. To conclude, a real dataset is investigated.""""Bayesian approach"", ""Gibbs sampler"", ""high-dimensional linear regression"", ""partial correlation"", ""partial graphical model"", ""Sparsity"", ""spike-and-slab"""
"""10.1214/22-BA1320""","""Bayesian Spatial Homogeneity Pursuit of Functional Data: An Application to the U.S. Income Distribution""","""An income distribution describes how an entity’s total wealth is distributed amongst its population. A problem of interest to regional economics researchers is to understand the spatial homogeneity of income distributions among different regions. In economics, the Lorenz curve is a well-known functional representation of income distribution. In this article, we propose a mixture of finite mixtures (MFM) model as well as a Markov random field constrained mixture of finite mixtures (MRFC-MFM) model in the context of spatial functional data analysis to capture spatial homogeneity of Lorenz curves. We design efficient Markov chain Monte Carlo (MCMC) algorithms to simultaneously infer the posterior distributions of the number of clusters and the clustering configuration of spatial functional data. Extensive simulation studies are carried out to show the effectiveness of the proposed methods compared with existing methods. We apply the proposed spatial functional clustering method to state level income Lorenz curves from the American Community Survey Public Use Microdata Sample (PUMS) data. The results reveal a number of important clustering patterns of state-level income distributions across the US.""","""Lorenz curve"", ""Markov random field"", ""mixture of finite mixtures"", ""spatial functional data clustering""","BA","""Bayesian Spatial Homogeneity Pursuit of Functional Data: An Application to the U.S. Income Distribution""""An income distribution describes how an entity’s total wealth is distributed amongst its population. A problem of interest to regional economics researchers is to understand the spatial homogeneity of income distributions among different regions. In economics, the Lorenz curve is a well-known functional representation of income distribution. In this article, we propose a mixture of finite mixtures (MFM) model as well as a Markov random field constrained mixture of finite mixtures (MRFC-MFM) model in the context of spatial functional data analysis to capture spatial homogeneity of Lorenz curves. We design efficient Markov chain Monte Carlo (MCMC) algorithms to simultaneously infer the posterior distributions of the number of clusters and the clustering configuration of spatial functional data. Extensive simulation studies are carried out to show the effectiveness of the proposed methods compared with existing methods. We apply the proposed spatial functional clustering method to state level income Lorenz curves from the American Community Survey Public Use Microdata Sample (PUMS) data. The results reveal a number of important clustering patterns of state-level income distributions across the US.""""Lorenz curve"", ""Markov random field"", ""mixture of finite mixtures"", ""spatial functional data clustering"""
"""10.1214/22-BA1307""","""Informative Priors for the Consensus Ranking in the Bayesian Mallows Model""","""The aim of this work is to study the problem of prior elicitation for the consensus ranking in the Mallows model with Spearman’s distance, a popular distance-based model for rankings or permutation data. Previous Bayesian inference for such a model has been limited to the use of the uniform prior over the space of permutations. We present a novel strategy to elicit informative prior beliefs on the location parameter of the model, discussing the interpretation of hyper-parameters and the implication of prior choices for the posterior analysis.""","""Bayesian subjective inference"", ""conjugate priors"", ""Mallows model for rankings"", ""permutations"", ""permutohedron"", ""Ranking data""","BA","""Informative Priors for the Consensus Ranking in the Bayesian Mallows Model""""The aim of this work is to study the problem of prior elicitation for the consensus ranking in the Mallows model with Spearman’s distance, a popular distance-based model for rankings or permutation data. Previous Bayesian inference for such a model has been limited to the use of the uniform prior over the space of permutations. We present a novel strategy to elicit informative prior beliefs on the location parameter of the model, discussing the interpretation of hyper-parameters and the implication of prior choices for the posterior analysis.""""Bayesian subjective inference"", ""conjugate priors"", ""Mallows model for rankings"", ""permutations"", ""permutohedron"", ""Ranking data"""
"""10.1214/21-BA1293""","""Deep Gaussian Processes for Calibration of Computer Models (with Discussion)""","""Bayesian calibration of black-box computer models offers an established framework for quantification of uncertainty of model parameters and predictions. Traditional Bayesian calibration involves the emulation of the computer model and an additive model discrepancy term using Gaussian processes; inference is then carried out using Markov chain Monte Carlo. This calibration approach is limited by the poor scalability of Gaussian processes and by the need to specify a sensible covariance function to deal with the complexity of the computer model and the discrepancy. In this work, we propose a novel calibration framework, where these challenges are addressed by means of compositions of Gaussian processes into Deep Gaussian processes and scalable variational inference techniques. Thanks to this formulation, it is possible to obtain a flexible calibration approach, which is easy to implement in development environments featuring automatic differentiation and exploiting GPU-type hardware. We show how our proposal yields a powerful alternative to the state-of-the-art by means of experimental validations on various calibration problems. We conclude the paper by showing how we can carry out adaptive experimental design, and by discussing the identifiability properties of the proposed calibration model.""","""Bayesian inference"", ""computer experiments"", ""deep Gaussian process"", ""Neural nets""","BA","""Deep Gaussian Processes for Calibration of Computer Models (with Discussion)""""Bayesian calibration of black-box computer models offers an established framework for quantification of uncertainty of model parameters and predictions. Traditional Bayesian calibration involves the emulation of the computer model and an additive model discrepancy term using Gaussian processes; inference is then carried out using Markov chain Monte Carlo. This calibration approach is limited by the poor scalability of Gaussian processes and by the need to specify a sensible covariance function to deal with the complexity of the computer model and the discrepancy. In this work, we propose a novel calibration framework, where these challenges are addressed by means of compositions of Gaussian processes into Deep Gaussian processes and scalable variational inference techniques. Thanks to this formulation, it is possible to obtain a flexible calibration approach, which is easy to implement in development environments featuring automatic differentiation and exploiting GPU-type hardware. We show how our proposal yields a powerful alternative to the state-of-the-art by means of experimental validations on various calibration problems. We conclude the paper by showing how we can carry out adaptive experimental design, and by discussing the identifiability properties of the proposed calibration model.""""Bayesian inference"", ""computer experiments"", ""deep Gaussian process"", ""Neural nets"""
"""10.1214/21-BA1297""","""Bayesian Causal Inference with Bipartite Record Linkage""","""In some scenarios, the observational data needed for causal inferences are spread over two data files. In particular, we consider scenarios where one file includes covariates and the treatment measured on a set of individuals, and a second file includes responses measured on another, partially overlapping set of individuals. In the absence of error-free direct identifiers like social security numbers, straightforward merging of separate files is not feasible, so that records must be linked using error-prone variables such as names, birth dates, and demographic characteristics. Typical practice in such situations generally follows a two-stage procedure: first link the two files using a probabilistic linkage technique, then make causal inferences with the linked dataset. This does not propagate uncertainty due to imperfect linkages to the causal inference, nor does it leverage relationships among the study variables to improve the quality of the linkages. We propose a joint model for simultaneous Bayesian inference on probabilistic linkage and causal effects that addresses these deficiencies. Using simulation studies and theoretical arguments, we show that the joint model can improve the accuracy of estimated treatment effects, as well as the record linkages, compared to the two-stage modeling option. We illustrate the joint model using a constructed causal study of the effects of debit card possession on household spending.""","""Fusion"", ""Matching"", ""observational"", ""propensity"", ""treatment""","BA","""Bayesian Causal Inference with Bipartite Record Linkage""""In some scenarios, the observational data needed for causal inferences are spread over two data files. In particular, we consider scenarios where one file includes covariates and the treatment measured on a set of individuals, and a second file includes responses measured on another, partially overlapping set of individuals. In the absence of error-free direct identifiers like social security numbers, straightforward merging of separate files is not feasible, so that records must be linked using error-prone variables such as names, birth dates, and demographic characteristics. Typical practice in such situations generally follows a two-stage procedure: first link the two files using a probabilistic linkage technique, then make causal inferences with the linked dataset. This does not propagate uncertainty due to imperfect linkages to the causal inference, nor does it leverage relationships among the study variables to improve the quality of the linkages. We propose a joint model for simultaneous Bayesian inference on probabilistic linkage and causal effects that addresses these deficiencies. Using simulation studies and theoretical arguments, we show that the joint model can improve the accuracy of estimated treatment effects, as well as the record linkages, compared to the two-stage modeling option. We illustrate the joint model using a constructed causal study of the effects of debit card possession on household spending.""""Fusion"", ""Matching"", ""observational"", ""propensity"", ""treatment"""
"""10.1214/21-BA1287""","""Bayesian Hierarchical Stacking: Some Models Are (Somewhere) Useful""","""Stacking is a widely used model averaging technique that asymptotically yields optimal predictions among linear averages. We show that stacking is most effective when model predictive performance is heterogeneous in inputs, and we can further improve the stacked mixture with a hierarchical model. We generalize stacking to Bayesian hierarchical stacking. The model weights are varying as a function of data, partially-pooled, and inferred using Bayesian inference. We further incorporate discrete and continuous inputs, other structured priors, and time series and longitudinal data. To verify the performance gain of the proposed method, we derive theory bounds, and demonstrate on several applied problems.""","""Bayesian hierarchical modeling"", ""conditional prediction"", ""covariate shift"", ""model averaging"", ""prior construction"", ""stacking""","BA","""Bayesian Hierarchical Stacking: Some Models Are (Somewhere) Useful""""Stacking is a widely used model averaging technique that asymptotically yields optimal predictions among linear averages. We show that stacking is most effective when model predictive performance is heterogeneous in inputs, and we can further improve the stacked mixture with a hierarchical model. We generalize stacking to Bayesian hierarchical stacking. The model weights are varying as a function of data, partially-pooled, and inferred using Bayesian inference. We further incorporate discrete and continuous inputs, other structured priors, and time series and longitudinal data. To verify the performance gain of the proposed method, we derive theory bounds, and demonstrate on several applied problems.""""Bayesian hierarchical modeling"", ""conditional prediction"", ""covariate shift"", ""model averaging"", ""prior construction"", ""stacking"""
"""10.1214/21-BA1296""","""Bayesian Nonparametric Density Autoregression with Lag Selection""","""We develop a Bayesian nonparametric autoregressive model applied to flexibly estimate general transition densities exhibiting nonlinear lag dependence. Our approach is related to Bayesian density regression using Dirichlet process mixtures, with the Markovian likelihood defined through the conditional distribution obtained from the mixture. This results in a Bayesian nonparametric extension of a mixtures-of-experts model formulation. We address computational challenges to posterior sampling that arise from the Markovian structure in the likelihood. The base model is illustrated with synthetic data from a classical model for population dynamics, as well as a series of waiting times between eruptions of Old Faithful Geyser. We study inferences available through the base model before extending the methodology to include automatic relevance detection among a pre-specified set of lags. Inference for global and local lag selection is explored with additional simulation studies, and the methods are illustrated through analysis of an annual time series of pink salmon abundance in a stream in Alaska. We further explore and compare transition density estimation performance for alternative configurations of the proposed model. Supplementary materials are available online.""","""Dirichlet process mixtures"", ""dynamical system"", ""local regression"", ""Markov chain Monte Carlo"", ""order selection""","BA","""Bayesian Nonparametric Density Autoregression with Lag Selection""""We develop a Bayesian nonparametric autoregressive model applied to flexibly estimate general transition densities exhibiting nonlinear lag dependence. Our approach is related to Bayesian density regression using Dirichlet process mixtures, with the Markovian likelihood defined through the conditional distribution obtained from the mixture. This results in a Bayesian nonparametric extension of a mixtures-of-experts model formulation. We address computational challenges to posterior sampling that arise from the Markovian structure in the likelihood. The base model is illustrated with synthetic data from a classical model for population dynamics, as well as a series of waiting times between eruptions of Old Faithful Geyser. We study inferences available through the base model before extending the methodology to include automatic relevance detection among a pre-specified set of lags. Inference for global and local lag selection is explored with additional simulation studies, and the methods are illustrated through analysis of an annual time series of pink salmon abundance in a stream in Alaska. We further explore and compare transition density estimation performance for alternative configurations of the proposed model. Supplementary materials are available online.""""Dirichlet process mixtures"", ""dynamical system"", ""local regression"", ""Markov chain Monte Carlo"", ""order selection"""
"""10.1214/21-BA1286""","""Bayesian Decision-Theoretic Design of Experiments Under an Alternative Model""","""Traditionally Bayesian decision-theoretic design of experiments proceeds by choosing a design to minimise expectation of a given loss function over the space of all designs. The loss function encapsulates the aim of the experiment, and the expectation is taken with respect to the joint distribution of all unknown quantities implied by the statistical model that will be fitted to observed responses. In this paper, an extended framework is proposed whereby the expectation of the loss is taken with respect to a joint distribution implied by an alternative statistical model. Motivation for this includes promoting robustness, ensuring computational feasibility and for allowing realistic prior specification when deriving a design. To aid in exploring the new framework, an asymptotic approximation to the expected loss under an alternative model is derived, and the properties of different loss functions are established. The framework is then demonstrated on a linear regression versus full-treatment model scenario, on estimating parameters of a non-linear model under model discrepancy and a cubic spline model under an unknown number of basis functions.""","""cubic spline basis"", ""expected loss function"", ""full-treatment model"", ""model discrepancy"", ""Non-linear model"", ""normal linear model""","BA","""Bayesian Decision-Theoretic Design of Experiments Under an Alternative Model""""Traditionally Bayesian decision-theoretic design of experiments proceeds by choosing a design to minimise expectation of a given loss function over the space of all designs. The loss function encapsulates the aim of the experiment, and the expectation is taken with respect to the joint distribution of all unknown quantities implied by the statistical model that will be fitted to observed responses. In this paper, an extended framework is proposed whereby the expectation of the loss is taken with respect to a joint distribution implied by an alternative statistical model. Motivation for this includes promoting robustness, ensuring computational feasibility and for allowing realistic prior specification when deriving a design. To aid in exploring the new framework, an asymptotic approximation to the expected loss under an alternative model is derived, and the properties of different loss functions are established. The framework is then demonstrated on a linear regression versus full-treatment model scenario, on estimating parameters of a non-linear model under model discrepancy and a cubic spline model under an unknown number of basis functions.""""cubic spline basis"", ""expected loss function"", ""full-treatment model"", ""model discrepancy"", ""Non-linear model"", ""normal linear model"""
"""10.1214/21-BA1292""","""Bayesian Sparse Spiked Covariance Model with a Continuous Matrix Shrinkage Prior""","""We propose a Bayesian methodology for estimating spiked covariance matrices with a jointly sparse structure in high dimensions. The spiked covariance matrix is reparameterized in terms of the latent factor model, where the loading matrix is equipped with a novel matrix spike-and-slab LASSO prior, which is a continuous shrinkage prior for modeling jointly sparse matrices. We establish the rate-optimal posterior contraction for the covariance matrix with respect to the spectral norm as well as that for the principal subspace with respect to the projection spectral norm loss. We also study the posterior contraction rate of the principal subspace with respect to the two-to-infinity norm loss, a novel loss function measuring the distance between subspaces that is able to capture entrywise eigenvector perturbations. We show that the posterior contraction rate with respect to the two-to-infinity norm loss is tighter than that with respect to the routinely used projection spectral norm loss under certain low-rank and bounded coherence conditions. In addition, a point estimator for the principal subspace is proposed with the rate-optimal risk bound with respect to the projection spectral norm loss. The numerical performance of the proposed methodology is assessed through synthetic examples and the analysis of a real-world face data example.""","""joint sparsity"", ""latent factor model"", ""matrix spike-and-slab LASSO"", ""rate-optimal posterior contraction"", ""two-to-infinity norm loss""","BA","""Bayesian Sparse Spiked Covariance Model with a Continuous Matrix Shrinkage Prior""""We propose a Bayesian methodology for estimating spiked covariance matrices with a jointly sparse structure in high dimensions. The spiked covariance matrix is reparameterized in terms of the latent factor model, where the loading matrix is equipped with a novel matrix spike-and-slab LASSO prior, which is a continuous shrinkage prior for modeling jointly sparse matrices. We establish the rate-optimal posterior contraction for the covariance matrix with respect to the spectral norm as well as that for the principal subspace with respect to the projection spectral norm loss. We also study the posterior contraction rate of the principal subspace with respect to the two-to-infinity norm loss, a novel loss function measuring the distance between subspaces that is able to capture entrywise eigenvector perturbations. We show that the posterior contraction rate with respect to the two-to-infinity norm loss is tighter than that with respect to the routinely used projection spectral norm loss under certain low-rank and bounded coherence conditions. In addition, a point estimator for the principal subspace is proposed with the rate-optimal risk bound with respect to the projection spectral norm loss. The numerical performance of the proposed methodology is assessed through synthetic examples and the analysis of a real-world face data example.""""joint sparsity"", ""latent factor model"", ""matrix spike-and-slab LASSO"", ""rate-optimal posterior contraction"", ""two-to-infinity norm loss"""
"""10.1214/21-BA1290""","""Functional Central Limit Theorems for Stick-Breaking Priors""","""We obtain the strong law of large numbers, Glivenko-Cantelli theorem, central limit theorem, functional central limit theorem for various Bayesian nonparametric priors which include the stick-breaking process with general stick-breaking weights, the two-parameter Poisson-Dirichlet process, the normalized inverse Gaussian process, the normalized generalized gamma process, and the generalized Dirichlet process. For the stick-breaking process with general stick-breaking weights, we introduce two general conditions such that the central limit theorem and functional central limit theorem hold. Except in the case of the generalized Dirichlet process, since the finite dimensional distributions of these processes are either hard to obtain or are complicated to use even they are available, we use the method of moments to obtain the convergence results. For the generalized Dirichlet process we use its marginal distributions to obtain the asymptotics although the computations are highly technical.""","""Bayesian nonparametric priors"", ""Dirichlet process"", ""functional central limit theorem"", ""generalized Dirichlet process"", ""normalized generalized gamma process"", ""normalized inverse Gaussian process"", ""stick-breaking process"", ""Strong law of large numbers"", ""Two-parameter Poisson-Dirichlet process""","BA","""Functional Central Limit Theorems for Stick-Breaking Priors""""We obtain the strong law of large numbers, Glivenko-Cantelli theorem, central limit theorem, functional central limit theorem for various Bayesian nonparametric priors which include the stick-breaking process with general stick-breaking weights, the two-parameter Poisson-Dirichlet process, the normalized inverse Gaussian process, the normalized generalized gamma process, and the generalized Dirichlet process. For the stick-breaking process with general stick-breaking weights, we introduce two general conditions such that the central limit theorem and functional central limit theorem hold. Except in the case of the generalized Dirichlet process, since the finite dimensional distributions of these processes are either hard to obtain or are complicated to use even they are available, we use the method of moments to obtain the convergence results. For the generalized Dirichlet process we use its marginal distributions to obtain the asymptotics although the computations are highly technical.""""Bayesian nonparametric priors"", ""Dirichlet process"", ""functional central limit theorem"", ""generalized Dirichlet process"", ""normalized generalized gamma process"", ""normalized inverse Gaussian process"", ""stick-breaking process"", ""Strong law of large numbers"", ""Two-parameter Poisson-Dirichlet process"""
"""10.1214/21-BA1295""","""Gaussian Orthogonal Latent Factor Processes for Large Incomplete Matrices of Correlated Data""","""We introduce Gaussian orthogonal latent factor processes for modeling and predicting large correlated data. To handle the computational challenge, we first decompose the likelihood function of the Gaussian random field with a multi-dimensional input domain into a product of densities at the orthogonal components with lower-dimensional inputs. The continuous-time Kalman filter is implemented to compute the likelihood function efficiently without making approximations. We also show that the posterior distribution of the factor processes is independent, as a consequence of prior independence of factor processes and orthogonal factor loading matrix. For studies with large sample sizes, we propose a flexible way to model the mean, and we derive the marginal posterior distribution to solve identifiability issues in sampling these parameters. Both simulated and real data applications confirm the outstanding performance of this method.""","""correlated data"", ""Gaussian processes"", ""marginalization"", ""orthogonality""","BA","""Gaussian Orthogonal Latent Factor Processes for Large Incomplete Matrices of Correlated Data""""We introduce Gaussian orthogonal latent factor processes for modeling and predicting large correlated data. To handle the computational challenge, we first decompose the likelihood function of the Gaussian random field with a multi-dimensional input domain into a product of densities at the orthogonal components with lower-dimensional inputs. The continuous-time Kalman filter is implemented to compute the likelihood function efficiently without making approximations. We also show that the posterior distribution of the factor processes is independent, as a consequence of prior independence of factor processes and orthogonal factor loading matrix. For studies with large sample sizes, we propose a flexible way to model the mean, and we derive the marginal posterior distribution to solve identifiability issues in sampling these parameters. Both simulated and real data applications confirm the outstanding performance of this method.""""correlated data"", ""Gaussian processes"", ""marginalization"", ""orthogonality"""
"""10.1214/21-BA1298""","""Finite Mixtures of ERGMs for Modeling Ensembles of Networks""","""Ensembles of networks arise in many scientific fields, but there are relatively few statistical tools for inferring their generative processes, particularly in the presence of both dyadic dependence and cross-graph heterogeneity. To address this gap, we propose characterizing network ensembles via finite mixtures of exponential family random graph models (ERGMs), a class of parametric statistical models that has been successful in explicitly modeling the complex stochastic processes that govern the structure of edges in a network. Our proposed modeling framework can also be used for applications such as model-based clustering of ensembles of networks and density estimation for complex graph distributions. We develop a joint approach to estimate the number of mixture components and identify cluster-specific parameters simultaneously as well as to obtain an identified model under the Bayesian paradigm. Specifically, we develop a Metropolis-within-Gibbs algorithm to perform Bayesian inference, and estimate the number of mixture components using a strategy of deliberate overfitting with sparse priors that removes excess components during MCMC. As the true ERGM likelihood is generally intractable for model specifications with dyadic dependence terms, we consider two tractable approximations (pseudolikelihood and adjusted pseudolikelihood) to facilitate efficient statistical inference. We run simulation studies to compare the performance of these two approximations with respect to multiple metrics, showing conditions under which both are useful. We demonstrate the utility of the proposed approach using an ensemble of political co-voting networks among U.S. Senators and an ensemble of brain functional connectivity networks.""","""adjusted pseudolikelihood"", ""Bayesian mixture model"", ""brain functional connectivity networks"", ""exponential-family random graph models (ERGMs)"", ""MCMC"", ""political co-voting networks""","BA","""Finite Mixtures of ERGMs for Modeling Ensembles of Networks""""Ensembles of networks arise in many scientific fields, but there are relatively few statistical tools for inferring their generative processes, particularly in the presence of both dyadic dependence and cross-graph heterogeneity. To address this gap, we propose characterizing network ensembles via finite mixtures of exponential family random graph models (ERGMs), a class of parametric statistical models that has been successful in explicitly modeling the complex stochastic processes that govern the structure of edges in a network. Our proposed modeling framework can also be used for applications such as model-based clustering of ensembles of networks and density estimation for complex graph distributions. We develop a joint approach to estimate the number of mixture components and identify cluster-specific parameters simultaneously as well as to obtain an identified model under the Bayesian paradigm. Specifically, we develop a Metropolis-within-Gibbs algorithm to perform Bayesian inference, and estimate the number of mixture components using a strategy of deliberate overfitting with sparse priors that removes excess components during MCMC. As the true ERGM likelihood is generally intractable for model specifications with dyadic dependence terms, we consider two tractable approximations (pseudolikelihood and adjusted pseudolikelihood) to facilitate efficient statistical inference. We run simulation studies to compare the performance of these two approximations with respect to multiple metrics, showing conditions under which both are useful. We demonstrate the utility of the proposed approach using an ensemble of political co-voting networks among U.S. Senators and an ensemble of brain functional connectivity networks.""""adjusted pseudolikelihood"", ""Bayesian mixture model"", ""brain functional connectivity networks"", ""exponential-family random graph models (ERGMs)"", ""MCMC"", ""political co-voting networks"""
"""10.1214/21-BA1288""","""Power-Expected-Posterior Priors as Mixtures of <i>g</i>-Priors in Normal Linear Models""","""One of the main approaches used to construct prior distributions for objective Bayes methods is the concept of random imaginary observations. Under this setup, the expected-posterior prior (EPP) offers several advantages, among which it has a nice and simple interpretation and provides an effective way to establish compatibility of priors among models. In this paper, we study the power-expected-posterior prior as a generalization to the EPP in objective Bayesian model selection under normal linear models. We prove that it can be represented as a mixture of g-prior, like a wide range of prior distributions under normal linear models, and thus posterior distributions and Bayes factors are derived in closed form, keeping therefore its computational tractability. Following this result, we can naturally prove that desiderata (criteria for objective Bayesian model comparison) hold for the PEP prior. Comparisons with other mixtures of g-prior are made and results are presented in simulated and real-life datasets.""","""Bayesian model comparison"", ""expected-posterior priors"", ""imaginary training samples"", ""mixtures of g-priors"", ""objective priors""","BA","""Power-Expected-Posterior Priors as Mixtures of <i>g</i>-Priors in Normal Linear Models""""One of the main approaches used to construct prior distributions for objective Bayes methods is the concept of random imaginary observations. Under this setup, the expected-posterior prior (EPP) offers several advantages, among which it has a nice and simple interpretation and provides an effective way to establish compatibility of priors among models. In this paper, we study the power-expected-posterior prior as a generalization to the EPP in objective Bayesian model selection under normal linear models. We prove that it can be represented as a mixture of g-prior, like a wide range of prior distributions under normal linear models, and thus posterior distributions and Bayes factors are derived in closed form, keeping therefore its computational tractability. Following this result, we can naturally prove that desiderata (criteria for objective Bayesian model comparison) hold for the PEP prior. Comparisons with other mixtures of g-prior are made and results are presented in simulated and real-life datasets.""""Bayesian model comparison"", ""expected-posterior priors"", ""imaginary training samples"", ""mixtures of g-priors"", ""objective priors"""
"""10.1214/21-BA1291""","""Informative Bayesian Neural Network Priors for Weak Signals""","""Encoding domain knowledge into the prior over the high-dimensional weight space of a neural network is challenging but essential in applications with limited data and weak signals. Two types of domain knowledge are commonly available in scientific applications: 1. feature sparsity (fraction of features deemed relevant); 2. signal-to-noise ratio, quantified, for instance, as the proportion of variance explained. We show how to encode both types of domain knowledge into the widely used Gaussian scale mixture priors with Automatic Relevance Determination. Specifically, we propose a new joint prior over the local (i.e., feature-specific) scale parameters that encodes knowledge about feature sparsity, and a Stein gradient optimization to tune the hyperparameters in such a way that the distribution induced on the model’s proportion of variance explained matches the prior distribution. We show empirically that the new prior improves prediction accuracy compared to existing neural network priors on publicly available datasets and in a genetics application where signals are weak and sparse, often outperforming even computationally intensive cross-validation for hyperparameter tuning.""","""informative prior"", ""neural network"", ""proportion of variance explained"", ""Sparsity""","BA","""Informative Bayesian Neural Network Priors for Weak Signals""""Encoding domain knowledge into the prior over the high-dimensional weight space of a neural network is challenging but essential in applications with limited data and weak signals. Two types of domain knowledge are commonly available in scientific applications: 1. feature sparsity (fraction of features deemed relevant); 2. signal-to-noise ratio, quantified, for instance, as the proportion of variance explained. We show how to encode both types of domain knowledge into the widely used Gaussian scale mixture priors with Automatic Relevance Determination. Specifically, we propose a new joint prior over the local (i.e., feature-specific) scale parameters that encodes knowledge about feature sparsity, and a Stein gradient optimization to tune the hyperparameters in such a way that the distribution induced on the model’s proportion of variance explained matches the prior distribution. We show empirically that the new prior improves prediction accuracy compared to existing neural network priors on publicly available datasets and in a genetics application where signals are weak and sparse, often outperforming even computationally intensive cross-validation for hyperparameter tuning.""""informative prior"", ""neural network"", ""proportion of variance explained"", ""Sparsity"""
"""10.1214/21-BA1274""","""Bayesian Dependent Functional Mixture Estimation for Area and Time-Indexed Data: An Application for the Prediction of Monthly County Employment""","""The U.S. Bureau of Labor Statistics (BLS) publishes employment totals for all U.S. counties on a monthly basis. BLS use the Quarterly Census of Employment and Wages, where responses are received on a 6–7 month lagged basis and aggregated to county, and apply a time series forecast model to each county and project forward to the current month, which ignores the dependence among counties. Our approach treats these by-county employment time series as a collection of area indexed noisy functions that we co-model. Our model includes predictor, trend and seasonality terms indexed by county. This application is among the first in the U.S. Federal Statistical System to address the joint modeling of a collection of time series expressing heterogenous seasonality patterns between them. We demonstrate that use of a Fourier basis to model seasonality outperforms a locally-adaptive, intrinsic conditional autoregressive construction on our collection of time series where the degree of expressed seasonality varies. County-indexed parameters of the 3 terms are drawn from a dependent Dirichlet process (DDP) prior to allow the borrowing of information. We show that employment of both spatial and industry concentration predictors into the prior probabilities for co-clustering among the counties produces better prediction accuracy. Our DDP prior accounts for the possibility that nearby counties may express distinct underlying economic structures. A feature of our joint modeling framework is that it computes efficiently to support the monthly BLS production cycle. We compare the performances of alternative formulations for the dependent Dirichlet process prior on monthly, county employment data from 2002–2016.""","""Bayesian hierarchical models"", ""Dirichlet process"", ""functional data estimation"", ""Gaussian Markov random field"", ""latent models"", ""nonparametric statistics"", ""spatio-temporal modeling"", ""survey sampling""","BA","""Bayesian Dependent Functional Mixture Estimation for Area and Time-Indexed Data: An Application for the Prediction of Monthly County Employment""""The U.S. Bureau of Labor Statistics (BLS) publishes employment totals for all U.S. counties on a monthly basis. BLS use the Quarterly Census of Employment and Wages, where responses are received on a 6–7 month lagged basis and aggregated to county, and apply a time series forecast model to each county and project forward to the current month, which ignores the dependence among counties. Our approach treats these by-county employment time series as a collection of area indexed noisy functions that we co-model. Our model includes predictor, trend and seasonality terms indexed by county. This application is among the first in the U.S. Federal Statistical System to address the joint modeling of a collection of time series expressing heterogenous seasonality patterns between them. We demonstrate that use of a Fourier basis to model seasonality outperforms a locally-adaptive, intrinsic conditional autoregressive construction on our collection of time series where the degree of expressed seasonality varies. County-indexed parameters of the 3 terms are drawn from a dependent Dirichlet process (DDP) prior to allow the borrowing of information. We show that employment of both spatial and industry concentration predictors into the prior probabilities for co-clustering among the counties produces better prediction accuracy. Our DDP prior accounts for the possibility that nearby counties may express distinct underlying economic structures. A feature of our joint modeling framework is that it computes efficiently to support the monthly BLS production cycle. We compare the performances of alternative formulations for the dependent Dirichlet process prior on monthly, county employment data from 2002–2016.""""Bayesian hierarchical models"", ""Dirichlet process"", ""functional data estimation"", ""Gaussian Markov random field"", ""latent models"", ""nonparametric statistics"", ""spatio-temporal modeling"", ""survey sampling"""
"""10.1214/21-BA1276""","""Personalized Dynamic Treatment Regimes in Continuous Time: A Bayesian Approach for Optimizing Clinical Decisions with Timing""","""Accurate models of clinical actions and their impacts on disease progression are critical for estimating personalized optimal dynamic treatment regimes (DTRs) in medical/health research, especially in managing chronic conditions. Traditional statistical methods for DTRs usually focus on estimating the optimal treatment or dosage at each given medical intervention, but overlook the important question of “when this intervention should happen.” We fill this gap by developing a two-step Bayesian approach to optimize clinical decisions with timing. In the first step, we build a generative model for a sequence of medical interventions—which are discrete events in continuous time—with a marked temporal point process (MTPP) where the mark is the assigned treatment or dosage. Then this clinical action model is embedded into a Bayesian joint framework where the other components model clinical observations including longitudinal medical measurements and time-to-event data conditional on treatment histories. In the second step, we propose a policy gradient method to learn the personalized optimal clinical decision that maximizes the patient survival by interacting the MTPP with the model on clinical observations while accounting for uncertainties in clinical observations learned from the posterior inference of the Bayesian joint model in the first step. A signature application of the proposed approach is to schedule follow-up visitations and assign a dosage at each visitation for patients after kidney transplantation. We evaluate our approach with comparison to alternative methods on both simulated and real-world datasets. In our experiments, the personalized decisions made by the proposed method are clinically useful: they are interpretable and successfully help improve patient survival.""","""Bayesian joint model"", ""dynamic treatment regimes"", ""electronic health records"", ""marked temporal point process"", ""policy gradient""","BA","""Personalized Dynamic Treatment Regimes in Continuous Time: A Bayesian Approach for Optimizing Clinical Decisions with Timing""""Accurate models of clinical actions and their impacts on disease progression are critical for estimating personalized optimal dynamic treatment regimes (DTRs) in medical/health research, especially in managing chronic conditions. Traditional statistical methods for DTRs usually focus on estimating the optimal treatment or dosage at each given medical intervention, but overlook the important question of “when this intervention should happen.” We fill this gap by developing a two-step Bayesian approach to optimize clinical decisions with timing. In the first step, we build a generative model for a sequence of medical interventions—which are discrete events in continuous time—with a marked temporal point process (MTPP) where the mark is the assigned treatment or dosage. Then this clinical action model is embedded into a Bayesian joint framework where the other components model clinical observations including longitudinal medical measurements and time-to-event data conditional on treatment histories. In the second step, we propose a policy gradient method to learn the personalized optimal clinical decision that maximizes the patient survival by interacting the MTPP with the model on clinical observations while accounting for uncertainties in clinical observations learned from the posterior inference of the Bayesian joint model in the first step. A signature application of the proposed approach is to schedule follow-up visitations and assign a dosage at each visitation for patients after kidney transplantation. We evaluate our approach with comparison to alternative methods on both simulated and real-world datasets. In our experiments, the personalized decisions made by the proposed method are clinically useful: they are interpretable and successfully help improve patient survival.""""Bayesian joint model"", ""dynamic treatment regimes"", ""electronic health records"", ""marked temporal point process"", ""policy gradient"""
"""10.1214/21-BA1277""","""Bayesian Concentration Ratio and Dissonance""","""We propose two new classes of Bayesian measure to investigate conflict among data sets from multiple studies. The first (“concentration ratio”) is used to quantify the amount of information provided by a single data set through the comparison of the prior and its posterior distribution, or two data sets according to their corresponding posterior distributions. The second class (“dissonance”) quantifies the extent of contradiction between two data sets. Both classes are based on volumes of highest density regions. They are well calibrated, supported by simulation, and computational algorithms are provided for their calculation. We illustrate these two classes in three real data applications: a benchmark dose toxicology study, a missing data study related to health effects of pollution, and a pediatric cancer study leveraging adult data.""","""area under the curve"", ""data compatibility"", ""highest density regions"", ""Monte Carlo methods"", ""power prior""","BA","""Bayesian Concentration Ratio and Dissonance""""We propose two new classes of Bayesian measure to investigate conflict among data sets from multiple studies. The first (“concentration ratio”) is used to quantify the amount of information provided by a single data set through the comparison of the prior and its posterior distribution, or two data sets according to their corresponding posterior distributions. The second class (“dissonance”) quantifies the extent of contradiction between two data sets. Both classes are based on volumes of highest density regions. They are well calibrated, supported by simulation, and computational algorithms are provided for their calculation. We illustrate these two classes in three real data applications: a benchmark dose toxicology study, a missing data study related to health effects of pollution, and a pediatric cancer study leveraging adult data.""""area under the curve"", ""data compatibility"", ""highest density regions"", ""Monte Carlo methods"", ""power prior"""
"""10.1214/21-BA1281""","""On Posterior Consistency of Bayesian Factor Models in High Dimensions""","""As a principled dimension reduction technique, factor models have been widely adopted in applications. However, conducting a proper Bayesian factor analysis can be subtle in high-dimensional settings since it requires both a careful prescription of the prior distribution and a suitable computational strategy. We analyze issues of posterior inconsistency and sensitivity under different priors for high-dimensional sparse normal factor models, and show why adopting the n-orthonormal factor assumption can resolve these issues and lead to a more robust and efficient Bayesian analysis. We also provide an efficient Gibbs sampler to conduct the required computation, and show that it can be orders of magnitude more efficient than compared existing algorithms.""","""factor analysis"", ""Gibbs sampling"", ""high dimensional data"", ""orthogonality"", ""posterior consistency""","BA","""On Posterior Consistency of Bayesian Factor Models in High Dimensions""""As a principled dimension reduction technique, factor models have been widely adopted in applications. However, conducting a proper Bayesian factor analysis can be subtle in high-dimensional settings since it requires both a careful prescription of the prior distribution and a suitable computational strategy. We analyze issues of posterior inconsistency and sensitivity under different priors for high-dimensional sparse normal factor models, and show why adopting the n-orthonormal factor assumption can resolve these issues and lead to a more robust and efficient Bayesian analysis. We also provide an efficient Gibbs sampler to conduct the required computation, and show that it can be orders of magnitude more efficient than compared existing algorithms.""""factor analysis"", ""Gibbs sampling"", ""high dimensional data"", ""orthogonality"", ""posterior consistency"""
"doi.org/10.1111/biom.13892","On Interquantile Smoothness of Censored Quantile Regression with Induced Smoothing","Quantile regression has emerged as a useful and effective tool in modeling survival data, especially for cases where noises demonstrate heterogeneity. Despite recent advancements, non-smooth components involved in censored quantile regression estimators may often yield numerically unstable results, which, in turn, lead to potentially self-contradicting conclusions. We propose an estimating equation-based approach to obtain consistent estimators of the regression coefficients of interest via the induced smoothing technique to circumvent the difficulty. Our proposed estimator can be shown to be asymptotically equivalent to its original unsmoothed version, whose consistency and asymptotic normality can be readily established. Extensions to handle functional covariate data and recurrent event data are also discussed. To alleviate the heavy computational burden of bootstrap-based variance estimation, we also propose an efficient resampling procedure that reduces the computational time considerably. Our numerical studies demonstrate that our proposed estimator provides substantially smoother model parameter estimates across different quantile levels and can achieve better statistical efficiency compared to a plain estimator under various finite-sample settings. The proposed method is also illustrated via four survival datasets, including the HMO (health maintenance organizations) HIV (human immunodeficiency virus) data, the primary biliary cirrhosis (PBC) data, and so forth.","censored quantile regression, efficient resampling, induced smoothing, interquantile smoothness, MSE reduction","Biometrics","On Interquantile Smoothness of Censored Quantile Regression with Induced SmoothingQuantile regression has emerged as a useful and effective tool in modeling survival data, especially for cases where noises demonstrate heterogeneity. Despite recent advancements, non-smooth components involved in censored quantile regression estimators may often yield numerically unstable results, which, in turn, lead to potentially self-contradicting conclusions. We propose an estimating equation-based approach to obtain consistent estimators of the regression coefficients of interest via the induced smoothing technique to circumvent the difficulty. Our proposed estimator can be shown to be asymptotically equivalent to its original unsmoothed version, whose consistency and asymptotic normality can be readily established. Extensions to handle functional covariate data and recurrent event data are also discussed. To alleviate the heavy computational burden of bootstrap-based variance estimation, we also propose an efficient resampling procedure that reduces the computational time considerably. Our numerical studies demonstrate that our proposed estimator provides substantially smoother model parameter estimates across different quantile levels and can achieve better statistical efficiency compared to a plain estimator under various finite-sample settings. The proposed method is also illustrated via four survival datasets, including the HMO (health maintenance organizations) HIV (human immunodeficiency virus) data, the primary biliary cirrhosis (PBC) data, and so forth.censored quantile regression, efficient resampling, induced smoothing, interquantile smoothness, MSE reduction"
"doi.org/10.1111/biom.13889","Ensuring Valid Inference for Cox Hazard Ratios After Variable Selection","The problem of how to best select variables for confounding adjustment forms one of the key challenges in the evaluation of exposure effects in observational studies, and has been the subject of vigorous recent activity in causal inference. A major drawback of routine procedures is that there is no finite sample size at which they are guaranteed to deliver exposure effect estimators and associated confidence intervals with adequate performance. In this work, we will consider this problem when inferring conditional causal hazard ratios from observational studies under the assumption of no unmeasured confounding. The major complication that we face with survival data is that the key confounding variables may not be those that explain the censoring mechanism. In this paper, we overcome this problem using a novel and simple procedure that can be implemented using off-the-shelf software for penalized Cox regression. In particular, we will propose tests of the null hypothesis that the exposure has no effect on the considered survival endpoint, which are uniformly valid under standard sparsity conditions. Simulation results show that the proposed methods yield valid inferences even when covariates are high-dimensional.","causal inference, confounding, double selection, post-selection inference, variable selection","Biometrics","Ensuring Valid Inference for Cox Hazard Ratios After Variable SelectionThe problem of how to best select variables for confounding adjustment forms one of the key challenges in the evaluation of exposure effects in observational studies, and has been the subject of vigorous recent activity in causal inference. A major drawback of routine procedures is that there is no finite sample size at which they are guaranteed to deliver exposure effect estimators and associated confidence intervals with adequate performance. In this work, we will consider this problem when inferring conditional causal hazard ratios from observational studies under the assumption of no unmeasured confounding. The major complication that we face with survival data is that the key confounding variables may not be those that explain the censoring mechanism. In this paper, we overcome this problem using a novel and simple procedure that can be implemented using off-the-shelf software for penalized Cox regression. In particular, we will propose tests of the null hypothesis that the exposure has no effect on the considered survival endpoint, which are uniformly valid under standard sparsity conditions. Simulation results show that the proposed methods yield valid inferences even when covariates are high-dimensional.causal inference, confounding, double selection, post-selection inference, variable selection"
"doi.org/10.1111/biom.13893","Hierarchical Nuclear Norm Penalization for Multi-View Data Integration","The prevalence of data collected on the same set of samples from multiple sources (i.e., multi-view data) has prompted significant development of data integration methods based on low-rank matrix factorizations. These methods decompose signal matrices from each view into the sum of shared and individual structures, which are further used for dimension reduction, exploratory analyses, and quantifying associations across views. However, existing methods have limitations in modeling partially-shared structures due to either too restrictive models, or restrictive identifiability conditions. To address these challenges, we propose a new formulation for signal structures that include partially-shared signals based on grouping the views into so-called hierarchical levels with identifiable guarantees under suitable conditions. The proposed hierarchy leads us to introduce a new penalty, hierarchical nuclear norm (HNN), for signal estimation. In contrast to existing methods, HNN penalization avoids scores and loadings factorization of the signals and leads to a convex optimization problem, which we solve using a dual forward–backward algorithm. We propose a simple refitting procedure to adjust the penalization bias and develop an adapted version of bi-cross-validation for selecting tuning parameters. Extensive simulation studies and analysis of the genotype-tissue expression data demonstrate the advantages of our method over existing alternatives.","bi-cross-validation, data fusion, low-rank matrix, multi-source data, optimization, rank selection","Biometrics","Hierarchical Nuclear Norm Penalization for Multi-View Data IntegrationThe prevalence of data collected on the same set of samples from multiple sources (i.e., multi-view data) has prompted significant development of data integration methods based on low-rank matrix factorizations. These methods decompose signal matrices from each view into the sum of shared and individual structures, which are further used for dimension reduction, exploratory analyses, and quantifying associations across views. However, existing methods have limitations in modeling partially-shared structures due to either too restrictive models, or restrictive identifiability conditions. To address these challenges, we propose a new formulation for signal structures that include partially-shared signals based on grouping the views into so-called hierarchical levels with identifiable guarantees under suitable conditions. The proposed hierarchy leads us to introduce a new penalty, hierarchical nuclear norm (HNN), for signal estimation. In contrast to existing methods, HNN penalization avoids scores and loadings factorization of the signals and leads to a convex optimization problem, which we solve using a dual forward–backward algorithm. We propose a simple refitting procedure to adjust the penalization bias and develop an adapted version of bi-cross-validation for selecting tuning parameters. Extensive simulation studies and analysis of the genotype-tissue expression data demonstrate the advantages of our method over existing alternatives.bi-cross-validation, data fusion, low-rank matrix, multi-source data, optimization, rank selection"
"doi.org/10.1111/biom.13871","Bi-Level Structured Functional Analysis for Genome-Wide Association Studies","Genome-wide association studies (GWAS) have led to great successes in identifying genotype–phenotype associations for complex human diseases. In such studies, the high dimensionality of single nucleotide polymorphisms (SNPs) often makes analysis difficult. Functional analysis, which interprets SNPs densely distributed in a chromosomal region as a continuous process rather than discrete observations, has emerged as a promising avenue for overcoming the high dimensionality challenges. However, the majority of the existing functional studies continue to be individual SNP based and are unable to sufficiently account for the intricate underpinning structures of SNP data. SNPs are often found in groups (e.g., genes or pathways) and have a natural group structure. Additionally, these SNP groups can be highly correlated with coordinated biological functions and interact in a network. Motivated by these unique characteristics of SNP data, we develop a novel bi-level structured functional analysis method and investigate disease-associated genetic variants at the SNP level and SNP group level simultaneously. The penalization technique is adopted for bi-level selection and also to accommodate the group-level network structure. Both the estimation and selection consistency properties are rigorously established. The superiority of the proposed method over alternatives is shown through extensive simulation studies. A type 2 diabetes SNP data application yields some biologically intriguing results.","bi-level selection, functional analysis, genome-wide association study, structured analysis","Biometrics","Bi-Level Structured Functional Analysis for Genome-Wide Association StudiesGenome-wide association studies (GWAS) have led to great successes in identifying genotype–phenotype associations for complex human diseases. In such studies, the high dimensionality of single nucleotide polymorphisms (SNPs) often makes analysis difficult. Functional analysis, which interprets SNPs densely distributed in a chromosomal region as a continuous process rather than discrete observations, has emerged as a promising avenue for overcoming the high dimensionality challenges. However, the majority of the existing functional studies continue to be individual SNP based and are unable to sufficiently account for the intricate underpinning structures of SNP data. SNPs are often found in groups (e.g., genes or pathways) and have a natural group structure. Additionally, these SNP groups can be highly correlated with coordinated biological functions and interact in a network. Motivated by these unique characteristics of SNP data, we develop a novel bi-level structured functional analysis method and investigate disease-associated genetic variants at the SNP level and SNP group level simultaneously. The penalization technique is adopted for bi-level selection and also to accommodate the group-level network structure. Both the estimation and selection consistency properties are rigorously established. The superiority of the proposed method over alternatives is shown through extensive simulation studies. A type 2 diabetes SNP data application yields some biologically intriguing results.bi-level selection, functional analysis, genome-wide association study, structured analysis"
"doi.org/10.1111/biom.13864","Estimating Optimal Individualized Treatment Rules with Multistate Processes","Multistate process data are common in studies of chronic diseases such as cancer. These data are ideal for precision medicine purposes as they can be leveraged to improve more refined health outcomes, compared to standard survival outcomes, as well as incorporate patient preferences regarding quantity versus quality of life. However, there are currently no methods for the estimation of optimal individualized treatment rules with such data. In this paper, we propose a nonparametric outcome weighted learning approach for this problem in randomized clinical trial settings. The theoretical properties of the proposed methods, including Fisher consistency and asymptotic normality of the estimated expected outcome under the estimated optimal individualized treatment rule, are rigorously established. A consistent closed-form variance estimator is provided and methodology for the calculation of simultaneous confidence intervals is proposed. Simulation studies show that the proposed methodology and inference procedures work well even with small-sample sizes and high rates of right censoring. The methodology is illustrated using data from a randomized clinical trial on the treatment of metastatic squamous-cell carcinoma of the head and neck.","multistate model, outcome-weighted learning, patient preferences, precision medicine, response","Biometrics","Estimating Optimal Individualized Treatment Rules with Multistate ProcessesMultistate process data are common in studies of chronic diseases such as cancer. These data are ideal for precision medicine purposes as they can be leveraged to improve more refined health outcomes, compared to standard survival outcomes, as well as incorporate patient preferences regarding quantity versus quality of life. However, there are currently no methods for the estimation of optimal individualized treatment rules with such data. In this paper, we propose a nonparametric outcome weighted learning approach for this problem in randomized clinical trial settings. The theoretical properties of the proposed methods, including Fisher consistency and asymptotic normality of the estimated expected outcome under the estimated optimal individualized treatment rule, are rigorously established. A consistent closed-form variance estimator is provided and methodology for the calculation of simultaneous confidence intervals is proposed. Simulation studies show that the proposed methodology and inference procedures work well even with small-sample sizes and high rates of right censoring. The methodology is illustrated using data from a randomized clinical trial on the treatment of metastatic squamous-cell carcinoma of the head and neck.multistate model, outcome-weighted learning, patient preferences, precision medicine, response"
"doi.org/10.1111/biom.13866","Asynchronous and Error-Prone Longitudinal Data Analysis via Functional Calibration","In many longitudinal settings, time-varying covariates may not be measured at the same time as responses and are often prone to measurement error. Naive last-observation-carried-forward methods incur estimation biases, and existing kernel-based methods suffer from slow convergence rates and large variations. To address these challenges, we propose a new functional calibration approach to efficiently learn longitudinal covariate processes based on sparse functional data with measurement error. Our approach, stemming from functional principal component analysis, calibrates the unobserved synchronized covariate values from the observed asynchronous and error-prone covariate values, and is broadly applicable to asynchronous longitudinal regression with time-invariant or time-varying coefficients. For regression with time-invariant coefficients, our estimator is asymptotically unbiased, root-n consistent, and asymptotically normal; for time-varying coefficient models, our estimator has the optimal varying coefficient model convergence rate with inflated asymptotic variance from the calibration. In both cases, our estimators present asymptotic properties superior to the existing methods. The feasibility and usability of the proposed methods are verified by simulations and an application to the Study of Women's Health Across the Nation, a large-scale multisite longitudinal study on women's health during midlife.","functional principal component analysis, kernel smoothing, measurement error, regression calibration, sparse functional data, varying coefficient model","Biometrics","Asynchronous and Error-Prone Longitudinal Data Analysis via Functional CalibrationIn many longitudinal settings, time-varying covariates may not be measured at the same time as responses and are often prone to measurement error. Naive last-observation-carried-forward methods incur estimation biases, and existing kernel-based methods suffer from slow convergence rates and large variations. To address these challenges, we propose a new functional calibration approach to efficiently learn longitudinal covariate processes based on sparse functional data with measurement error. Our approach, stemming from functional principal component analysis, calibrates the unobserved synchronized covariate values from the observed asynchronous and error-prone covariate values, and is broadly applicable to asynchronous longitudinal regression with time-invariant or time-varying coefficients. For regression with time-invariant coefficients, our estimator is asymptotically unbiased, root-n consistent, and asymptotically normal; for time-varying coefficient models, our estimator has the optimal varying coefficient model convergence rate with inflated asymptotic variance from the calibration. In both cases, our estimators present asymptotic properties superior to the existing methods. The feasibility and usability of the proposed methods are verified by simulations and an application to the Study of Women's Health Across the Nation, a large-scale multisite longitudinal study on women's health during midlife.functional principal component analysis, kernel smoothing, measurement error, regression calibration, sparse functional data, varying coefficient model"
"doi.org/10.1111/biom.13862","Identifying and Estimating Effects of Sustained Interventions under Parallel Trends Assumptions","Many research questions in public health and medicine concern sustained interventions in populations defined by substantive priorities. Existing methods to answer such questions typically require a measured covariate set sufficient to control confounding, which can be questionable in observational studies. Differences-in-differences rely instead on the parallel trends assumption, allowing for some types of time-invariant unmeasured confounding. However, most existing difference-in-differences implementations are limited to point treatments in restricted subpopulations. We derive identification results for population effects of sustained treatments under parallel trends assumptions. In particular, in settings where all individuals begin follow-up with exposure status consistent with the treatment plan of interest but may deviate at later times, a version of Robins' g-formula identifies the intervention-specific mean under stable unit treatment value assumption, positivity, and parallel trends. We develop consistent asymptotically normal estimators based on inverse-probability weighting, outcome regression, and a double robust estimator based on targeted maximum likelihood. Simulation studies confirm theoretical results and support the use of the proposed estimators at realistic sample sizes. As an example, the methods are used to estimate the effect of a hypothetical federal stay-at-home order on all-cause mortality during the COVID-19 pandemic in spring 2020 in the United States.","causal inference, difference-in-differences, g-formula, observational study, unmeasured confounding","Biometrics","Identifying and Estimating Effects of Sustained Interventions under Parallel Trends AssumptionsMany research questions in public health and medicine concern sustained interventions in populations defined by substantive priorities. Existing methods to answer such questions typically require a measured covariate set sufficient to control confounding, which can be questionable in observational studies. Differences-in-differences rely instead on the parallel trends assumption, allowing for some types of time-invariant unmeasured confounding. However, most existing difference-in-differences implementations are limited to point treatments in restricted subpopulations. We derive identification results for population effects of sustained treatments under parallel trends assumptions. In particular, in settings where all individuals begin follow-up with exposure status consistent with the treatment plan of interest but may deviate at later times, a version of Robins' g-formula identifies the intervention-specific mean under stable unit treatment value assumption, positivity, and parallel trends. We develop consistent asymptotically normal estimators based on inverse-probability weighting, outcome regression, and a double robust estimator based on targeted maximum likelihood. Simulation studies confirm theoretical results and support the use of the proposed estimators at realistic sample sizes. As an example, the methods are used to estimate the effect of a hypothetical federal stay-at-home order on all-cause mortality during the COVID-19 pandemic in spring 2020 in the United States.causal inference, difference-in-differences, g-formula, observational study, unmeasured confounding"
"doi.org/10.1111/biom.13859","Longitudinal Incremental Propensity Score Interventions for Limited Resource Settings ","Many real-life treatments are of limited supply and cannot be provided to all individuals in the population. For example, patients on the liver transplant waiting list usually cannot be assigned a liver transplant immediately at the time they reach highest priority because a suitable organ is not immediately available. In settings with limited supply, investigators are often interested in the effects of treatment strategies in which a limited proportion of patients receive an organ at a given time, that is, treatment regimes satisfying resource constraints. Here, we describe an estimand that allows us to define causal effects of treatment strategies that satisfy resource constraints: incremental propensity score interventions (IPSIs) for limited resources. IPSIs flexibly constrain time-varying resource utilization through proportional scaling of patients' natural propensities for treatment, thereby preserving existing propensity rank ordering compared to the status quo. We derive a simple class of inverse-probability-weighted estimators, and we apply one such estimator to evaluate the effect of restricting or expanding utilization of “increased risk” liver organs to treat patients with end-stage liver disease.","causal inference, lifetime and survival analysis, nonparametric methods","Biometrics","Longitudinal Incremental Propensity Score Interventions for Limited Resource Settings Many real-life treatments are of limited supply and cannot be provided to all individuals in the population. For example, patients on the liver transplant waiting list usually cannot be assigned a liver transplant immediately at the time they reach highest priority because a suitable organ is not immediately available. In settings with limited supply, investigators are often interested in the effects of treatment strategies in which a limited proportion of patients receive an organ at a given time, that is, treatment regimes satisfying resource constraints. Here, we describe an estimand that allows us to define causal effects of treatment strategies that satisfy resource constraints: incremental propensity score interventions (IPSIs) for limited resources. IPSIs flexibly constrain time-varying resource utilization through proportional scaling of patients' natural propensities for treatment, thereby preserving existing propensity rank ordering compared to the status quo. We derive a simple class of inverse-probability-weighted estimators, and we apply one such estimator to evaluate the effect of restricting or expanding utilization of “increased risk” liver organs to treat patients with end-stage liver disease.causal inference, lifetime and survival analysis, nonparametric methods"
"doi.org/10.1111/biom.13855","Information Criteria for Detecting Change-Points in the Cox Proportional Hazards Model ","The Cox proportional hazards model, commonly used in clinical trials, assumes proportional hazards. However, it does not hold when, for example, there is a delayed onset of the treatment effect. In such a situation, an acute change in the hazard ratio function is expected to exist. This paper considers the Cox model with change-points and derives Akaike information criterion (AIC)-type information criteria for detecting those change-points. The change-point model does not allow for conventional statistical asymptotics due to its irregularity, thus a formal AIC that penalizes twice the number of parameters would not be analytically derived, and using it would clearly give overfitting analysis results. Therefore, we will construct specific asymptotics using the partial likelihood estimation method in the Cox model with change-points, and propose information criteria based on the original derivation method for AIC. If the partial likelihood is used in the estimation, information criteria with penalties much larger than twice the number of parameters could be obtained in an explicit form. Numerical experiments confirm that the proposed criteria are clearly superior in terms of the original purpose of AIC, which are to provide an estimate that is close to the true structure. We also apply the proposed criterion to actual clinical trial data to indicate that it will easily lead to different results from the formal AIC.","Brownian motion, model misspecification, model selection, statistical asymptotic theory, structural change, survival time analysis","Biometrics","Information Criteria for Detecting Change-Points in the Cox Proportional Hazards Model The Cox proportional hazards model, commonly used in clinical trials, assumes proportional hazards. However, it does not hold when, for example, there is a delayed onset of the treatment effect. In such a situation, an acute change in the hazard ratio function is expected to exist. This paper considers the Cox model with change-points and derives Akaike information criterion (AIC)-type information criteria for detecting those change-points. The change-point model does not allow for conventional statistical asymptotics due to its irregularity, thus a formal AIC that penalizes twice the number of parameters would not be analytically derived, and using it would clearly give overfitting analysis results. Therefore, we will construct specific asymptotics using the partial likelihood estimation method in the Cox model with change-points, and propose information criteria based on the original derivation method for AIC. If the partial likelihood is used in the estimation, information criteria with penalties much larger than twice the number of parameters could be obtained in an explicit form. Numerical experiments confirm that the proposed criteria are clearly superior in terms of the original purpose of AIC, which are to provide an estimate that is close to the true structure. We also apply the proposed criterion to actual clinical trial data to indicate that it will easily lead to different results from the formal AIC.Brownian motion, model misspecification, model selection, statistical asymptotic theory, structural change, survival time analysis"
"doi.org/10.1111/biom.13854","Interim Monitoring of Sequential Multiple Assignment Randomized Trials Using Partial Information","The sequential multiple assignment randomized trial (SMART) is the gold standard trial design to generate data for the evaluation of multistage treatment regimes. As with conventional (single-stage) randomized clinical trials, interim monitoring allows early stopping; however, there are few methods for principled interim analysis in SMARTs. Because SMARTs involve multiple stages of treatment, a key challenge is that not all enrolled participants will have progressed through all treatment stages at the time of an interim analysis. Wu et al. (2021) propose basing interim analyses on an estimator for the mean outcome under a given regime that uses data only from participants who have completed all treatment stages. We propose an estimator for the mean outcome under a given regime that gains efficiency by using partial information from enrolled participants regardless of their progression through treatment stages. Using the asymptotic distribution of this estimator, we derive associated Pocock and O'Brien-Fleming testing procedures for early stopping. In simulation experiments, the estimator controls type I error and achieves nominal power while reducing expected sample size relative to the method of Wu et al. (2021). We present an illustrative application of the proposed estimator based on a recent SMART evaluating behavioral pain interventions for breast cancer patients.","augmented inverse probability weighting, clinical trials, double robustness, dynamic treatment regimes, early stopping, group sequential analysis","Biometrics","Interim Monitoring of Sequential Multiple Assignment Randomized Trials Using Partial InformationThe sequential multiple assignment randomized trial (SMART) is the gold standard trial design to generate data for the evaluation of multistage treatment regimes. As with conventional (single-stage) randomized clinical trials, interim monitoring allows early stopping; however, there are few methods for principled interim analysis in SMARTs. Because SMARTs involve multiple stages of treatment, a key challenge is that not all enrolled participants will have progressed through all treatment stages at the time of an interim analysis. Wu et al. (2021) propose basing interim analyses on an estimator for the mean outcome under a given regime that uses data only from participants who have completed all treatment stages. We propose an estimator for the mean outcome under a given regime that gains efficiency by using partial information from enrolled participants regardless of their progression through treatment stages. Using the asymptotic distribution of this estimator, we derive associated Pocock and O'Brien-Fleming testing procedures for early stopping. In simulation experiments, the estimator controls type I error and achieves nominal power while reducing expected sample size relative to the method of Wu et al. (2021). We present an illustrative application of the proposed estimator based on a recent SMART evaluating behavioral pain interventions for breast cancer patients.augmented inverse probability weighting, clinical trials, double robustness, dynamic treatment regimes, early stopping, group sequential analysis"
"doi.org/10.1111/biom.13851","Latent Deformation Models for Multivariate Functional Data and Time-Warping Separability","Multivariate functional data present theoretical and practical complications that are not found in univariate functional data. One of these is a situation where the component functions of multivariate functional data are positive and are subject to mutual time warping. That is, the component processes exhibit a common shape but are subject to systematic phase variation across their domains in addition to subject-specific time warping, where each subject has its own internal clock. This motivates a novel model for multivariate functional data that connect such mutual time warping to a latent-deformation-based framework by exploiting a novel time-warping separability assumption. This separability assumption allows for meaningful interpretation and dimension reduction. The resulting latent deformation model is shown to be well suited to represent commonly encountered functional vector data. The proposed approach combines a random amplitude factor for each component with population-based registration across the components of a multivariate functional data vector and includes a latent population function, which corresponds to a common underlying trajectory. We propose estimators for all components of the model, enabling implementation of the proposed data-based representation for multivariate functional data and downstream analyses such as Fréchet regression. Rates of convergence are established when curves are fully observed or observed with measurement error. The usefulness of the model, interpretations, and practical aspects are illustrated in simulations and with application to multivariate human growth curves and multivariate environmental pollution data.","component processes, cross-component registration, functional data analysis, longitudinal studies, multivariate functional data, time warping","Biometrics","Latent Deformation Models for Multivariate Functional Data and Time-Warping SeparabilityMultivariate functional data present theoretical and practical complications that are not found in univariate functional data. One of these is a situation where the component functions of multivariate functional data are positive and are subject to mutual time warping. That is, the component processes exhibit a common shape but are subject to systematic phase variation across their domains in addition to subject-specific time warping, where each subject has its own internal clock. This motivates a novel model for multivariate functional data that connect such mutual time warping to a latent-deformation-based framework by exploiting a novel time-warping separability assumption. This separability assumption allows for meaningful interpretation and dimension reduction. The resulting latent deformation model is shown to be well suited to represent commonly encountered functional vector data. The proposed approach combines a random amplitude factor for each component with population-based registration across the components of a multivariate functional data vector and includes a latent population function, which corresponds to a common underlying trajectory. We propose estimators for all components of the model, enabling implementation of the proposed data-based representation for multivariate functional data and downstream analyses such as Fréchet regression. Rates of convergence are established when curves are fully observed or observed with measurement error. The usefulness of the model, interpretations, and practical aspects are illustrated in simulations and with application to multivariate human growth curves and multivariate environmental pollution data.component processes, cross-component registration, functional data analysis, longitudinal studies, multivariate functional data, time warping"
"doi.org/10.1111/biom.13840","DROID: Dose-Ranging Approach to Optimizing Dose in Oncology Drug Development","In the era of targeted therapy, there has been increasing concern about the development of oncology drugs based on the “more is better” paradigm, developed decades ago for chemotherapy. Recently, the US Food and Drug Administration (FDA) initiated Project Optimus to reform the dose optimization and dose selection paradigm in oncology drug development. To accommodate this paradigm shifting, we propose a dose-ranging approach to optimizing dose (DROID) for oncology trials with targeted drugs. DROID leverages the well-established dose-ranging study framework, which has been routinely used to develop non-oncology drugs for decades, and bridges it with established oncology dose-finding designs to optimize the dose of oncology drugs. DROID consists of two seamlessly connected stages. In the first stage, patients are sequentially enrolled and adaptively assigned to investigational doses to establish the therapeutic dose range (TDR), defined as the range of doses with acceptable toxicity and efficacy profiles, and the recommended phase 2 dose set (RP2S). In the second stage, patients are randomized to the doses in RP2S to assess the dose–response relationship and identify the optimal dose. The simulation study shows that DROID substantially outperforms the conventional approach, providing a new paradigm to efficiently optimize the dose of targeted oncology drugs. DROID aligns with the approach of a randomized, parallel dose-response trial design recommended by the FDA in the Guidance on Optimizing the Dosage of Human Prescription Drugs and Biological Products for the Treatment of Oncologic Diseases.","dose–response relationship, maximum tolerated dose, optimal dose, risk-benefit assessment, targeted drugs","Biometrics","DROID: Dose-Ranging Approach to Optimizing Dose in Oncology Drug DevelopmentIn the era of targeted therapy, there has been increasing concern about the development of oncology drugs based on the “more is better” paradigm, developed decades ago for chemotherapy. Recently, the US Food and Drug Administration (FDA) initiated Project Optimus to reform the dose optimization and dose selection paradigm in oncology drug development. To accommodate this paradigm shifting, we propose a dose-ranging approach to optimizing dose (DROID) for oncology trials with targeted drugs. DROID leverages the well-established dose-ranging study framework, which has been routinely used to develop non-oncology drugs for decades, and bridges it with established oncology dose-finding designs to optimize the dose of oncology drugs. DROID consists of two seamlessly connected stages. In the first stage, patients are sequentially enrolled and adaptively assigned to investigational doses to establish the therapeutic dose range (TDR), defined as the range of doses with acceptable toxicity and efficacy profiles, and the recommended phase 2 dose set (RP2S). In the second stage, patients are randomized to the doses in RP2S to assess the dose–response relationship and identify the optimal dose. The simulation study shows that DROID substantially outperforms the conventional approach, providing a new paradigm to efficiently optimize the dose of targeted oncology drugs. DROID aligns with the approach of a randomized, parallel dose-response trial design recommended by the FDA in the Guidance on Optimizing the Dosage of Human Prescription Drugs and Biological Products for the Treatment of Oncologic Diseases.dose–response relationship, maximum tolerated dose, optimal dose, risk-benefit assessment, targeted drugs"
"doi.org/10.1111/biom.13837","Improved Inference for Doubly Robust Estimators of Heterogeneous Treatment Effects","We propose a doubly robust approach to characterizing treatment effect heterogeneity in observational studies. We develop a frequentist inferential procedure that utilizes posterior distributions for both the propensity score and outcome regression models to provide valid inference on the conditional average treatment effect even when high-dimensional or nonparametric models are used. We show that our approach leads to conservative inference in finite samples or under model misspecification and provides a consistent variance estimator when both models are correctly specified. In simulations, we illustrate the utility of these results in difficult settings such as high-dimensional covariate spaces or highly flexible models for the propensity score and outcome regression. Lastly, we analyze environmental exposure data from NHANES to identify how the effects of these exposures vary by subject-level characteristics.","Bayesian nonparametrics, causal inference, doubly robust estimation, high-dimensional statistics, treatment effect heterogeneity","Biometrics","Improved Inference for Doubly Robust Estimators of Heterogeneous Treatment EffectsWe propose a doubly robust approach to characterizing treatment effect heterogeneity in observational studies. We develop a frequentist inferential procedure that utilizes posterior distributions for both the propensity score and outcome regression models to provide valid inference on the conditional average treatment effect even when high-dimensional or nonparametric models are used. We show that our approach leads to conservative inference in finite samples or under model misspecification and provides a consistent variance estimator when both models are correctly specified. In simulations, we illustrate the utility of these results in difficult settings such as high-dimensional covariate spaces or highly flexible models for the propensity score and outcome regression. Lastly, we analyze environmental exposure data from NHANES to identify how the effects of these exposures vary by subject-level characteristics.Bayesian nonparametrics, causal inference, doubly robust estimation, high-dimensional statistics, treatment effect heterogeneity"
"doi.org/10.1111/biom.13830","Competition-Based Control of the False Discovery Proportion","Recently, Barber and Candès laid the theoretical foundation for a general framework for false discovery rate (FDR) control based on the notion of “knockoffs.” A closely related FDR control methodology has long been employed in the analysis of mass spectrometry data, referred to there as “target–decoy competition” (TDC). However, any approach that aims to control the FDR, which is defined as the expected value of the false discovery proportion (FDP), suffers from a problem. Specifically, even when successfully controlling the FDR at level α, the FDP in the list of discoveries can significantly exceed α. We offer FDP-SD, a new procedure that rigorously controls the FDP in the knockoff/TDC competition setup by guaranteeing that the FDP is bounded by α at a desired confidence level. Compared with the recently published framework of Katsevich and Ramdas, FDP-SD generally delivers more power and often substantially so in simulated and real data.","FDP control, knockoffs, peptide detection, target–decoy competition","Biometrics","Competition-Based Control of the False Discovery ProportionRecently, Barber and Candès laid the theoretical foundation for a general framework for false discovery rate (FDR) control based on the notion of “knockoffs.” A closely related FDR control methodology has long been employed in the analysis of mass spectrometry data, referred to there as “target–decoy competition” (TDC). However, any approach that aims to control the FDR, which is defined as the expected value of the false discovery proportion (FDP), suffers from a problem. Specifically, even when successfully controlling the FDR at level α, the FDP in the list of discoveries can significantly exceed α. We offer FDP-SD, a new procedure that rigorously controls the FDP in the knockoff/TDC competition setup by guaranteeing that the FDP is bounded by α at a desired confidence level. Compared with the recently published framework of Katsevich and Ramdas, FDP-SD generally delivers more power and often substantially so in simulated and real data.FDP control, knockoffs, peptide detection, target–decoy competition"
"doi.org/10.1111/biom.13826","Relative Contrast Estimation and Inference for Treatment Recommendation","When there are resource constraints, it may be necessary to rank individualized treatment benefits to facilitate the prioritization of assigning different treatments. Most existing literature on individualized treatment rules targets absolute conditional treatment effect differences as a metric for the benefit. However, there can be settings where relative differences may better represent such benefit. In this paper, we consider modeling such relative differences formed as scale-invariant contrasts between the conditional treatment effects. By showing that all scale-invariant contrasts are monotonic transformations of each other, we posit a single index model for a particular relative contrast. We then characterize semiparametric estimating equations, including the efficient score, to estimate index parameters. To achieve semiparametric efficiency, we propose a two-step approach that minimizes a doubly robust loss function for initial estimation and then performs a one-step efficiency augmentation procedure. Careful theoretical and numerical studies are provided to show the superiority of our proposed approach.","individualized treatment rule, observational study, precision medicine, semiparametric efficiency, single index model","Biometrics","Relative Contrast Estimation and Inference for Treatment RecommendationWhen there are resource constraints, it may be necessary to rank individualized treatment benefits to facilitate the prioritization of assigning different treatments. Most existing literature on individualized treatment rules targets absolute conditional treatment effect differences as a metric for the benefit. However, there can be settings where relative differences may better represent such benefit. In this paper, we consider modeling such relative differences formed as scale-invariant contrasts between the conditional treatment effects. By showing that all scale-invariant contrasts are monotonic transformations of each other, we posit a single index model for a particular relative contrast. We then characterize semiparametric estimating equations, including the efficient score, to estimate index parameters. To achieve semiparametric efficiency, we propose a two-step approach that minimizes a doubly robust loss function for initial estimation and then performs a one-step efficiency augmentation procedure. Careful theoretical and numerical studies are provided to show the superiority of our proposed approach.individualized treatment rule, observational study, precision medicine, semiparametric efficiency, single index model"
"doi.org/10.1111/biom.13825","Entropy Balancing for Causal Generalization with Target Sample Summary Information","In this paper, we focus on estimating the average treatment effect (ATE) of a target population when individual-level data from a source population and summary-level data (e.g., first or second moments of certain covariates) from the target population are available. In the presence of the heterogeneous treatment effect, the ATE of the target population can be different from that of the source population when distributions of treatment effect modifiers are dissimilar in these two populations, a phenomenon also known as covariate shift. Many methods have been developed to adjust for covariate shift, but most require individual covariates from a representative target sample. We develop a weighting approach based on the summary-level information from the target sample to adjust for possible covariate shift in effect modifiers. In particular, weights of the treated and control groups within a source sample are calibrated by the summary-level information of the target sample. Our approach also seeks additional covariate balance between the treated and control groups in the source sample. We study the asymptotic behavior of the corresponding weighted estimator for the target population ATE under a wide range of conditions. The theoretical implications are confirmed in simulation studies and a real-data application.","average treatment effect, causal generalization, entropy balancing weights, summary-level data","Biometrics","Entropy Balancing for Causal Generalization with Target Sample Summary InformationIn this paper, we focus on estimating the average treatment effect (ATE) of a target population when individual-level data from a source population and summary-level data (e.g., first or second moments of certain covariates) from the target population are available. In the presence of the heterogeneous treatment effect, the ATE of the target population can be different from that of the source population when distributions of treatment effect modifiers are dissimilar in these two populations, a phenomenon also known as covariate shift. Many methods have been developed to adjust for covariate shift, but most require individual covariates from a representative target sample. We develop a weighting approach based on the summary-level information from the target sample to adjust for possible covariate shift in effect modifiers. In particular, weights of the treated and control groups within a source sample are calibrated by the summary-level information of the target sample. Our approach also seeks additional covariate balance between the treated and control groups in the source sample. We study the asymptotic behavior of the corresponding weighted estimator for the target population ATE under a wide range of conditions. The theoretical implications are confirmed in simulation studies and a real-data application.average treatment effect, causal generalization, entropy balancing weights, summary-level data"
"doi.org/10.1111/biom.13824","Optimal Sampling for Positive Only Electronic Health Record Data","Identifying a patient's disease/health status from electronic medical records is a frequently encountered task in electronic health records (EHR) related research, and estimation of a classification model often requires a benchmark training data with patients' known phenotype statuses. However, assessing a patient's phenotype is costly and labor intensive, hence a proper selection of EHR records as a training set is desired. We propose a procedure to tailor the best training subsample with limited sample size for a classification model, minimizing its mean-squared phenotyping/classification error (MSE). Our approach incorporates “positive only” information, an approximation of the true disease status without false alarm, when it is available. In addition, our sampling procedure is applicable for training a chosen classification model which can be misspecified. We provide theoretical justification on its optimality in terms of MSE. The performance gain from our method is illustrated through simulation and a real-data example, and is found often satisfactory under criteria beyond MSE.","electronic health records, mean-squared error, optimal sampling, positive only","Biometrics","Optimal Sampling for Positive Only Electronic Health Record DataIdentifying a patient's disease/health status from electronic medical records is a frequently encountered task in electronic health records (EHR) related research, and estimation of a classification model often requires a benchmark training data with patients' known phenotype statuses. However, assessing a patient's phenotype is costly and labor intensive, hence a proper selection of EHR records as a training set is desired. We propose a procedure to tailor the best training subsample with limited sample size for a classification model, minimizing its mean-squared phenotyping/classification error (MSE). Our approach incorporates “positive only” information, an approximation of the true disease status without false alarm, when it is available. In addition, our sampling procedure is applicable for training a chosen classification model which can be misspecified. We provide theoretical justification on its optimality in terms of MSE. The performance gain from our method is illustrated through simulation and a real-data example, and is found often satisfactory under criteria beyond MSE.electronic health records, mean-squared error, optimal sampling, positive only"
"doi.org/10.1111/biom.13823","Model Uncertainty Quantification in Cox Regression","We consider covariate selection and the ensuing model uncertainty aspects in the context of Cox regression. The perspective we take is probabilistic, and we handle it within a Bayesian framework. One of the critical elements in variable/model selection is choosing a suitable prior for model parameters. Here, we derive the so-called conventional prior approach and propose a comprehensive implementation that results in an automatic procedure. Our simulation studies and real applications show improvements over existing literature. For the sake of reproducibility but also for its intrinsic interest for practitioners, a web application requiring minimum statistical knowledge implements the proposed approach.","Bayesian variable selection, conventional prior, Fisher information, median model, survival analysis","Biometrics","Model Uncertainty Quantification in Cox RegressionWe consider covariate selection and the ensuing model uncertainty aspects in the context of Cox regression. The perspective we take is probabilistic, and we handle it within a Bayesian framework. One of the critical elements in variable/model selection is choosing a suitable prior for model parameters. Here, we derive the so-called conventional prior approach and propose a comprehensive implementation that results in an automatic procedure. Our simulation studies and real applications show improvements over existing literature. For the sake of reproducibility but also for its intrinsic interest for practitioners, a web application requiring minimum statistical knowledge implements the proposed approach.Bayesian variable selection, conventional prior, Fisher information, median model, survival analysis"
"doi.org/10.1111/biom.13821","Additive Subdistribution Hazards Regression for Competing Risks Data in Case-Cohort Studies","In survival data analysis, a competing risk is an event whose occurrence precludes or alters the chance of the occurrence of the primary event of interest. In large cohort studies with long-term follow-up, there are often competing risks. Further, if the event of interest is rare in such large studies, the case-cohort study design is widely used to reduce the cost and achieve the same efficiency as a cohort study. The conventional additive hazards modeling for competing risks data in case-cohort studies involves the cause-specific hazard function, under which direct assessment of covariate effects on the cumulative incidence function, or the subdistribution, is not possible. In this paper, we consider an additive hazard model for the subdistribution of a competing risk in case-cohort studies. We propose estimating equations based on inverse probability weighting methods for the estimation of the model parameters. Consistency and asymptotic normality of the proposed estimators are established. The performance of the proposed methods in finite samples is examined through simulation studies and the proposed approach is applied to a case-cohort dataset from the Sister Study.","additive hazards model, case-cohort study, competing risks, hazard of subdistribution, inverse probability weighting, partial pseudolikelihood","Biometrics","Additive Subdistribution Hazards Regression for Competing Risks Data in Case-Cohort StudiesIn survival data analysis, a competing risk is an event whose occurrence precludes or alters the chance of the occurrence of the primary event of interest. In large cohort studies with long-term follow-up, there are often competing risks. Further, if the event of interest is rare in such large studies, the case-cohort study design is widely used to reduce the cost and achieve the same efficiency as a cohort study. The conventional additive hazards modeling for competing risks data in case-cohort studies involves the cause-specific hazard function, under which direct assessment of covariate effects on the cumulative incidence function, or the subdistribution, is not possible. In this paper, we consider an additive hazard model for the subdistribution of a competing risk in case-cohort studies. We propose estimating equations based on inverse probability weighting methods for the estimation of the model parameters. Consistency and asymptotic normality of the proposed estimators are established. The performance of the proposed methods in finite samples is examined through simulation studies and the proposed approach is applied to a case-cohort dataset from the Sister Study.additive hazards model, case-cohort study, competing risks, hazard of subdistribution, inverse probability weighting, partial pseudolikelihood"
"doi.org/10.1111/biom.13822","Adjusting for Publication Bias in Meta-Analysis via Inverse Probability Weighting Using Clinical Trial Registries","Publication bias is a major concern in conducting systematic reviews and meta-analyses. Various sensitivity analysis or bias-correction methods have been developed based on selection models, and they have some advantages over the widely used trim-and-fill bias-correction method. However, likelihood methods based on selection models may have difficulty in obtaining precise estimates and reasonable confidence intervals, or require a rather complicated sensitivity analysis process. Herein, we develop a simple publication bias adjustment method by utilizing the information on conducted but still unpublished trials from clinical trial registries. We introduce an estimating equation for parameter estimation in the selection function by regarding the publication bias issue as a missing data problem under the missing not at random assumption. With the estimated selection function, we introduce the inverse probability weighting (IPW) method to estimate the overall mean across studies. Furthermore, the IPW versions of heterogeneity measures such as the between-study variance and the I2 measure are proposed. We propose methods to construct confidence intervals based on asymptotic normal approximation as well as on parametric bootstrap. Through numerical experiments, we observed that the estimators successfully eliminated bias, and the confidence intervals had empirical coverage probabilities close to the nominal level. On the other hand, the confidence interval based on asymptotic normal approximation is much wider in some scenarios than the bootstrap confidence interval. Therefore, the latter is recommended for practical use.","clinical trial registry, missing not at random, propensity score, sensitivity analysis, systematic review","Biometrics","Adjusting for Publication Bias in Meta-Analysis via Inverse Probability Weighting Using Clinical Trial RegistriesPublication bias is a major concern in conducting systematic reviews and meta-analyses. Various sensitivity analysis or bias-correction methods have been developed based on selection models, and they have some advantages over the widely used trim-and-fill bias-correction method. However, likelihood methods based on selection models may have difficulty in obtaining precise estimates and reasonable confidence intervals, or require a rather complicated sensitivity analysis process. Herein, we develop a simple publication bias adjustment method by utilizing the information on conducted but still unpublished trials from clinical trial registries. We introduce an estimating equation for parameter estimation in the selection function by regarding the publication bias issue as a missing data problem under the missing not at random assumption. With the estimated selection function, we introduce the inverse probability weighting (IPW) method to estimate the overall mean across studies. Furthermore, the IPW versions of heterogeneity measures such as the between-study variance and the I2 measure are proposed. We propose methods to construct confidence intervals based on asymptotic normal approximation as well as on parametric bootstrap. Through numerical experiments, we observed that the estimators successfully eliminated bias, and the confidence intervals had empirical coverage probabilities close to the nominal level. On the other hand, the confidence interval based on asymptotic normal approximation is much wider in some scenarios than the bootstrap confidence interval. Therefore, the latter is recommended for practical use.clinical trial registry, missing not at random, propensity score, sensitivity analysis, systematic review"
"doi.org/10.1111/biom.13818","Stabilized Direct Learning for Efficient Estimation of Individualized Treatment Rules","In recent years, the field of precision medicine has seen many advancements. Significant focus has been placed on creating algorithms to estimate individualized treatment rules (ITRs), which map from patient covariates to the space of available treatments with the goal of maximizing patient outcome. Direct learning (D-Learning) is a recent one-step method which estimates the ITR by directly modeling the treatment–covariate interaction. However, when the variance of the outcome is heterogeneous with respect to treatment and covariates, D-Learning does not leverage this structure. Stabilized direct learning (SD-Learning), proposed in this paper, utilizes potential heteroscedasticity in the error term through a residual reweighting which models the residual variance via flexible machine learning algorithms such as XGBoost and random forests. We also develop an internal cross-validation scheme which determines the best residual model among competing models. SD-Learning improves the efficiency of D-Learning estimates in binary and multi-arm treatment scenarios. The method is simple to implement and an easy way to improve existing algorithms within the D-Learning family, including original D-Learning, Angle-based D-Learning (AD-Learning), and Robust D-learning (RD-Learning). We provide theoretical properties and justification of the optimality of SD-Learning. Head-to-head performance comparisons with D-Learning methods are provided through simulations, which demonstrate improvement in terms of average prediction error (APE), misclassification rate, and empirical value, along with a data analysis of an acquired immunodeficiency syndrome (AIDS) randomized clinical trial.","D-Learning, heteroscedasticity, individualized treatment rule, multi-arm treatments, precision medicine, statistical machine learning","Biometrics","Stabilized Direct Learning for Efficient Estimation of Individualized Treatment RulesIn recent years, the field of precision medicine has seen many advancements. Significant focus has been placed on creating algorithms to estimate individualized treatment rules (ITRs), which map from patient covariates to the space of available treatments with the goal of maximizing patient outcome. Direct learning (D-Learning) is a recent one-step method which estimates the ITR by directly modeling the treatment–covariate interaction. However, when the variance of the outcome is heterogeneous with respect to treatment and covariates, D-Learning does not leverage this structure. Stabilized direct learning (SD-Learning), proposed in this paper, utilizes potential heteroscedasticity in the error term through a residual reweighting which models the residual variance via flexible machine learning algorithms such as XGBoost and random forests. We also develop an internal cross-validation scheme which determines the best residual model among competing models. SD-Learning improves the efficiency of D-Learning estimates in binary and multi-arm treatment scenarios. The method is simple to implement and an easy way to improve existing algorithms within the D-Learning family, including original D-Learning, Angle-based D-Learning (AD-Learning), and Robust D-learning (RD-Learning). We provide theoretical properties and justification of the optimality of SD-Learning. Head-to-head performance comparisons with D-Learning methods are provided through simulations, which demonstrate improvement in terms of average prediction error (APE), misclassification rate, and empirical value, along with a data analysis of an acquired immunodeficiency syndrome (AIDS) randomized clinical trial.D-Learning, heteroscedasticity, individualized treatment rule, multi-arm treatments, precision medicine, statistical machine learning"
"doi.org/10.1111/biom.13811","Two-Level Bayesian Interaction Analysis for Survival Data Incorporating Pathway Information ","Genetic interactions play an important role in the progression of complex diseases, providing explanation of variations in disease phenotype missed by main genetic effects. Comparatively, there are fewer studies on survival time, given its challenging characteristics such as censoring. In recent biomedical research, two-level analysis of both genes and their involved pathways has received much attention and been demonstrated as more effective than single-level analysis. However, such analysis is usually limited to main effects. Pathways are not isolated, and their interactions have also been suggested to have important contributions to the prognosis of complex diseases. In this paper, we develop a novel two-level Bayesian interaction analysis approach for survival data. This approach is the first to conduct the analysis of lower-level gene–gene interactions and higher-level pathway–pathway interactions simultaneously. Significantly advancing from the existing Bayesian studies based on the Markov Chain Monte Carlo (MCMC) technique, we propose a variational inference framework based on the accelerated failure time model with effective priors to accommodate two-level selection as well as censoring. Its computational efficiency is much desirable for high-dimensional interaction analysis. We examine performance of the proposed approach using extensive simulation. The application to TCGA melanoma and lung adenocarcinoma data leads to biologically sensible findings with satisfactory prediction accuracy and selection stability.","Bayesian analysis, interaction analysis, survival data, two-level selection","Biometrics","Two-Level Bayesian Interaction Analysis for Survival Data Incorporating Pathway Information Genetic interactions play an important role in the progression of complex diseases, providing explanation of variations in disease phenotype missed by main genetic effects. Comparatively, there are fewer studies on survival time, given its challenging characteristics such as censoring. In recent biomedical research, two-level analysis of both genes and their involved pathways has received much attention and been demonstrated as more effective than single-level analysis. However, such analysis is usually limited to main effects. Pathways are not isolated, and their interactions have also been suggested to have important contributions to the prognosis of complex diseases. In this paper, we develop a novel two-level Bayesian interaction analysis approach for survival data. This approach is the first to conduct the analysis of lower-level gene–gene interactions and higher-level pathway–pathway interactions simultaneously. Significantly advancing from the existing Bayesian studies based on the Markov Chain Monte Carlo (MCMC) technique, we propose a variational inference framework based on the accelerated failure time model with effective priors to accommodate two-level selection as well as censoring. Its computational efficiency is much desirable for high-dimensional interaction analysis. We examine performance of the proposed approach using extensive simulation. The application to TCGA melanoma and lung adenocarcinoma data leads to biologically sensible findings with satisfactory prediction accuracy and selection stability.Bayesian analysis, interaction analysis, survival data, two-level selection"
"doi.org/10.1111/biom.13800","Efficient Targeted Learning of Heterogeneous Treatment Effects for Multiple Subgroups","In biomedical science, analyzing treatment effect heterogeneity plays an essential role in assisting personalized medicine. The main goals of analyzing treatment effect heterogeneity include estimating treatment effects in clinically relevant subgroups and predicting whether a patient subpopulation might benefit from a particular treatment. Conventional approaches often evaluate the subgroup treatment effects via parametric modeling and can thus be susceptible to model mis-specifications. In this paper, we take a model-free semiparametric perspective and aim to efficiently evaluate the heterogeneous treatment effects of multiple subgroups simultaneously under the one-step targeted maximum-likelihood estimation (TMLE) framework. When the number of subgroups is large, we further expand this path of research by looking at a variation of the one-step TMLE that is robust to the presence of small estimated propensity scores in finite samples. From our simulations, our method demonstrates substantial finite sample improvements compared to conventional methods. In a case study, our method unveils the potential treatment effect heterogeneity of rs12916-T allele (a proxy for statin usage) in decreasing Alzheimer's disease risk.","causal inference, precision medicine, semiparametric statistics, subgroup analysis, treatment effect heterogeneity","Biometrics","Efficient Targeted Learning of Heterogeneous Treatment Effects for Multiple SubgroupsIn biomedical science, analyzing treatment effect heterogeneity plays an essential role in assisting personalized medicine. The main goals of analyzing treatment effect heterogeneity include estimating treatment effects in clinically relevant subgroups and predicting whether a patient subpopulation might benefit from a particular treatment. Conventional approaches often evaluate the subgroup treatment effects via parametric modeling and can thus be susceptible to model mis-specifications. In this paper, we take a model-free semiparametric perspective and aim to efficiently evaluate the heterogeneous treatment effects of multiple subgroups simultaneously under the one-step targeted maximum-likelihood estimation (TMLE) framework. When the number of subgroups is large, we further expand this path of research by looking at a variation of the one-step TMLE that is robust to the presence of small estimated propensity scores in finite samples. From our simulations, our method demonstrates substantial finite sample improvements compared to conventional methods. In a case study, our method unveils the potential treatment effect heterogeneity of rs12916-T allele (a proxy for statin usage) in decreasing Alzheimer's disease risk.causal inference, precision medicine, semiparametric statistics, subgroup analysis, treatment effect heterogeneity"
"doi.org/10.1111/biom.13795","A Semiparametric Joint Model for Cluster Size and Subunit-Specific Interval-Censored Outcomes ","Clustered data frequently arise in biomedical studies, where observations, or subunits, measured within a cluster are associated. The cluster size is said to be informative, if the outcome variable is associated with the number of subunits in a cluster. In most existing work, the informative cluster size issue is handled by marginal approaches based on within-cluster resampling, or cluster-weighted generalized estimating equations. Although these approaches yield consistent estimation of the marginal models, they do not allow estimation of within-cluster associations and are generally inefficient. In this paper, we propose a semiparametric joint model for clustered interval-censored event time data with informative cluster size. We use a random effect to account for the association among event times of the same cluster as well as the association between event times and the cluster size. For estimation, we propose a sieve maximum likelihood approach and devise a computationally-efficient expectation-maximization algorithm for implementation. The estimators are shown to be strongly consistent, with the Euclidean components being asymptotically normal and achieving semiparametric efficiency. Extensive simulation studies are conducted to evaluate the finite-sample performance, efficiency and robustness of the proposed method. We also illustrate our method via application to a motivating periodontal disease dataset.","dental study, EM algorithm, informative cluster size, random effect model, sieve estimation","Biometrics","A Semiparametric Joint Model for Cluster Size and Subunit-Specific Interval-Censored Outcomes Clustered data frequently arise in biomedical studies, where observations, or subunits, measured within a cluster are associated. The cluster size is said to be informative, if the outcome variable is associated with the number of subunits in a cluster. In most existing work, the informative cluster size issue is handled by marginal approaches based on within-cluster resampling, or cluster-weighted generalized estimating equations. Although these approaches yield consistent estimation of the marginal models, they do not allow estimation of within-cluster associations and are generally inefficient. In this paper, we propose a semiparametric joint model for clustered interval-censored event time data with informative cluster size. We use a random effect to account for the association among event times of the same cluster as well as the association between event times and the cluster size. For estimation, we propose a sieve maximum likelihood approach and devise a computationally-efficient expectation-maximization algorithm for implementation. The estimators are shown to be strongly consistent, with the Euclidean components being asymptotically normal and achieving semiparametric efficiency. Extensive simulation studies are conducted to evaluate the finite-sample performance, efficiency and robustness of the proposed method. We also illustrate our method via application to a motivating periodontal disease dataset.dental study, EM algorithm, informative cluster size, random effect model, sieve estimation"
"doi.org/10.1111/biom.13792","Instrumental Variable Estimation of the Causal Hazard Ratio","Cox's proportional hazards model is one of the most popular statistical models to evaluate associations of exposure with a censored failure time outcome. When confounding factors are not fully observed, the exposure hazard ratio estimated using a Cox model is subject to unmeasured confounding bias. To address this, we propose a novel approach for the identification and estimation of the causal hazard ratio in the presence of unmeasured confounding factors. Our approach is based on a binary instrumental variable, and an additional no-interaction assumption in a first-stage regression of the treatment on the IV and unmeasured confounders. We propose, to the best of our knowledge, the first consistent estimator of the (population) causal hazard ratio within an instrumental variable framework. A version of our estimator admits a closed-form representation. We derive the asymptotic distribution of our estimator and provide a consistent estimator for its asymptotic variance. Our approach is illustrated via simulation studies and a data application.","causal inference, Cox model, marginal structural model, survival analysis, unmeasured confounding","Biometrics","Instrumental Variable Estimation of the Causal Hazard RatioCox's proportional hazards model is one of the most popular statistical models to evaluate associations of exposure with a censored failure time outcome. When confounding factors are not fully observed, the exposure hazard ratio estimated using a Cox model is subject to unmeasured confounding bias. To address this, we propose a novel approach for the identification and estimation of the causal hazard ratio in the presence of unmeasured confounding factors. Our approach is based on a binary instrumental variable, and an additional no-interaction assumption in a first-stage regression of the treatment on the IV and unmeasured confounders. We propose, to the best of our knowledge, the first consistent estimator of the (population) causal hazard ratio within an instrumental variable framework. A version of our estimator admits a closed-form representation. We derive the asymptotic distribution of our estimator and provide a consistent estimator for its asymptotic variance. Our approach is illustrated via simulation studies and a data application.causal inference, Cox model, marginal structural model, survival analysis, unmeasured confounding"
"doi.org/10.1111/biom.13787","Marginal Proportional Hazards Models for Clustered Interval-Censored Data with Time-Dependent Covariates ","The Botswana Combination Prevention Project was a cluster-randomized HIV prevention trial whose follow-up period coincided with Botswana's national adoption of a universal test and treat strategy for HIV management. Of interest is whether, and to what extent, this change in policy modified the preventative effects of the study intervention. To address such questions, we adopt a stratified proportional hazards model for clustered interval-censored data with time-dependent covariates and develop a composite expectation maximization algorithm that facilitates estimation of model parameters without placing parametric assumptions on either the baseline hazard functions or the within-cluster dependence structure. We show that the resulting estimators for the regression parameters are consistent and asymptotically normal. We also propose and provide theoretical justification for the use of the profile composite likelihood function to construct a robust sandwich estimator for the variance. We characterize the finite-sample performance and robustness of these estimators through extensive simulation studies. Finally, we conclude by applying this stratified proportional hazards model to a re-analysis of the Botswana Combination Prevention Project, with the national adoption of a universal test and treat strategy now modeled as a time-dependent covariate.","clustered failure time data, composite em algorithm, composite likelihood, HIV, interval censoring, marginal models, nonparametric likelihood, proportional hazards, semiparametric regression, time-dependent covariates","Biometrics","Marginal Proportional Hazards Models for Clustered Interval-Censored Data with Time-Dependent Covariates The Botswana Combination Prevention Project was a cluster-randomized HIV prevention trial whose follow-up period coincided with Botswana's national adoption of a universal test and treat strategy for HIV management. Of interest is whether, and to what extent, this change in policy modified the preventative effects of the study intervention. To address such questions, we adopt a stratified proportional hazards model for clustered interval-censored data with time-dependent covariates and develop a composite expectation maximization algorithm that facilitates estimation of model parameters without placing parametric assumptions on either the baseline hazard functions or the within-cluster dependence structure. We show that the resulting estimators for the regression parameters are consistent and asymptotically normal. We also propose and provide theoretical justification for the use of the profile composite likelihood function to construct a robust sandwich estimator for the variance. We characterize the finite-sample performance and robustness of these estimators through extensive simulation studies. Finally, we conclude by applying this stratified proportional hazards model to a re-analysis of the Botswana Combination Prevention Project, with the national adoption of a universal test and treat strategy now modeled as a time-dependent covariate.clustered failure time data, composite em algorithm, composite likelihood, HIV, interval censoring, marginal models, nonparametric likelihood, proportional hazards, semiparametric regression, time-dependent covariates"
"doi.org/10.1111/biom.13783","Instrumented Difference-in-Differences","Unmeasured confounding is a key threat to reliable causal inference based on observational studies. Motivated from two powerful natural experiment devices, the instrumental variables and difference-in-differences, we propose a new method called instrumented difference-in-differences that explicitly leverages exogenous randomness in an exposure trend to estimate the average and conditional average treatment effect in the presence of unmeasured confounding. We develop the identification assumptions using the potential outcomes framework. We propose a Wald estimator and a class of multiply robust and efficient semiparametric estimators, with provable consistency and asymptotic normality. In addition, we extend the instrumented difference-in-differences to a two-sample design to facilitate investigations of delayed treatment effect and provide a measure of weak identification. We demonstrate our results in simulated and real datasets.","causal inference, effect modification, exclusion restriction, instrumental variables, multiple robustness","Biometrics","Instrumented Difference-in-DifferencesUnmeasured confounding is a key threat to reliable causal inference based on observational studies. Motivated from two powerful natural experiment devices, the instrumental variables and difference-in-differences, we propose a new method called instrumented difference-in-differences that explicitly leverages exogenous randomness in an exposure trend to estimate the average and conditional average treatment effect in the presence of unmeasured confounding. We develop the identification assumptions using the potential outcomes framework. We propose a Wald estimator and a class of multiply robust and efficient semiparametric estimators, with provable consistency and asymptotic normality. In addition, we extend the instrumented difference-in-differences to a two-sample design to facilitate investigations of delayed treatment effect and provide a measure of weak identification. We demonstrate our results in simulated and real datasets.causal inference, effect modification, exclusion restriction, instrumental variables, multiple robustness"
"doi.org/10.1111/biom.13782","Statistical Inference and Power Analysis for Direct and Spillover Effects in Two-Stage Randomized Experiments","Two-stage randomized experiments become an increasingly popular experimental design for causal inference when the outcome of one unit may be affected by the treatment assignments of other units in the same cluster. In this paper, we provide a methodological framework for general tools of statistical inference and power analysis for two-stage randomized experiments. Under the randomization-based framework, we consider the estimation of a new direct effect of interest as well as the average direct and spillover effects studied in the literature. We provide unbiased estimators of these causal quantities and their conservative variance estimators in a general setting. Using these results, we then develop hypothesis testing procedures and derive sample size formulas. We theoretically compare the two-stage randomized design with the completely randomized and cluster randomized designs, which represent two limiting designs. Finally, we conduct simulation studies to evaluate the empirical performance of our sample size formulas. For empirical illustration, the proposed methodology is applied to the randomized evaluation of the Indian National Health Insurance Program. An open-source software package is available for implementing the proposed methodology.","experimental design, interference between units, partial interference, spillover effects, statistical power","Biometrics","Statistical Inference and Power Analysis for Direct and Spillover Effects in Two-Stage Randomized ExperimentsTwo-stage randomized experiments become an increasingly popular experimental design for causal inference when the outcome of one unit may be affected by the treatment assignments of other units in the same cluster. In this paper, we provide a methodological framework for general tools of statistical inference and power analysis for two-stage randomized experiments. Under the randomization-based framework, we consider the estimation of a new direct effect of interest as well as the average direct and spillover effects studied in the literature. We provide unbiased estimators of these causal quantities and their conservative variance estimators in a general setting. Using these results, we then develop hypothesis testing procedures and derive sample size formulas. We theoretically compare the two-stage randomized design with the completely randomized and cluster randomized designs, which represent two limiting designs. Finally, we conduct simulation studies to evaluate the empirical performance of our sample size formulas. For empirical illustration, the proposed methodology is applied to the randomized evaluation of the Indian National Health Insurance Program. An open-source software package is available for implementing the proposed methodology.experimental design, interference between units, partial interference, spillover effects, statistical power"
"doi.org/10.1111/biom.13775","Identifying Brain Hierarchical Structures Associated with Alzheimer's Disease Using a Regularized Regression Method with Tree Predictors","Brain segmentation at different levels is generally represented as hierarchical trees. Brain regional atrophy at specific levels was found to be marginally associated with Alzheimer's disease outcomes. In this study, we propose an ℓ1-type regularization for predictors that follow a hierarchical tree structure. Considering a tree as a directed acyclic graph, we interpret the model parameters from a path analysis perspective. Under this concept, the proposed penalty regulates the total effect of each predictor on the outcome. With regularity conditions, it is shown that under the proposed regularization, the estimator of the model coefficient is consistent in ℓ2-norm and the model selection is also consistent. When applied to a brain sMRI dataset acquired from the Alzheimer's Disease Neuroimaging Initiative (ADNI), the proposed approach identifies brain regions where atrophy in these regions demonstrates the declination in memory. With regularization on the total effects, the findings suggest that the impact of atrophy on memory deficits is localized from small brain regions, but at various levels of brain segmentation. Data used in preparation of this paper were obtained from the ADNI database.","hierarchical predictors, path analysis, penalized linear models, structural neuroimaging, tree-based regularization","Biometrics","Identifying Brain Hierarchical Structures Associated with Alzheimer's Disease Using a Regularized Regression Method with Tree PredictorsBrain segmentation at different levels is generally represented as hierarchical trees. Brain regional atrophy at specific levels was found to be marginally associated with Alzheimer's disease outcomes. In this study, we propose an ℓ1-type regularization for predictors that follow a hierarchical tree structure. Considering a tree as a directed acyclic graph, we interpret the model parameters from a path analysis perspective. Under this concept, the proposed penalty regulates the total effect of each predictor on the outcome. With regularity conditions, it is shown that under the proposed regularization, the estimator of the model coefficient is consistent in ℓ2-norm and the model selection is also consistent. When applied to a brain sMRI dataset acquired from the Alzheimer's Disease Neuroimaging Initiative (ADNI), the proposed approach identifies brain regions where atrophy in these regions demonstrates the declination in memory. With regularization on the total effects, the findings suggest that the impact of atrophy on memory deficits is localized from small brain regions, but at various levels of brain segmentation. Data used in preparation of this paper were obtained from the ADNI database.hierarchical predictors, path analysis, penalized linear models, structural neuroimaging, tree-based regularization"
"doi.org/10.1111/biom.13776","Combining Parametric and Nonparametric Models to Estimate Treatment Effects in Observational Studies","Performing causal inference in observational studies requires we assume confounding variables are correctly adjusted for. In settings with few discrete-valued confounders, standard models can be employed. However, as the number of confounders increases these models become less feasible as there are fewer observations available for each unique combination of confounding variables. In this paper, we propose a new model for estimating treatment effects in observational studies that incorporates both parametric and nonparametric outcome models. By conceptually splitting the data, we can combine these models while maintaining a conjugate framework, allowing us to avoid the use of Markov chain Monte Carlo (MCMC) methods. Approximations using the central limit theorem and random sampling allow our method to be scaled to high-dimensional confounders. Through simulation studies we show our method can be competitive with benchmark models while maintaining efficient computation, and illustrate the method on a large epidemiological health survey.","Bayesian methods, causal inference, g-computation, nonparametric","Biometrics","Combining Parametric and Nonparametric Models to Estimate Treatment Effects in Observational StudiesPerforming causal inference in observational studies requires we assume confounding variables are correctly adjusted for. In settings with few discrete-valued confounders, standard models can be employed. However, as the number of confounders increases these models become less feasible as there are fewer observations available for each unique combination of confounding variables. In this paper, we propose a new model for estimating treatment effects in observational studies that incorporates both parametric and nonparametric outcome models. By conceptually splitting the data, we can combine these models while maintaining a conjugate framework, allowing us to avoid the use of Markov chain Monte Carlo (MCMC) methods. Approximations using the central limit theorem and random sampling allow our method to be scaled to high-dimensional confounders. Through simulation studies we show our method can be competitive with benchmark models while maintaining efficient computation, and illustrate the method on a large epidemiological health survey.Bayesian methods, causal inference, g-computation, nonparametric"
"doi.org/10.1111/biom.13772","Microbiome Subcommunity Learning with Logistic-Tree Normal Latent Dirichlet Allocation","Mixed-membership (MM) models such as latent Dirichlet allocation (LDA) have been applied to microbiome compositional data to identify latent subcommunities of microbial species. These subcommunities are informative for understanding the biological interplay of microbes and for predicting health outcomes. However, microbiome compositions typically display substantial cross-sample heterogeneities in subcommunity compositions—that is, the variability in the proportions of microbes in shared subcommunities across samples—which is not accounted for in prior analyses. As a result, LDA can produce inference, which is highly sensitive to the specification of the number of subcommunities and often divides a single subcommunity into multiple artificial ones. To address this limitation, we incorporate the logistic-tree normal (LTN) model into LDA to form a new MM model. This model allows cross-sample variation in the composition of each subcommunity around some “centroid” composition that defines the subcommunity. Incorporation of auxiliary Pólya-Gamma variables enables a computationally efficient collapsed blocked Gibbs sampler to carry out Bayesian inference under this model. By accounting for such heterogeneity, our new model restores the robustness of the inference in the specification of the number of subcommunities and allows meaningful subcommunities to be identified.","Bayesian inference, compositional data, latent variable models, mixed-membership models","Biometrics","Microbiome Subcommunity Learning with Logistic-Tree Normal Latent Dirichlet AllocationMixed-membership (MM) models such as latent Dirichlet allocation (LDA) have been applied to microbiome compositional data to identify latent subcommunities of microbial species. These subcommunities are informative for understanding the biological interplay of microbes and for predicting health outcomes. However, microbiome compositions typically display substantial cross-sample heterogeneities in subcommunity compositions—that is, the variability in the proportions of microbes in shared subcommunities across samples—which is not accounted for in prior analyses. As a result, LDA can produce inference, which is highly sensitive to the specification of the number of subcommunities and often divides a single subcommunity into multiple artificial ones. To address this limitation, we incorporate the logistic-tree normal (LTN) model into LDA to form a new MM model. This model allows cross-sample variation in the composition of each subcommunity around some “centroid” composition that defines the subcommunity. Incorporation of auxiliary Pólya-Gamma variables enables a computationally efficient collapsed blocked Gibbs sampler to carry out Bayesian inference under this model. By accounting for such heterogeneity, our new model restores the robustness of the inference in the specification of the number of subcommunities and allows meaningful subcommunities to be identified.Bayesian inference, compositional data, latent variable models, mixed-membership models"
"doi.org/10.1111/biom.13767","Asynchronous Functional Linear Regression Models for Longitudinal Data in Reproducing Kernel Hilbert Space","Motivated by the analysis of longitudinal neuroimaging studies, we study the longitudinal functional linear regression model under asynchronous data setting for modeling the association between clinical outcomes and functional (or imaging) covariates. In the asynchronous data setting, both covariates and responses may be measured at irregular and mismatched time points, posing methodological challenges to existing statistical methods. We develop a kernel weighted loss function with roughness penalty to obtain the functional estimator and derive its representer theorem. The rate of convergence, a Bahadur representation, and the asymptotic pointwise distribution of the functional estimator are obtained under the reproducing kernel Hilbert space framework. We propose a penalized likelihood ratio test to test the nullity of the functional coefficient, derive its asymptotic distribution under the null hypothesis, and investigate the separation rate under the alternative hypotheses. Simulation studies are conducted to examine the finite-sample performance of the proposed procedure. We apply the proposed methods to the analysis of multitype data obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study, which reveals significant association between 21 regional brain volume density curves and the cognitive function. Data used in preparation of this paper were obtained from the ADNI database (adni.loni.usc.edu).","asynchronous longitudinal functional data, Bahadur representation, functional regression, kernel-weighted loss function, penalized likelihood ratio test, reproducing kernel Hilbert space","Biometrics","Asynchronous Functional Linear Regression Models for Longitudinal Data in Reproducing Kernel Hilbert SpaceMotivated by the analysis of longitudinal neuroimaging studies, we study the longitudinal functional linear regression model under asynchronous data setting for modeling the association between clinical outcomes and functional (or imaging) covariates. In the asynchronous data setting, both covariates and responses may be measured at irregular and mismatched time points, posing methodological challenges to existing statistical methods. We develop a kernel weighted loss function with roughness penalty to obtain the functional estimator and derive its representer theorem. The rate of convergence, a Bahadur representation, and the asymptotic pointwise distribution of the functional estimator are obtained under the reproducing kernel Hilbert space framework. We propose a penalized likelihood ratio test to test the nullity of the functional coefficient, derive its asymptotic distribution under the null hypothesis, and investigate the separation rate under the alternative hypotheses. Simulation studies are conducted to examine the finite-sample performance of the proposed procedure. We apply the proposed methods to the analysis of multitype data obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study, which reveals significant association between 21 regional brain volume density curves and the cognitive function. Data used in preparation of this paper were obtained from the ADNI database (adni.loni.usc.edu).asynchronous longitudinal functional data, Bahadur representation, functional regression, kernel-weighted loss function, penalized likelihood ratio test, reproducing kernel Hilbert space"
"doi.org/10.1111/biom.13762","Change-Plane Analysis for Subgroup Detection with a Continuous Treatment","Detecting and characterizing subgroups with differential effects of a binary treatment has been widely studied and led to improvements in patient outcomes and population risk management. Under the setting of a continuous treatment, however, such investigations remain scarce. We propose a semiparametric change-plane model and consequently a doubly robust test statistic for assessing the existence of two subgroups with differential treatment effects under a continuous treatment. The proposed testing procedure is valid when either the baseline function for the covariate effects or the generalized propensity score function for the continuous treatment is correctly specified. The asymptotic distributions of the test statistic under the null and local alternative hypotheses are established. When the null hypothesis of no subgroup is rejected, the change-plane parameters that define the subgroups can be estimated. This paper provides a unified framework of the change-plane method to handle various types of outcomes, including the exponential family of distributions and time-to-event outcomes. Additional extensions with nonparametric estimation approaches are also provided. We evaluate the performance of our proposed methods through extensive simulation studies under various scenarios. An application to the Health Effects of Arsenic Longitudinal Study with a continuous environmental exposure of arsenic is presented.","double robustness, environmental exposure, heterogeneity, semiparametric model, spline","Biometrics","Change-Plane Analysis for Subgroup Detection with a Continuous TreatmentDetecting and characterizing subgroups with differential effects of a binary treatment has been widely studied and led to improvements in patient outcomes and population risk management. Under the setting of a continuous treatment, however, such investigations remain scarce. We propose a semiparametric change-plane model and consequently a doubly robust test statistic for assessing the existence of two subgroups with differential treatment effects under a continuous treatment. The proposed testing procedure is valid when either the baseline function for the covariate effects or the generalized propensity score function for the continuous treatment is correctly specified. The asymptotic distributions of the test statistic under the null and local alternative hypotheses are established. When the null hypothesis of no subgroup is rejected, the change-plane parameters that define the subgroups can be estimated. This paper provides a unified framework of the change-plane method to handle various types of outcomes, including the exponential family of distributions and time-to-event outcomes. Additional extensions with nonparametric estimation approaches are also provided. We evaluate the performance of our proposed methods through extensive simulation studies under various scenarios. An application to the Health Effects of Arsenic Longitudinal Study with a continuous environmental exposure of arsenic is presented.double robustness, environmental exposure, heterogeneity, semiparametric model, spline"
"doi.org/10.1111/biom.13764","Maximum Likelihood Estimation in the Additive Hazards Model","The additive hazards model specifies the effect of covariates on the hazard in an additive way, in contrast to the popular Cox model, in which it is multiplicative. As the non-parametric model, additive hazards offer a very flexible way of modeling time-varying covariate effects. It is most commonly estimated by ordinary least squares. In this paper, we consider the case where covariates are bounded, and derive the maximum likelihood estimator under the constraint that the hazard is non-negative for all covariate values in their domain. We show that the maximum likelihood estimator may be obtained by separately maximizing the log-likelihood contribution of each event time point, and we show that the maximizing problem is equivalent to fitting a series of Poisson regression models with an identity link under non-negativity constraints. We derive an analytic solution to the maximum likelihood estimator. We contrast the maximum likelihood estimator with the ordinary least-squares estimator in a simulation study and show that the maximum likelihood estimator has smaller mean squared error than the ordinary least-squares estimator. An illustration with data on patients with carcinoma of the oropharynx is provided.","additive hazards, constrained optimization, maximum likelihood","Biometrics","Maximum Likelihood Estimation in the Additive Hazards ModelThe additive hazards model specifies the effect of covariates on the hazard in an additive way, in contrast to the popular Cox model, in which it is multiplicative. As the non-parametric model, additive hazards offer a very flexible way of modeling time-varying covariate effects. It is most commonly estimated by ordinary least squares. In this paper, we consider the case where covariates are bounded, and derive the maximum likelihood estimator under the constraint that the hazard is non-negative for all covariate values in their domain. We show that the maximum likelihood estimator may be obtained by separately maximizing the log-likelihood contribution of each event time point, and we show that the maximizing problem is equivalent to fitting a series of Poisson regression models with an identity link under non-negativity constraints. We derive an analytic solution to the maximum likelihood estimator. We contrast the maximum likelihood estimator with the ordinary least-squares estimator in a simulation study and show that the maximum likelihood estimator has smaller mean squared error than the ordinary least-squares estimator. An illustration with data on patients with carcinoma of the oropharynx is provided.additive hazards, constrained optimization, maximum likelihood"
"doi.org/10.1111/biom.13756","A General Modeling Framework for Open Wildlife Populations Based on the Polya Tree Prior","Wildlife monitoring for open populations can be performed using a number of different survey methods. Each survey method gives rise to a type of data and, in the last five decades, a large number of associated statistical models have been developed for analyzing these data. Although these models have been parameterized and fitted using different approaches, they have all been designed to either model the pattern with which individuals enter and/or exit the population, or to estimate the population size by accounting for the corresponding observation process, or both. However, existing approaches rely on a predefined model structure and complexity, either by assuming that parameters linked to the entry and exit pattern (EEP) are specific to sampling occasions, or by employing parametric curves to describe the EEP. Instead, we propose a novel Bayesian nonparametric framework for modeling EEPs based on the Polya tree (PT) prior for densities. Our Bayesian nonparametric approach avoids overfitting when inferring EEPs, while simultaneously allowing more flexibility than is possible using parametric curves. Finally, we introduce the replicate PT prior for defining classes of models for these data allowing us to impose constraints on the EEPs, when required. We demonstrate our new approach using capture–recapture, count, and ring-recovery data for two different case studies.","Bayesian nonparametrics, capture–recapture, count data, Polya tree, ring recovery, statistical ecology","Biometrics","A General Modeling Framework for Open Wildlife Populations Based on the Polya Tree PriorWildlife monitoring for open populations can be performed using a number of different survey methods. Each survey method gives rise to a type of data and, in the last five decades, a large number of associated statistical models have been developed for analyzing these data. Although these models have been parameterized and fitted using different approaches, they have all been designed to either model the pattern with which individuals enter and/or exit the population, or to estimate the population size by accounting for the corresponding observation process, or both. However, existing approaches rely on a predefined model structure and complexity, either by assuming that parameters linked to the entry and exit pattern (EEP) are specific to sampling occasions, or by employing parametric curves to describe the EEP. Instead, we propose a novel Bayesian nonparametric framework for modeling EEPs based on the Polya tree (PT) prior for densities. Our Bayesian nonparametric approach avoids overfitting when inferring EEPs, while simultaneously allowing more flexibility than is possible using parametric curves. Finally, we introduce the replicate PT prior for defining classes of models for these data allowing us to impose constraints on the EEPs, when required. We demonstrate our new approach using capture–recapture, count, and ring-recovery data for two different case studies.Bayesian nonparametrics, capture–recapture, count data, Polya tree, ring recovery, statistical ecology"
"doi.org/10.1111/biom.13755","Multidimensional Adaptive P-Splines with Application to Neurons' Activity Studies","The receptive field (RF) of a visual neuron is the region of the space that elicits neuronal responses. It can be mapped using different techniques that allow inferring its spatial and temporal properties. Raw RF maps (RFmaps) are usually noisy, making it difficult to obtain and study important features of the RF. A possible solution is to smooth them using P-splines. Yet, raw RFmaps are characterized by sharp transitions in both space and time. Their analysis thus asks for spatiotemporal adaptive P-spline models, where smoothness can be locally adapted to the data. However, the literature lacks proposals for adaptive P-splines in more than two dimensions. Furthermore, the extra flexibility afforded by adaptive P-spline models is obtained at the cost of a high computational burden, especially in a multidimensional setting. To fill these gaps, this work presents a novel anisotropic locally adaptive P-spline model in two (e.g., space) and three (space and time) dimensions. Estimation is based on the recently proposed SOP (Separation of Overlapping Precision matrices) method, which provides the speed we look for. Besides the spatiotemporal analysis of the neuronal activity data that motivated this work, the practical performance of the proposal is evaluated through simulations, and comparisons with alternative methods are reported.","anisotropy, local adaptivity, penalized splines, smoothing, visual receptive fields","Biometrics","Multidimensional Adaptive P-Splines with Application to Neurons' Activity StudiesThe receptive field (RF) of a visual neuron is the region of the space that elicits neuronal responses. It can be mapped using different techniques that allow inferring its spatial and temporal properties. Raw RF maps (RFmaps) are usually noisy, making it difficult to obtain and study important features of the RF. A possible solution is to smooth them using P-splines. Yet, raw RFmaps are characterized by sharp transitions in both space and time. Their analysis thus asks for spatiotemporal adaptive P-spline models, where smoothness can be locally adapted to the data. However, the literature lacks proposals for adaptive P-splines in more than two dimensions. Furthermore, the extra flexibility afforded by adaptive P-spline models is obtained at the cost of a high computational burden, especially in a multidimensional setting. To fill these gaps, this work presents a novel anisotropic locally adaptive P-spline model in two (e.g., space) and three (space and time) dimensions. Estimation is based on the recently proposed SOP (Separation of Overlapping Precision matrices) method, which provides the speed we look for. Besides the spatiotemporal analysis of the neuronal activity data that motivated this work, the practical performance of the proposal is evaluated through simulations, and comparisons with alternative methods are reported.anisotropy, local adaptivity, penalized splines, smoothing, visual receptive fields"
"doi.org/10.1111/biom.13735","Mendelian Randomization Mixed-Scale Treatment Effect Robust Identification and Estimation for Causal Inference","Standard Mendelian randomization (MR) analysis can produce biased results if the genetic variant defining an instrumental variable (IV) is confounded and/or has a horizontal pleiotropic effect on the outcome of interest not mediated by the treatment variable. We provide novel identification conditions for the causal effect of a treatment in the presence of unmeasured confounding by leveraging a possibly invalid IV for which both the IV independence and exclusion restriction assumptions may be violated. The proposed Mendelian randomization mixed-scale treatment effect robust identification (MR MiSTERI) approach relies on (i) an assumption that the treatment effect does not vary with the possibly invalid IV on the additive scale; (ii) that the confounding bias does not vary with the possibly invalid IV on the odds ratio scale; and (iii) that the residual variance for the outcome is heteroskedastic with respect to the possibly invalid IV. Although assumptions (i) and (ii) have, respectively, appeared in the IV literature, assumption (iii) has not; we formally establish that their conjunction can identify a causal effect even with an invalid IV. MR MiSTERI is shown to be particularly advantageous in the presence of pervasive heterogeneity of pleiotropic effects on the additive scale. We propose a simple and consistent three-stage estimator that can be used as a preliminary estimator to a carefully constructed efficient one-step-update estimator. In order to incorporate multiple, possibly correlated, and weak invalid IVs, a common challenge in MR studies, we develop a MAny Weak Invalid Instruments (MR MaWII MiSTERI) approach for strengthened identification and improved estimation accuracy. Both simulation studies and UK Biobank data analysis results demonstrate the robustness of the proposed methods.","causal inference, horizontal pleiotropy, invalid instrument, Mendelian randomization, unmeasured confounding, weak instrument","Biometrics","Mendelian Randomization Mixed-Scale Treatment Effect Robust Identification and Estimation for Causal InferenceStandard Mendelian randomization (MR) analysis can produce biased results if the genetic variant defining an instrumental variable (IV) is confounded and/or has a horizontal pleiotropic effect on the outcome of interest not mediated by the treatment variable. We provide novel identification conditions for the causal effect of a treatment in the presence of unmeasured confounding by leveraging a possibly invalid IV for which both the IV independence and exclusion restriction assumptions may be violated. The proposed Mendelian randomization mixed-scale treatment effect robust identification (MR MiSTERI) approach relies on (i) an assumption that the treatment effect does not vary with the possibly invalid IV on the additive scale; (ii) that the confounding bias does not vary with the possibly invalid IV on the odds ratio scale; and (iii) that the residual variance for the outcome is heteroskedastic with respect to the possibly invalid IV. Although assumptions (i) and (ii) have, respectively, appeared in the IV literature, assumption (iii) has not; we formally establish that their conjunction can identify a causal effect even with an invalid IV. MR MiSTERI is shown to be particularly advantageous in the presence of pervasive heterogeneity of pleiotropic effects on the additive scale. We propose a simple and consistent three-stage estimator that can be used as a preliminary estimator to a carefully constructed efficient one-step-update estimator. In order to incorporate multiple, possibly correlated, and weak invalid IVs, a common challenge in MR studies, we develop a MAny Weak Invalid Instruments (MR MaWII MiSTERI) approach for strengthened identification and improved estimation accuracy. Both simulation studies and UK Biobank data analysis results demonstrate the robustness of the proposed methods.causal inference, horizontal pleiotropy, invalid instrument, Mendelian randomization, unmeasured confounding, weak instrument"
"doi.org/10.1111/biom.13732","A Novel Penalized Inverse-Variance Weighted Estimator for Mendelian Randomization with Applications to COVID-19 Outcomes","Mendelian randomization utilizes genetic variants as instrumental variables (IVs) to estimate the causal effect of an exposure variable on an outcome of interest even in the presence of unmeasured confounders. However, the popular inverse-variance weighted (IVW) estimator could be biased in the presence of weak IVs, a common challenge in MR studies. In this article, we develop a novel penalized inverse-variance weighted (pIVW) estimator, which adjusts the original IVW estimator to account for the weak IV issue by using a penalization approach to prevent the denominator of the pIVW estimator from being close to zero. Moreover, we adjust the variance estimation of the pIVW estimator to account for the presence of balanced horizontal pleiotropy. We show that the recently proposed debiased IVW (dIVW) estimator is a special case of our proposed pIVW estimator. We further prove that the pIVW estimator has smaller bias and variance than the dIVW estimator under some regularity conditions. We also conduct extensive simulation studies to demonstrate the performance of the proposed pIVW estimator. Furthermore, we apply the pIVW estimator to estimate the causal effects of five obesity-related exposures on three coronavirus disease 2019 (COVID-19) outcomes. Notably, we find that hypertensive disease is associated with an increased risk of hospitalized COVID-19; and peripheral vascular disease and higher body mass index are associated with increased risks of COVID-19 infection, hospitalized COVID-19, and critically ill COVID-19.","COVID-19, horizontal pleiotropy, instrumental variables, Mendelian randomization, penalization, weak instruments","Biometrics","A Novel Penalized Inverse-Variance Weighted Estimator for Mendelian Randomization with Applications to COVID-19 OutcomesMendelian randomization utilizes genetic variants as instrumental variables (IVs) to estimate the causal effect of an exposure variable on an outcome of interest even in the presence of unmeasured confounders. However, the popular inverse-variance weighted (IVW) estimator could be biased in the presence of weak IVs, a common challenge in MR studies. In this article, we develop a novel penalized inverse-variance weighted (pIVW) estimator, which adjusts the original IVW estimator to account for the weak IV issue by using a penalization approach to prevent the denominator of the pIVW estimator from being close to zero. Moreover, we adjust the variance estimation of the pIVW estimator to account for the presence of balanced horizontal pleiotropy. We show that the recently proposed debiased IVW (dIVW) estimator is a special case of our proposed pIVW estimator. We further prove that the pIVW estimator has smaller bias and variance than the dIVW estimator under some regularity conditions. We also conduct extensive simulation studies to demonstrate the performance of the proposed pIVW estimator. Furthermore, we apply the pIVW estimator to estimate the causal effects of five obesity-related exposures on three coronavirus disease 2019 (COVID-19) outcomes. Notably, we find that hypertensive disease is associated with an increased risk of hospitalized COVID-19; and peripheral vascular disease and higher body mass index are associated with increased risks of COVID-19 infection, hospitalized COVID-19, and critically ill COVID-19.COVID-19, horizontal pleiotropy, instrumental variables, Mendelian randomization, penalization, weak instruments"
"doi.org/10.1111/biom.13726","Optimal Multiple Testing and Design in Clinical Trials","A central goal in designing clinical trials is to find the test that maximizes power (or equivalently minimizes required sample size) for finding a false null hypothesis subject to the constraint of type I error. When there is more than one test, such as in clinical trials with multiple endpoints, the issues of optimal design and optimal procedures become more complex. In this paper, we address the question of how such optimal tests should be defined and how they can be found. We review different notions of power and how they relate to study goals, and also consider the requirements of type I error control and the nature of the procedures. This leads us to an explicit optimization problem with objective and constraints that describe its specific desiderata. We present a complete solution for deriving optimal procedures for two hypotheses, which have desired monotonicity properties, and are computationally simple. For some of the optimization formulations this yields optimal procedures that are identical to existing procedures, such as Hommel's procedure or the procedure of Bittman et al. (2009), while for other cases it yields completely novel and more powerful procedures than existing ones. We demonstrate the nature of our novel procedures and their improved power extensively in a simulation and on the APEX study (Cohen et al., 2016).","family wise error, most powerful test, multiple end-points, sample size determination, strong control","Biometrics","Optimal Multiple Testing and Design in Clinical TrialsA central goal in designing clinical trials is to find the test that maximizes power (or equivalently minimizes required sample size) for finding a false null hypothesis subject to the constraint of type I error. When there is more than one test, such as in clinical trials with multiple endpoints, the issues of optimal design and optimal procedures become more complex. In this paper, we address the question of how such optimal tests should be defined and how they can be found. We review different notions of power and how they relate to study goals, and also consider the requirements of type I error control and the nature of the procedures. This leads us to an explicit optimization problem with objective and constraints that describe its specific desiderata. We present a complete solution for deriving optimal procedures for two hypotheses, which have desired monotonicity properties, and are computationally simple. For some of the optimization formulations this yields optimal procedures that are identical to existing procedures, such as Hommel's procedure or the procedure of Bittman et al. (2009), while for other cases it yields completely novel and more powerful procedures than existing ones. We demonstrate the nature of our novel procedures and their improved power extensively in a simulation and on the APEX study (Cohen et al., 2016).family wise error, most powerful test, multiple end-points, sample size determination, strong control"
"doi.org/10.1111/biom.13724","Translocation Detection from Hi-C Data via Scan Statistics","Recent Hi-C technology enables more comprehensive chromosomal conformation research, including the detection of structural variations, especially translocations. In this paper, we formulate the interchromosomal translocation detection as a problem of scan clustering in a spatial point process. We then develop TranScan, a new translocation detection method through scan statistics with the control of false discovery. The simulation shows that TranScan is more powerful than an existing sophisticated scan clustering method, especially under strong signal situations. Evaluation of TranScan against current translocation detection methods on realistic breakpoint simulations generated from real data suggests better discriminative power under the receiver-operating characteristic curve. Power analysis also highlights TranScan's consistent outperformance when sequencing depth and heterozygosity rate is varied. Comparatively, Type I error rate is lowest when evaluated using a karyotypically normal cell line. Both the simulation and real data analysis indicate that TranScan has great potentials in interchromosomal translocation detection using Hi-C data.","anomaly detection, false discovery exceedance, hot-spot(s) detection, scan clustering","Biometrics","Translocation Detection from Hi-C Data via Scan StatisticsRecent Hi-C technology enables more comprehensive chromosomal conformation research, including the detection of structural variations, especially translocations. In this paper, we formulate the interchromosomal translocation detection as a problem of scan clustering in a spatial point process. We then develop TranScan, a new translocation detection method through scan statistics with the control of false discovery. The simulation shows that TranScan is more powerful than an existing sophisticated scan clustering method, especially under strong signal situations. Evaluation of TranScan against current translocation detection methods on realistic breakpoint simulations generated from real data suggests better discriminative power under the receiver-operating characteristic curve. Power analysis also highlights TranScan's consistent outperformance when sequencing depth and heterozygosity rate is varied. Comparatively, Type I error rate is lowest when evaluated using a karyotypically normal cell line. Both the simulation and real data analysis indicate that TranScan has great potentials in interchromosomal translocation detection using Hi-C data.anomaly detection, false discovery exceedance, hot-spot(s) detection, scan clustering"
"doi.org/10.1111/biom.13723","Quantile Regression for Nonignorable Missing Data with Its Application of Analyzing Electronic Medical Records","Over the past decade, there has been growing enthusiasm for using electronic medical records (EMRs) for biomedical research. Quantile regression estimates distributional associations, providing unique insights into the intricacies and heterogeneity of the EMR data. However, the widespread nonignorable missing observations in EMR often obscure the true associations and challenge its potential for robust biomedical discoveries. We propose a novel method to estimate the covariate effects in the presence of nonignorable missing responses under quantile regression. This method imposes no parametric specifications on response distributions, which subtly uses implicit distributions induced by the corresponding quantile regression models. We show that the proposed estimator is consistent and asymptotically normal. We also provide an efficient algorithm to obtain the proposed estimate and a randomly weighted bootstrap approach for statistical inferences. Numerical studies, including an empirical analysis of real-world EMR data, are used to assess the proposed method's finite-sample performance compared to existing literature.","estimating equations, inverse probability weight, missing data, Monte Carlo integration, quantile regression","Biometrics","Quantile Regression for Nonignorable Missing Data with Its Application of Analyzing Electronic Medical RecordsOver the past decade, there has been growing enthusiasm for using electronic medical records (EMRs) for biomedical research. Quantile regression estimates distributional associations, providing unique insights into the intricacies and heterogeneity of the EMR data. However, the widespread nonignorable missing observations in EMR often obscure the true associations and challenge its potential for robust biomedical discoveries. We propose a novel method to estimate the covariate effects in the presence of nonignorable missing responses under quantile regression. This method imposes no parametric specifications on response distributions, which subtly uses implicit distributions induced by the corresponding quantile regression models. We show that the proposed estimator is consistent and asymptotically normal. We also provide an efficient algorithm to obtain the proposed estimate and a randomly weighted bootstrap approach for statistical inferences. Numerical studies, including an empirical analysis of real-world EMR data, are used to assess the proposed method's finite-sample performance compared to existing literature.estimating equations, inverse probability weight, missing data, Monte Carlo integration, quantile regression"
"doi.org/10.1111/biom.13720","Solutions for Surrogacy Validation with Longitudinal Outcomes for a Gene Therapy","Valid surrogate endpoints S can be used as a substitute for a true outcome of interest T to measure treatment efficacy in a clinical trial. We propose a causal inference approach to validate a surrogate by incorporating longitudinal measurements of the true outcomes using a mixed modeling approach, and we define models and quantities for validation that may vary across the study period using principal surrogacy criteria. We consider a surrogate-dependent treatment efficacy curve that allows us to validate the surrogate at different time points. We extend these methods to accommodate a delayed-start treatment design where all patients eventually receive the treatment. Not all parameters are identified in the general setting. We apply a Bayesian approach for estimation and inference, utilizing more informative prior distributions for selected parameters. We consider the sensitivity of these prior assumptions as well as assumptions of independence among certain counterfactual quantities conditional on pretreatment covariates to improve identifiability. We examine the frequentist properties (bias of point and variance estimates, credible interval coverage) of a Bayesian imputation method. Our work is motivated by a clinical trial of a gene therapy where the functional outcomes are measured repeatedly throughout the trial.","Bayesian methods, crossover design, delayed-start, longitudinal outcomes, principal stratification, surrogate endpoints","Biometrics","Solutions for Surrogacy Validation with Longitudinal Outcomes for a Gene TherapyValid surrogate endpoints S can be used as a substitute for a true outcome of interest T to measure treatment efficacy in a clinical trial. We propose a causal inference approach to validate a surrogate by incorporating longitudinal measurements of the true outcomes using a mixed modeling approach, and we define models and quantities for validation that may vary across the study period using principal surrogacy criteria. We consider a surrogate-dependent treatment efficacy curve that allows us to validate the surrogate at different time points. We extend these methods to accommodate a delayed-start treatment design where all patients eventually receive the treatment. Not all parameters are identified in the general setting. We apply a Bayesian approach for estimation and inference, utilizing more informative prior distributions for selected parameters. We consider the sensitivity of these prior assumptions as well as assumptions of independence among certain counterfactual quantities conditional on pretreatment covariates to improve identifiability. We examine the frequentist properties (bias of point and variance estimates, credible interval coverage) of a Bayesian imputation method. Our work is motivated by a clinical trial of a gene therapy where the functional outcomes are measured repeatedly throughout the trial.Bayesian methods, crossover design, delayed-start, longitudinal outcomes, principal stratification, surrogate endpoints"
"doi.org/10.1111/biom.13718","Grouped Generalized Estimating Equations for Longitudinal Data Analysis","Generalized estimating equation (GEE) is widely adopted for regression modeling for longitudinal data, taking account of potential correlations within the same subjects. Although the standard GEE assumes common regression coefficients among all the subjects, such an assumption may not be realistic when there is potential heterogeneity in regression coefficients among subjects. In this paper, we develop a flexible and interpretable approach, called grouped GEE analysis, to modeling longitudinal data with allowing heterogeneity in regression coefficients. The proposed method assumes that the subjects are divided into a finite number of groups and subjects within the same group share the same regression coefficient. We provide a simple algorithm for grouping subjects and estimating the regression coefficients simultaneously, and show the asymptotic properties of the proposed estimator. The number of groups can be determined by the cross validation with averaging method. We demonstrate the proposed method through simulation studies and an application to a real data set.","estimating equation, grouping, k-means algorithm, unobserved heterogeneity","Biometrics","Grouped Generalized Estimating Equations for Longitudinal Data AnalysisGeneralized estimating equation (GEE) is widely adopted for regression modeling for longitudinal data, taking account of potential correlations within the same subjects. Although the standard GEE assumes common regression coefficients among all the subjects, such an assumption may not be realistic when there is potential heterogeneity in regression coefficients among subjects. In this paper, we develop a flexible and interpretable approach, called grouped GEE analysis, to modeling longitudinal data with allowing heterogeneity in regression coefficients. The proposed method assumes that the subjects are divided into a finite number of groups and subjects within the same group share the same regression coefficient. We provide a simple algorithm for grouping subjects and estimating the regression coefficients simultaneously, and show the asymptotic properties of the proposed estimator. The number of groups can be determined by the cross validation with averaging method. We demonstrate the proposed method through simulation studies and an application to a real data set.estimating equation, grouping, k-means algorithm, unobserved heterogeneity"
"doi.org/10.1111/biom.13716","Efficient and Robust Methods for Causally Interpretable Meta-Analysis: Transporting Inferences from Multiple Randomized Trials to a Target Population","We present methods for causally interpretable meta-analyses that combine information from multiple randomized trials to draw causal inferences for a target population of substantive interest. We consider identifiability conditions, derive implications of the conditions for the law of the observed data, and obtain identification results for transporting causal inferences from a collection of independent randomized trials to a new target population in which experimental data may not be available. We propose an estimator for the potential outcome mean in the target population under each treatment studied in the trials. The estimator uses covariate, treatment, and outcome data from the collection of trials, but only covariate data from the target population sample. We show that it is doubly robust in the sense that it is consistent and asymptotically normal when at least one of the models it relies on is correctly specified. We study the finite sample properties of the estimator in simulation studies and demonstrate its implementation using data from a multicenter randomized trial.","causal inference, combining information, evidence synthesis, generalizability, meta-analysis, research synthesis, transportability","Biometrics","Efficient and Robust Methods for Causally Interpretable Meta-Analysis: Transporting Inferences from Multiple Randomized Trials to a Target PopulationWe present methods for causally interpretable meta-analyses that combine information from multiple randomized trials to draw causal inferences for a target population of substantive interest. We consider identifiability conditions, derive implications of the conditions for the law of the observed data, and obtain identification results for transporting causal inferences from a collection of independent randomized trials to a new target population in which experimental data may not be available. We propose an estimator for the potential outcome mean in the target population under each treatment studied in the trials. The estimator uses covariate, treatment, and outcome data from the collection of trials, but only covariate data from the target population sample. We show that it is doubly robust in the sense that it is consistent and asymptotically normal when at least one of the models it relies on is correctly specified. We study the finite sample properties of the estimator in simulation studies and demonstrate its implementation using data from a multicenter randomized trial.causal inference, combining information, evidence synthesis, generalizability, meta-analysis, research synthesis, transportability"
"doi.org/10.1111/biom.13714","Concordance Indices with Left-Truncated and Right-Censored Data","In the context of time-to-event analysis, a primary objective is to model the risk of experiencing a particular event in relation to a set of observed predictors. The Concordance Index (C-Index) is a statistic frequently used in practice to assess how well such models discriminate between various risk levels in a population. However, the properties of conventional C-Index estimators when applied to left-truncated time-to-event data have not been well studied, despite the fact that left-truncation is commonly encountered in observational studies. We show that the limiting values of the conventional C-Index estimators depend on the underlying distribution of truncation times, which is similar to the situation with right-censoring as discussed in Uno et al. (2011) [On the C-statistics for evaluating overall adequacy of risk prediction procedures with censored survival data. Statistics in Medicine 30(10), 1105–1117]. We develop a new C-Index estimator based on inverse probability weighting (IPW) that corrects for this limitation, and we generalize this estimator to settings with left-truncated and right-censored data. The proposed IPW estimators are highly robust to the underlying truncation distribution and often outperform the conventional methods in terms of bias, mean squared error, and coverage probability. We apply these estimators to evaluate a predictive survival model for mortality among patients with end-stage renal disease.","C-index, left-truncation, risk prediction, survival analysis","Biometrics","Concordance Indices with Left-Truncated and Right-Censored DataIn the context of time-to-event analysis, a primary objective is to model the risk of experiencing a particular event in relation to a set of observed predictors. The Concordance Index (C-Index) is a statistic frequently used in practice to assess how well such models discriminate between various risk levels in a population. However, the properties of conventional C-Index estimators when applied to left-truncated time-to-event data have not been well studied, despite the fact that left-truncation is commonly encountered in observational studies. We show that the limiting values of the conventional C-Index estimators depend on the underlying distribution of truncation times, which is similar to the situation with right-censoring as discussed in Uno et al. (2011) [On the C-statistics for evaluating overall adequacy of risk prediction procedures with censored survival data. Statistics in Medicine 30(10), 1105–1117]. We develop a new C-Index estimator based on inverse probability weighting (IPW) that corrects for this limitation, and we generalize this estimator to settings with left-truncated and right-censored data. The proposed IPW estimators are highly robust to the underlying truncation distribution and often outperform the conventional methods in terms of bias, mean squared error, and coverage probability. We apply these estimators to evaluate a predictive survival model for mortality among patients with end-stage renal disease.C-index, left-truncation, risk prediction, survival analysis"
